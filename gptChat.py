import subprocess
import os
import pickle
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd
import matplotlib.pyplot as plt
import re
import io
import base64
import sqlite3
import requests
from autogen import ConversableAgent
from agents.sql_agent import SQLAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from agents.semantic_agent import SemanticAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from agents.query_classifier_agent import QueryClassifierAgent
from agents.followup_agent import FollowUpAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from utils.kb_loader import load_kb  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½å‡½å¼
import json
# ----------- å…¨åŸŸè¨­å®š -----------
DB_PATH = "resultDB.db"  # ä½ åœ¨ build_kb.py è£¡è¨­å®šçš„ DB åç¨±
kb_model, kb_index, kb_texts = load_kb() # è¼‰å…¥çŸ¥è­˜åº«æ¨¡å‹ã€ç´¢å¼•å’Œæ–‡æœ¬


# ----------- åˆå§‹åŒ–ä»£ç† -----------
classifier = QueryClassifierAgent() # âœ… åˆå§‹åŒ–æŸ¥è©¢åˆ†é¡ä»£ç†
sql_agent = SQLAgent() # âœ… åˆå§‹åŒ– SQLAgent
semantic_agent = SemanticAgent(kb_model=kb_model, kb_index=kb_index, kb_texts=kb_texts) # âœ… åˆå§‹åŒ–èªæ„ä»£ç†
followup_agent = FollowUpAgent()  # âœ… åˆå§‹åŒ–è¿½å•ä»£ç†


# ConversableAgent åŒ…è£
classifier_agent = ConversableAgent(
    name="ClassifierAgent",
    llm_config={},  # é€™è£¡ä¸ç”¨çµ¦ LLM configï¼Œåæ­£ä½ åªç”¨ .generate_reply() åŒ… handle
    description="Classify the user's query as either a Semantic Query or a Structured SQL Query. Helps determine which agent should handle the request.",
    human_input_mode="NEVER",  # ä¸éœ€è¦äººå·¥å›è¦†
    code_execution_config=False,
    is_termination_msg=lambda x: True,  # çµæœæ°¸é åªå› 1 æ¬¡
    function_map={
        "handle": classifier.handle
    }
)
sql_agent_wrapper = ConversableAgent(
    name="SQLAgent",
    llm_config={},
    description="Execute structured SQL queries on the incident database. Suitable for questions involving counts, trends, filters, and structured data summaries.",
    human_input_mode="NEVER",
    code_execution_config=False,
    is_termination_msg=lambda x: True,
    function_map={
        "handle": sql_agent.handle
    }
)
semantic_agent_wrapper = ConversableAgent(
    name="SemanticAgent",
    llm_config={},
    description="Perform semantic search using vector embeddings to retrieve similar historical incidents and suggest relevant solutions. Best for vague, context-based queries.",
    human_input_mode="NEVER",
    code_execution_config=False,
    is_termination_msg=lambda x: True,
    function_map={
        "handle": semantic_agent.handle
    }
)
followup_agent_wrapper = ConversableAgent(
    name="FollowUpAgent",
    llm_config={},
    description="Handle follow-up questions by leveraging previous context. Useful when the user refers to past queries like 'the previous one' or 'that issue you mentioned earlier'.",
    human_input_mode="NEVER",
    code_execution_config=False,
    is_termination_msg=lambda x: True,
    function_map={
        "handle": followup_agent.handle
    }
)

# ----------- å„²å­˜æŸ¥è©¢ä¸Šä¸‹æ–‡ -----------
def save_query_context(chat_id, query, result_type, filter_info=None, result_summary=None):
    filepath = f"chat_history/{chat_id}.json"
    print(f"ğŸ“ å˜—è©¦è®€å–å°è©±è¨˜éŒ„æª”ï¼š{filepath}")

    # å˜—è©¦è®€å–å°è©±æ­·å²
    try:
        with open(filepath, encoding="utf-8") as f:
            history = json.load(f)
        if not isinstance(history, list):
            print("âš ï¸ è¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼ˆé listï¼‰ï¼Œé‡æ–°åˆå§‹åŒ–æ­·å²")
            history = []
        else:
            print(f"ğŸ“– æˆåŠŸè®€å–æ­·å²è¨˜éŒ„ï¼Œç¾æœ‰ç­†æ•¸ï¼š{len(history)}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è®€å–æ­·å²ï¼Œåˆå§‹åŒ–ç‚ºç©ºï¼š{e}")
        history = []

    # æº–å‚™ context å…§å®¹
    context = {
        "type": result_type,
        "query": query,
        "filters": filter_info,
        "summary": result_summary
    }
    print(f"ğŸ§  æº–å‚™å„²å­˜çš„ contextï¼š{context}")

    # è‹¥ç„¡æ­·å²ï¼Œå»ºç«‹ä½”ä½å°è©±
    if not history:
        print("ğŸ“Œ å°šç„¡å°è©±æ­·å²ï¼Œè‡ªå‹•æ–°å¢ä¸€å‰‡ä½”ä½è¨Šæ¯ä¸¦é™„åŠ  contextã€‚")
        history.append({
            "role": "user",
            "content": query,
            "context": context
        })
    else:
        print("ğŸ” å·²æœ‰æ­·å²ï¼Œå°‡ context å¯«å…¥æœ€å¾Œä¸€å‰‡å°è©±...")
        history[-1]["context"] = context

    # é¡¯ç¤ºå°‡è¦å¯«å…¥çš„å®Œæ•´æ­·å²
    print("ğŸ“¦ å³å°‡å„²å­˜çš„å®Œæ•´æ­·å²å…§å®¹é è¦½ï¼ˆæœ€å¾Œ 1 ç­†ï¼‰ï¼š")
    print(json.dumps(history[-1], ensure_ascii=False, indent=2))

    # å„²å­˜å› JSON æª”æ¡ˆ
    try:
        os.makedirs("chat_history", exist_ok=True)  # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²æˆåŠŸå¯«å…¥æª”æ¡ˆï¼š{filepath}")
    except Exception as e:
        print(f"âŒ å„²å­˜è¨˜æ†¶å¤±æ•—ï¼š{e}")




# ----------- GPT ä¸»å‡½å¼ -----------
def run_offline_gpt(message, model="orca2:13b", history=[], chat_id=None):
    print("ğŸŸ¢ å•Ÿå‹• GPT å›ç­”æµç¨‹...")
    print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")
    print(f"ğŸ§  ä½¿ç”¨æ¨¡å‹ï¼š{model} / chat_id: {chat_id}")

    # åˆ†é¡å™¨ â†’ AutoGen agent
    classify_result = classifier_agent.generate_reply(
        message,
        function_call="handle"
    )
    query_type = classify_result["content"] if isinstance(classify_result, dict) and "content" in classify_result else classify_result
    print(f"ğŸ” åˆ¤æ–·çµæœï¼š{query_type}")


    # å¦‚æœæ˜¯è¿½å•æŸ¥è©¢ï¼Œç›´æ¥è½‰äº¤è™•ç†(å°šæœªå®Œæˆ) æ‡‰è©²è¦ä½µåˆ° classify_query_type è£¡é¢
    # é€™è£¡å‡è¨­è¿½å•æŸ¥è©¢æœƒæœ‰ chat_idï¼Œå¦å‰‡ç„¡æ³•æ‰¾åˆ°å°æ‡‰çš„æ­·å²è¨˜éŒ„
    # åœ¨ run_offline_gpt è£¡é¢é€™æ®µæ”¹å¯«ï¼š
    # è¿½å•æŸ¥è©¢
    if followup_agent.is_follow_up(message) and chat_id:
        print("ğŸ” åµæ¸¬ç‚ºè¿½å•æŸ¥è©¢ï¼Œè½‰äº¤ FollowUpAgent è™•ç†...")
        result = followup_agent_wrapper.generate_reply(
            (chat_id, message),
            function_call="handle"
        )
        return result["content"] if isinstance(result, dict) and "content" in result else result


    # å¦‚æœæ˜¯sql çµæ§‹åŒ–æŸ¥è©¢ï¼Œèµ°sqlæŸ¥è©¢æµç¨‹ 
    if query_type == "Structured SQL":
        print("ğŸ§¾ é¡å‹ç‚º SQL çµæ§‹åŒ–æŸ¥è©¢ï¼Œæ”¹ç”± SQLAgent è™•ç†...")
        sql_agent.set_model("deepseek-coder-v2:latest")
        result = sql_agent_wrapper.generate_reply(
            message,
            function_call="handle"
        )
        reply = result["content"] if isinstance(result, dict) and "content" in result else result
        print(f"ğŸ“¥ SQLAgent å›è¦†ï¼ˆå‰ 500 å­—ï¼‰ï¼š{reply[:500]}{'...' if len(reply) > 500 else ''}")
        if not reply:
            print("âš ï¸ SQLAgent å›è¦†ç‚ºç©ºï¼Œå¯èƒ½æ˜¯æŸ¥è©¢èªå¥ç”Ÿæˆå¤±æ•—")
            return "âš ï¸ ç„¡æ³•ç”Ÿæˆæœ‰æ•ˆçš„ SQL æŸ¥è©¢èªå¥ã€‚è«‹æª¢æŸ¥æ‚¨çš„å•é¡Œæè¿°ã€‚"
        save_query_context(chat_id, message, query_type, result_summary=reply[:500])
        return reply
    


    # é è¨­ç‚º Semantic Query
    semantic_agent.model = model  # âœ… å‹•æ…‹æŒ‡å®šä½¿ç”¨è€…ç•¶å‰é¸æ“‡çš„æ¨¡å‹
    print("ğŸ”„ é¡å‹ç‚ºèªæ„æŸ¥è©¢ï¼Œæ”¹ç”± SemanticAgent è™•ç†...")
    result = semantic_agent_wrapper.generate_reply(
        message,
        function_call="handle"
    )
    kb_context = result["content"] if isinstance(result, dict) and "content" in result else result
    print("ğŸ§  çŸ¥è­˜åº«æ‘˜è¦è™•ç†å®Œæˆ")
    print(f"ğŸ“š çŸ¥è­˜åº«æª¢ç´¢çµæœç­†æ•¸ï¼š{len(kb_context)}")
    print(f"ğŸ“š çŸ¥è­˜åº«æ‘˜è¦ï¼ˆå‰ 300 å­—ï¼‰ï¼š{kb_context[:300]}{'...' if len(kb_context) > 300 else ''}")
    if not kb_context:
        print("âš ï¸ çŸ¥è­˜åº«æª¢ç´¢çµæœç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦")
        return "âš ï¸ ç„¡æ³•å¾çŸ¥è­˜åº«æª¢ç´¢åˆ°ç›¸é—œè³‡æ–™ã€‚è«‹å˜—è©¦æ›´æ”¹æ‚¨çš„å•é¡Œæè¿°ã€‚"
    print("ğŸ“š çŸ¥è­˜åº«æ‘˜è¦å®Œæˆ")

    # çµ„åˆå°è©±æ­·å²
    context = ""
    if not isinstance(history, list):
        print("âš ï¸ å°è©±æ­·å²æ ¼å¼éŒ¯èª¤ï¼Œåˆå§‹åŒ–ç‚ºç©º list")
        history = []
    # åƒ…å–æœ€å¾Œ 5 ç­†
    # æ ¹æ“šå‰å¹¾è¼ªçš„å°è©±ç´€éŒ„ historyï¼Œçµ„åˆæˆä¸€æ®µæ–‡å­— contextï¼Œè®“æ¨¡å‹èƒ½çœ‹æ‡‚ä¸Šä¸‹æ–‡è„ˆçµ¡ï¼ˆå¤šè¼ªå°è©±ï¼‰ã€‚
    for turn in history[-5:]:
        
        role = "User" if turn["role"] == "user" else "Assistant" # å°‡ role è½‰ç‚º User/Assistant
        context += f"{role}: {turn['content']}\n" # åƒ…å–æœ€å¾Œ 5 ç­†

    prompt = (
        "You are a knowledgeable helpdesk assistant. "
        "You will first review some relevant case entries, then answer the user's question based on context and your reasoning.\n\n"
        
        "==== [Retrieved Knowledge Base Entries] ====\n"
        f"{kb_context.strip()}\n\n"
        
        "==== [Conversation History] ====\n"
        f"{context.strip()}\n"
        
        "==== [User's Current Question] ====\n"
        f"User: {message}\n"
        
        "==== [Your Response] ====\n"
        "Assistant:"
    )
    print("\n[Prompt Preview] ğŸ§¾ ç™¼é€çµ¦æ¨¡å‹çš„ Prompt å‰ 300 å­—ï¼š")
    print(prompt[:300] + ("..." if len(prompt) > 300 else ""))
    try:
        print("ğŸš€ ç™¼é€ prompt çµ¦æ¨¡å‹ä¸­...")
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt.encode("utf-8"),
            capture_output=True,
            timeout=600
        )

        if result.returncode != 0:
            err = result.stderr.decode('utf-8')
            print("âŒ æ¨¡å‹éŒ¯èª¤ï¼š", err)
            return f"âš ï¸ Ollama éŒ¯èª¤ï¼š{err}"

        reply = result.stdout.decode("utf-8").strip()
        print("ğŸ“¥ æ¨¡å‹å›è¦†ï¼ˆå‰ 300 å­—ï¼‰ï¼š")
        print(reply[:300] + ("..." if len(reply) > 300 else ""))

        save_query_context(chat_id, message, query_type, result_summary=reply[:200])
        return reply if reply else "âš ï¸ æ²’æœ‰æ”¶åˆ°æ¨¡å‹å›æ‡‰ã€‚"

    except Exception as e:
        print(f"âŒ å‘¼å«æ¨¡å‹å¤±æ•—ï¼š{str(e)}")
        return f"âš ï¸ å‘¼å«æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
    
# -----------------------------------------------ä»¥ä¸‹æ˜¯è¨»è§£--------------------------------------------------------
    
    
    
    
    
    




# ----------- çµ±è¨ˆåˆ†ææŸ¥è©¢ -----------
# def analyze_metadata_query(message):
#     print("ğŸ“Š åŸ·è¡Œçµ±è¨ˆåˆ†æ...")
#     print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç¸½ç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print("âŒ metadata è¼‰å…¥éŒ¯èª¤")
#         return f"âš ï¸ ç„¡æ³•è¼‰å…¥ metadataï¼š{str(e)}"

#     system_prompt = (
#         "You are helping analyze a structured knowledge base.\n"
#         "From the user's question, choose ONE of the following fields to do statistical aggregation:\n"
#         " - subcategory\n - configurationItem\n - roleComponent\n - location\n"
#         "If the request is vague or unclear, respond with '__fallback__'.\n"
#         "Only return one word: the field name or '__fallback__'.\n"
#         "Do not return any explanation or code block. Just the field name."
#     )
#     prompt = f"{system_prompt}\n\nUser: {message}"
#     print(f"ğŸ“¤ ç™¼é€çµ¦æ¨¡å‹çš„ promptï¼š\n{prompt}")

#     try:
#         print("ğŸš€ å‘¼å«æ¨¡å‹ phi3:mini åˆ†ææ¬„ä½...")
#         result = subprocess.run(
#             ["ollama", "run", "phi3:mini"],
#             input=prompt.encode("utf-8"),
#             capture_output=True,
#             timeout=600
#         )
#         print(f"ğŸ”š æ¨¡å‹å›å‚³ç¢¼ï¼š{result.returncode}")
#         raw = result.stdout.decode("utf-8").strip()

#         # âœ… ç§»é™¤ markdown æ ¼å¼åŒ…è£¹
#         if raw.startswith("```"):
#             print("âš ï¸ åµæ¸¬åˆ° markdown åŒ…è£¹ï¼Œå˜—è©¦ç§»é™¤...")
#             raw = raw.strip("`").strip()
#             if "\n" in raw:
#                 raw = "\n".join(raw.split("\n")[1:-1])

#         field = raw.strip().strip('"').strip("'").lower()  # âœ… æ¨™æº–åŒ–å­—ä¸²
#         print(f"[æ¬„ä½åˆ¤æ–·] GPT å›è¦†æ¬„ä½ï¼š{field}")

#         allowed_fields = {"subcategory", "configurationItem", "roleComponent", "location"}

#         if field == "__fallback__":
#             print("ğŸ” å›å‚³ fallbackï¼Œæ”¹ç‚ºåˆ—å‡º subcategory å’Œ configurationItem çµ±è¨ˆ")
#             return "\n\n".join([
#                 summarize_field("subcategory", metadata),
#                 summarize_field("configurationItem", metadata)
#             ])

#         if field not in allowed_fields:
#             print("âš ï¸ æ¨¡å‹å›å‚³ä¸åœ¨å…è¨±æ¬„ä½ä¸­")
#             return f"âš ï¸ ç„¡æ³•åˆ¤æ–·è¦çµ±è¨ˆçš„æ¬„ä½ï¼ˆå›è¦†ç‚ºï¼š{field}ï¼‰"

#         print(f"âœ… é€²è¡Œæ¬„ä½ {field} çš„çµ±è¨ˆ")
#         return summarize_field(field, metadata)

#     except Exception as e:
#         print(f"âŒ å‘¼å«æ¨¡å‹éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
#         return f"âš ï¸ å‘¼å«æ¨¡å‹åˆ†é¡æ¬„ä½æ™‚å‡ºéŒ¯ï¼š{str(e)}"





# ----------- çµ±è¨ˆæ¬„ä½å€¼ -----------
# def summarize_field(field, metadata):
#     print(f"ğŸ“Š é–‹å§‹çµ±è¨ˆæ¬„ä½ï¼š{field}")
#     counts = {}
#     for item in metadata:
#         key = item.get(field, "æœªæ¨™è¨»")
#         counts[key] = counts.get(key, 0) + 1

#     print(f"ğŸ“ˆ çµ±è¨ˆå®Œæˆï¼Œå…±æœ‰ {len(counts)} ç¨®ä¸åŒå€¼")

#     sorted_counts = sorted(counts.items(), key=lambda x: -x[1])
#     for i, (k, v) in enumerate(sorted_counts[:5]):
#         print(f"  ğŸ”¢ Top{i+1}: {k} - {v} ç­†")

#     result_lines = [f"{i+1}. {k}ï¼š{v} ç­†" for i, (k, v) in enumerate(sorted_counts[:5])]
#     return f"ğŸ“Š çµ±è¨ˆçµæœï¼ˆä¾ {field}ï¼‰ï¼š\n" + "\n".join(result_lines)


# ----------- æ¬„ä½å€¼æ¸…å–®æŸ¥è©¢ -----------
# def list_field_values(message):
#     print("ğŸ” å•Ÿå‹•æ¬„ä½å€¼åˆ—èˆ‰ä»»å‹™...")
#     print(f"ğŸ“ ä½¿ç”¨è€…æå•ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print("âŒ metadata è¼‰å…¥å¤±æ•—")
#         return f"âš ï¸ Failed to load metadata: {str(e)}"

#     system_prompt = (
#         "You are a parser. The user is asking about what values are available in a certain field.\n"
#         "Please extract which field they want to list.\n"
#         "Return the field name only. Must be one of: configurationItem, subcategory, roleComponent, location"
#     )
#     prompt = f"{system_prompt}\n\nUser: {message}"
#     print(f"ğŸ“¤ ç™¼é€çµ¦æ¨¡å‹çš„ promptï¼š\n{prompt}")

#     try:
#         print("ğŸ§  ä½¿ç”¨æ¨¡å‹ phi3:mini åˆ¤æ–·æ¬„ä½åç¨±...")
#         result = subprocess.run(
#             ["ollama", "run", "phi3:mini"],
#             input=prompt.encode("utf-8"),
#             capture_output=True,
#             timeout=600
#         )

#         raw_reply = result.stdout.decode("utf-8").strip()
#         print(f"[å›æ‡‰] GPT å›å‚³ï¼š{raw_reply}")

#         field = raw_reply.strip()
#         if field not in ["configurationItem", "subcategory", "roleComponent", "location"]:
#             print("âš ï¸ æ¨¡å‹å›å‚³ç„¡æ•ˆæ¬„ä½")
#             return f"âš ï¸ Invalid field: {field}"

#         print(f"âœ… æ¬„ä½åˆ¤å®šæˆåŠŸï¼š{field}")
#         values = set()
#         for item in metadata:
#             value = item.get(field)
#             if value:
#                 values.add(value)

#         print(f"ğŸ“Š æ“·å–æ¬„ä½å€¼å®Œæˆï¼Œå…± {len(values)} ç¨®ä¸åŒå€¼")
#         sorted_vals = sorted(values)
#         for i, v in enumerate(sorted_vals[:5]):
#             print(f"  - Top {i+1}: {v}")

#         lines = [f"- {v}" for v in sorted_vals[:20]]
#         return f"ğŸ“‹ Values in '{field}' field:\n" + "\n".join(lines)

#     except Exception as e:
#         print(f"âŒ å‘¼å«æ¨¡å‹æˆ–è§£æéŒ¯èª¤ï¼š{str(e)}")
#         return f"âš ï¸ Failed to process: {str(e)}"

    


# ----------- æ¬„ä½æŸ¥è©¢åˆ†æ -----------

# def analyze_field_query(message):
#     print("ğŸ” å•Ÿå‹•å¤šæ¬„ä½æ¢ä»¶æŸ¥è©¢åˆ†æ...")
#     print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç¸½ç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print(f"âŒ metadata è¼‰å…¥å¤±æ•—ï¼š{str(e)}")
#         return f"âš ï¸ Failed to load metadata: {str(e)}"

#     # âœ… å–å¾—åˆæ³•å€¼æ¸…å–®ï¼ˆé™åˆ¶æ¨¡å‹è¼¸å‡ºï¼‰
#     allowed_fields = ["configurationItem", "subcategory", "roleComponent", "location"]
#     valid_values = {
#         field: sorted(set(str(item.get(field, "")).strip() for item in metadata if item.get(field)))
#         for field in allowed_fields
#     }
#     value_hint = "\n".join([f"{field}: {valid_values[field]}" for field in allowed_fields])

#     system_prompt = (
#         "You are a parser. Extract all field=value conditions from the user's message for filtering.\n"
#         "Only include fields: configurationItem, subcategory, roleComponent, location.\n"
#         "Use only values from these lists:\n"
#         f"{value_hint}\n"
#         "Return ONLY a raw JSON array like:\n"
#         '[{"field": "subcategory", "value": "Login/Access"}, {"field": "location", "value": "Taipei"}]\n'
#         "Do not include any explanation or markdown formatting.\n"
#         "The entire output must be a compact JSON array and must not exceed 500 characters in total."
#     )

#     prompt = f"{system_prompt}\n\nUser: {message}"
#     print(f"ğŸ“¤ ç™¼é€çµ¦æ¨¡å‹çš„ promptï¼š\n{prompt}")

#     try:
#         print("ğŸ§  å‘¼å«æ¨¡å‹ phi3:mini åˆ¤æ–·éæ¿¾æ¬„ä½æ¢ä»¶...")
#         result = subprocess.run(
#             ["ollama", "run", "phi3:mini"],
#             input=prompt.encode("utf-8"),
#             capture_output=True,
#             timeout=600
#         )
#         raw = result.stdout.decode("utf-8").strip()
#         print("[ğŸ” å¤šæ¬„ä½æŸ¥è©¢åŸå§‹å›è¦†]", raw)

#     # âœ… å»é™¤ markdown åŒ…è£¹èˆ‡æ¨™ç±¤å¹²æ“¾
#         if "```" in raw:
#             print("âš ï¸ åµæ¸¬åˆ° markdown æ ¼å¼ï¼Œæ­£åœ¨æ¸…ç†...")
#             raw = raw.split("```")[1].strip()
#         if raw.startswith("json"):
#             raw = raw[len("json"):].strip()

#         print("ğŸ“¥ æ¸…ç†å¾Œçš„ JSON å­—ä¸²ï¼š", raw)

#         # âœ… å˜—è©¦å¾åŸå§‹å­—ä¸²ä¸­æ“·å–åˆæ³• JSON é™£åˆ—
#         match = re.search(r'\[\s*{.*?}\s*\]', raw, re.DOTALL)
#         if not match:
#             return "âš ï¸ Failed to extract valid JSON array from model output."
#         json_part = match.group(0)

#         try:
#             parsed_conditions = json.loads(json_part)
#             print(f"âœ… æˆåŠŸè§£æç‚º JSON é™£åˆ—ï¼Œå…± {len(parsed_conditions)} ç­†æ¢ä»¶")
#         except Exception as e:
#             print(f"âŒ JSON è§£æå¤±æ•—ï¼š{e}")
#             return f"âš ï¸ JSON decode error: {str(e)}"


#         if not isinstance(parsed_conditions, list):
#             print("âŒ è§£æçµæœé list æ ¼å¼")
#             return "âš ï¸ Invalid parsed result format (not a list)."

#         filters = [(c["field"], c["value"]) for c in parsed_conditions if c.get("field") in allowed_fields]

#         print("ğŸ” éæ¿¾å¾Œçš„æœ‰æ•ˆæ¢ä»¶ï¼š")
#         for f, v in filters:
#             print(f"  â€¢ {f} = {v}")

#         if not filters:
#             print("âš ï¸ æ²’æœ‰æ“·å–åˆ°æœ‰æ•ˆæ¢ä»¶")
#             return "âš ï¸ No valid filters extracted from the query."

#         # ç¯©é¸è³‡æ–™ï¼ˆç¬¦åˆæ‰€æœ‰æ¢ä»¶ï¼Œå¤§å°å¯«ä¸æ•æ„Ÿï¼Œç©ºç™½å®¹éŒ¯ï¼‰
#         def match_all(item):
#             for field, value in filters:
#                 actual = str(item.get(field, "")).strip().lower()
#                 expected = str(value).strip().lower()
#                 if expected not in actual:  # âœ… æ”¹ç‚ºæ¨¡ç³Šæ¯”å°
#                     return False
#             return True

#         matches = [item for item in metadata if match_all(item)]

#         print(f"ğŸ“Š ç¬¦åˆæ¢ä»¶çš„çµæœç­†æ•¸ï¼š{len(matches)}")

#         if not matches:
#             print("ğŸ“­ æŸ¥ç„¡çµæœ")
#             return f"ğŸ” No results found for: " + " AND ".join([f"{f}={v}" for f, v in filters])

#         lines = [f"- {item.get('text', '')[:500]}" for item in matches[:5]]
#         # ğŸ” å¾ matches ä¸­å–å‡ºå¯¦éš›å‘½ä¸­çš„åŸå§‹å€¼
#         actual_values = {field: set() for field, _ in filters}
#         for item in matches:
#             for field, _ in filters:
#                 val = item.get(field, "").strip()
#                 if val:
#                     actual_values[field].add(val)

#         summary_lines = [
#             f"â€¢ {field} = {', '.join(sorted(actual_values[field])) or 'N/A'}"
#             for field in actual_values
#         ]

#         return (
#             "ğŸ” Top matches for:\n" +
#             "\n".join(summary_lines) +
#             "\n\n" + "\n".join(lines)
#         )
    
#     except Exception as e:
#         print(f"âŒ å‘¼å«æ¨¡å‹æˆ–è§£æéç¨‹å‡ºéŒ¯ï¼š{str(e)}")
#         return f"âš ï¸ Failed to parse or search: {str(e)}"




# ----------- æ™‚é–“è¶¨å‹¢åˆ†æ -----------
# def analyze_temporal_trend(message):
#     print("ğŸ“ˆ å•Ÿå‹•æ™‚é–“è¶¨å‹¢åˆ†æ...")
#     print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print("ğŸ“¦ ç¬¬ä¸€ç­†è³‡æ–™ï¼š", metadata[0])
#         print("ğŸ” æ˜¯å¦æœ‰ analysisTime æ¬„ä½ï¼š", "analysisTime" in metadata[0])
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç¸½ç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print(f"âŒ metadata è¼‰å…¥å¤±æ•—ï¼š{str(e)}")
#         return f"âš ï¸ ç„¡æ³•è¼‰å…¥è³‡æ–™ï¼š{str(e)}"

#     if not metadata:
#         print("âš ï¸ metadata ç‚ºç©º")
#         return "âš ï¸ ç„¡è³‡æ–™å¯åˆ†æã€‚"

#     if "analysisTime" not in metadata[0]:
#         print("âš ï¸ analysisTime æ¬„ä½ä¸å­˜åœ¨")
#         return "âš ï¸ ç¼ºå°‘ analysisTime æ¬„ä½ï¼Œç„¡æ³•åˆ†æè¶¨å‹¢ã€‚"

#     print("ğŸ§ª é–‹å§‹è½‰æ›ç‚º DataFrame ä¸¦è™•ç†æ™‚é–“æ¬„ä½...")
#     df = pd.DataFrame(metadata)
#     print(f"ğŸ“Š DataFrame æ¬„ä½ï¼š{list(df.columns)}")

#     df["analysisTime"] = pd.to_datetime(df["analysisTime"], errors="coerce")
#     initial_len = len(df)
#     df = df.dropna(subset=["analysisTime"])
#     print(f"ğŸ§¹ è™•ç†ç„¡æ•ˆæ™‚é–“å¾Œï¼Œå‰©é¤˜ç­†æ•¸ï¼š{len(df)} / åŸå§‹ {initial_len}")

#     print("ğŸ“† å»ºç«‹æœˆä»½æ¬„ä½ä¸¦è¨ˆç®—æ¯æœˆæ•¸é‡...")
#     df["month"] = df["analysisTime"].dt.to_period("M")
#     trend = df.groupby("month").size()
#     print("ğŸ“Š æ¯æœˆçµ±è¨ˆçµæœï¼š")
#     for month, count in trend.items():
#         print(f"  â€¢ {month}: {count} ç­†")

#     # âœ… ç”¨æ–‡å­—æ•˜è¿°æ¯æœˆè¶¨å‹¢
#     summary_lines = ["ğŸ“Š æ¯æœˆæ¡ˆä»¶è¶¨å‹¢ï¼š"]
#     for month, count in trend.items():
#         summary_lines.append(f"- {month.strftime('%Y-%m')}: {count} ç­†")
#     print("âœ… å·²è½‰æ›ç‚ºç´”æ–‡å­—æè¿°")

#     return "\n".join(summary_lines)

    
    
    
# ----------- è§£æ³•çµ±æ•´ -----------
# def summarize_solutions(message):
#     print("ğŸ› ï¸ å•Ÿå‹•ç›¸ä¼¼æ¡ˆä¾‹è™•ç†æ–¹æ¡ˆæ‘˜è¦æµç¨‹...")
#     print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print(f"âŒ metadata è¼‰å…¥å¤±æ•—ï¼š{str(e)}")
#         return f"âš ï¸ Failed to load metadata: {str(e)}"

#     print("ğŸ” é–‹å§‹èªæ„æ¯”å°ç›¸é—œæ¡ˆä¾‹...")
#     related_cases = search_knowledge_base(message, top_k=5)
#     print(f"ğŸ§  å–å¾—ç›¸ä¼¼ç‰‡æ®µæ•¸é‡ï¼š{len(related_cases)}")
#     for i, r in enumerate(related_cases, 1):
#         print(f"  {i}. {r[:60]}{'...' if len(r) > 60 else ''}")

#     if not related_cases:
#         print("âš ï¸ ç„¡ç›¸é—œæ¡ˆä¾‹")
#         return "âš ï¸ No similar cases found to extract solutions."

#     print("ğŸ“¦ æ“·å–ç›¸é—œæ¡ˆä¾‹ä¸­çš„è§£æ±ºæ–¹æ¡ˆ...")
#     solutions = []
#     for item in metadata:
#         text = item.get("text", "")
#         if any(snippet in text for snippet in related_cases):
#             solution = item.get("solution", "")
#             if solution:
#                 solutions.append(solution)

#     print(f"âœ… æ“·å–åˆ° solution æ•¸é‡ï¼š{len(solutions)}")
#     if not solutions:
#         print("âš ï¸ ç„¡å°æ‡‰è§£æ±ºæ–¹æ¡ˆæ¬„ä½")
#         return "âš ï¸ No resolution data found for related cases."

#     print("ğŸ“ çµ„è£ prompt é€²è¡Œ GPT çµ±æ•´...")
#     prompt = "Please summarize the following resolution steps into a brief, clear list:\n\n"
#     prompt += "\n---\n".join(solutions[:10])
#     prompt += "\n\nSummary:"

#     print("ğŸ“¤ ç™¼é€ prompt çµ¦æ¨¡å‹ï¼ˆå‰ 300 å­—ï¼‰ï¼š")
#     print(prompt[:300] + ("..." if len(prompt) > 300 else ""))

#     try:
#         result = subprocess.run(
#             ["ollama", "run", "phi4"],
#             input=prompt.encode("utf-8"),
#             capture_output=True,
#             timeout=600
#         )
#         output = result.stdout.decode("utf-8").strip()
#         print("âœ… æ¨¡å‹æˆåŠŸå›æ‡‰ï¼ˆå‰ 200 å­—ï¼‰ï¼š")
#         print(output[:200] + ("..." if len(output) > 200 else ""))
#         return output

#     except Exception as e:
#         print(f"âŒ æ¨¡å‹æ‘˜è¦å¤±æ•—ï¼š{str(e)}")
#         return f"âš ï¸ Failed to summarize solutions: {str(e)}"
    
    
    

    # if query_type == "Statistical Analysis":
    #     print("ğŸ“Š é¡å‹ç‚ºçµ±è¨ˆåˆ†æï¼Œé–‹å§‹è™•ç†...")
    #     reply = analyze_metadata_query(message)
    #     print("ğŸ“¦ çµ±è¨ˆåˆ†æå®Œæˆï¼Œæ‘˜è¦å‰ 200 å­—ï¼š", reply[:200])
    #     save_query_context(chat_id, message, query_type, result_summary=reply[:200])
    #     return reply

    # if query_type == "Field Filter":
    #     print("ğŸ”„ é¡å‹ç‚ºæ¬„ä½éæ¿¾ï¼Œé–‹å§‹é€²è¡Œéæ¿¾...")
    #     reply = analyze_field_query(message)
    #     print("ğŸ“¦ éæ¿¾æŸ¥è©¢å®Œæˆï¼Œå‰ 200 å­—ï¼š", reply[:200])
    #     filters = []
    #     for line in reply.splitlines():
    #         if line.strip().startswith("â€¢ "):
    #             try:
    #                 field_part = line.replace("â€¢", "").strip()
    #                 field, value = field_part.split("=", 1)
    #                 filters.append({"field": field.strip(), "value": value.strip()})
    #             except Exception as e:
    #                 print(f"âš ï¸ ç„¡æ³•è§£ææ¢ä»¶è¡Œï¼š{line}ï¼ŒéŒ¯èª¤ï¼š{e}")
    #                 continue
    #     print("ğŸ§¾ æ“·å–åˆ°çš„ filtersï¼š", filters)
    #     save_query_context(chat_id, message, query_type, filter_info=filters if filters else None, result_summary=reply[:200])
    #     return reply

    # if query_type == "Field Values":
    #     print("ğŸ“‹ é¡å‹ç‚ºæ¬„ä½å€¼æ¸…å–®ï¼Œé–‹å§‹åˆ—å‡ºæ¬„ä½å€¼...")
    #     reply = list_field_values(message)
    #     print("ğŸ“¦ æ¬„ä½å€¼æŸ¥è©¢å®Œæˆï¼Œå‰ 200 å­—ï¼š", reply[:200])
    #     save_query_context(chat_id, message, query_type, result_summary=reply[:200])
    #     return reply

    # if query_type == "Temporal Trend":
    #     print("ğŸ“ˆ é¡å‹ç‚ºæ™‚é–“è¶¨å‹¢æŸ¥è©¢ï¼Œé–‹å§‹ç¹ªè£½åœ–è¡¨...")
    #     reply = analyze_temporal_trend(message)
    #     print("ğŸ“¦ è¶¨å‹¢åœ–å®Œæˆï¼ˆHTML ç‰‡æ®µï¼‰")
    #     save_query_context(chat_id, message, query_type, result_summary=reply[:200])
    #     return reply

    # if query_type == "Solution Summary":
    #     print("ğŸ›  é¡å‹ç‚ºè§£æ³•çµ±æ•´ï¼Œé–‹å§‹å½™æ•´è™•ç†æ–¹å¼...")
    #     reply = summarize_solutions(message)
    #     print("ğŸ“¦ è§£æ³•çµ±æ•´å®Œæˆï¼Œå‰ 200 å­—ï¼š", reply[:200])
    #     save_query_context(chat_id, message, query_type, result_summary=reply[:200])
    #     return reply

