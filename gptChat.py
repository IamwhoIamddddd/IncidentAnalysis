import subprocess
import os
import pickle
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd
import matplotlib.pyplot as plt
import re
import io
import base64
import sqlite3

DB_PATH = "resultDB.db"  # ä½ åœ¨ build_kb.py è£¡è¨­å®šçš„ DB åç¨±



# ----------- çŸ¥è­˜åº«å‘é‡è¼‰å…¥èˆ‡æª¢ç´¢ -----------

def load_kb():
    print("ğŸ”„ æ­£åœ¨è¼‰å…¥çŸ¥è­˜åº«...")
    if not os.path.exists("kb_index.faiss") or not os.path.exists("kb_texts.pkl"):
        print("âš ï¸ æ‰¾ä¸åˆ°çŸ¥è­˜åº«æª”æ¡ˆï¼ŒRAG åŠŸèƒ½åœç”¨")
        return None, None, None
    model = SentenceTransformer("all-MiniLM-L6-v2")
    index = faiss.read_index("kb_index.faiss")
    with open("kb_texts.pkl", "rb") as f:
        kb_texts = pickle.load(f)
    print(f"âœ… å·²è¼‰å…¥çŸ¥è­˜åº«ï¼Œå…± {len(kb_texts)} ç­†")
    return model, index, kb_texts

kb_model, kb_index, kb_texts = load_kb()

# ----------- çŸ¥è­˜åº«æ‘˜è¦å£“ç¸® -----------



def summarize_retrieved_kb(retrieved, primary_model="orca2:13b"):
    print("ğŸ” é–‹å§‹çŸ¥è­˜åº«æ‘˜è¦è™•ç†...")
    if not retrieved:
        print("âš ï¸ ç„¡è³‡æ–™å¯æ‘˜è¦ï¼ˆretrieved ç‚ºç©ºï¼‰")
        return ""
    print("ğŸ§  æ­£åœ¨é€²è¡Œåˆ†æ®µæ‘˜è¦è™•ç†ï¼ˆretrieved KBï¼‰...")
    print(f"ğŸ“¦ è¼¸å…¥ç­†æ•¸ï¼š{len(retrieved)}")
    model_token_limits = {
        "orca2:13b": 8192,
        "nous-hermes2:10.7b": 8192,
        "mistral": 8192,
        "phi4-mini": 4096,
        "phi3:mini": 4096,
        "command-r7b:latest": 4096,
        "openchat:7b": 4096,
        "deepseek-coder-v2:latest": 16384,
        "deepseek-coder:latest": 16384,
    }

    fallback_models = ["nous-hermes2:10.7b", "mistral", "phi4:mini"]
    token_limit = model_token_limits.get(primary_model, 4096)
    prompt_reserve = 500
    available_tokens = token_limit - prompt_reserve
    print(f"ğŸ§  æ¨¡å‹ {primary_model} å¯ç”¨ token é™åˆ¶ï¼š{available_tokens}")
    def estimate_token(text):
        return int(len(text) / 4)
    # åˆ†æ®µ

    groups = []
    group = []
    token_sum = 0

    for text in retrieved:
        tokens = estimate_token(text)
        if token_sum + tokens > available_tokens and group:
            groups.append(group)
            group = [text]
            token_sum = tokens
        else:
            group.append(text)
            token_sum += tokens
    if group:
        groups.append(group)
    def try_models(prompt):
        for model in [primary_model] + fallback_models:
            try:
                print(f"ğŸ§  ä½¿ç”¨æ¨¡å‹æ‘˜è¦ï¼š{model}")
                result = subprocess.run(
                    ["ollama", "run", model],
                    input=prompt.encode("utf-8"),
                    capture_output=True,
                    timeout=240  # è¨­å®šè¼ƒé•·çš„ timeout ä»¥é¿å…è¶…æ™‚
                )
                if result.returncode == 0:
                    return result.stdout.decode("utf-8").strip()
                else:
                    print(f"âš ï¸ æ¨¡å‹ {model} æ‘˜è¦å¤±æ•—")
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model} éŒ¯èª¤ï¼š{e}")
        return "âŒ å…¨éƒ¨æ¨¡å‹æ‘˜è¦å¤±æ•—"
    # å°æ¯æ®µé€²è¡Œæ‘˜è¦
    chunk_summaries = []
    for i, group in enumerate(groups, 1):
        prompt = "Please summarize the key points and handling methods based on the following knowledge entries (respond in English):\n\n"
        for j, txt in enumerate(group, 1):
            prompt += f"{j}. {txt.strip()}\n"
        prompt += "\nPlease provide a single summary paragraph:"
        reply = try_models(prompt)
        chunk_summaries.append(reply)
        print(f"âœ… ç¬¬ {i} çµ„æ‘˜è¦å®Œæˆ")
    if len(chunk_summaries) == 1:
        return chunk_summaries[0]
    # åˆä½µæ‘˜è¦éè¿´

    def recursive_merge(summaries):
        available_tokens = token_limit - prompt_reserve
        merged_groups = []
        group = []
        token_count = 0
        for s in summaries:
            t = estimate_token(s)
            if token_count + t > available_tokens and group:
                merged_groups.append(group)
                group = [s]
                token_count = t
            else:
                group.append(s)
                token_count += t
        if group:
            merged_groups.append(group)
        results = []
        for i, group in enumerate(merged_groups, 1):
            merge_prompt = "Based on the following summaries, please synthesize the main insights:\n\n"
            for j, s in enumerate(group, 1):
                merge_prompt += f"ï¼ˆç¬¬ {j} æ®µæ‘˜è¦ï¼‰{s}\n\n"
            merge_prompt += "Please provide an overall concluding observation:"
            reply = try_models(merge_prompt)
            results.append(reply)
        return results[0] if len(results) == 1 else recursive_merge(results)
    return recursive_merge(chunk_summaries)

def determine_top_k_with_llm(user_input, fallback=3, min_top_k=1, max_top_k=10):
    print("ğŸ§  ä½¿ç”¨ LLM é æ¸¬åˆé©çš„ top_k æ•¸é‡...")
    prompt = (
        "You are a knowledge retrieval assistant. Based on the user's question, decide how many similar cases (top_k) should be retrieved from the knowledge base.\n\n"
        "Guidelines:\n"
        "- If the question is **very specific** (mentions error codes, clear symptoms, or keywords), return a **small** top_k (1â€“3).\n"
        "- If the question is **vague or general** (like 'why is it slow?' or 'something went wrong'), return a **larger** top_k (5â€“10).\n"
        "- If the user asks for a **summary, report, or trend**, use a **larger** top_k (8â€“10).\n"
        "- Only reply with a **single integer** between 1 and 10. Do not add explanation.\n\n"
        f"User question: {user_input}\n\nAnswer:"
    )
 
    def try_model(model_name):
        try:
            print(f"ğŸ§  å˜—è©¦æ¨¡å‹ï¼š{model_name}")
            result = subprocess.run(
                ["ollama", "run", model_name],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=240  # è¨­å®šè¼ƒé•·çš„ timeout ä»¥é¿å…è¶…æ™‚
            )
            if result.returncode == 0:
                reply = result.stdout.decode("utf-8").strip()
                print(f"ğŸ“¥ æ¨¡å‹å›è¦†ï¼š{reply}")
                match = re.search(r"\b([1-9]|10)\b", reply)
                if match:
                    top_k = int(match.group(1))
                    return max(min_top_k, min(top_k, max_top_k))
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model_name} å¤±æ•—ï¼š{e}")
        return None
    for model in ["command-r7b:latest", "openchat:7b", "phi4-mini"]:
        top_k = try_model(model)
        if top_k:
            return top_k
    print("âš ï¸ å…¨éƒ¨æ¨¡å‹å¤±æ•—ï¼Œä½¿ç”¨ fallback")
    return fallback

 





# ----------- çŸ¥è­˜åº«æª¢ç´¢(èªæ„æ¯”å°é¡åˆ¥) -----------
def search_knowledge_base(query, top_k=None):
    print("ğŸ” åŸ·è¡Œèªæ„æŸ¥è©¢...")

    if kb_model is None or kb_index is None or kb_texts is None:
        print("âŒ çŸ¥è­˜åº«å°šæœªè¼‰å…¥")
        return []

    # å¦‚æœæ²’æœ‰æŒ‡å®š top_kï¼Œå°±è‡ªå‹•åˆ¤æ–·
    if top_k is None:
        top_k = determine_top_k_with_llm(query)  # å‘¼å« LLM æ±ºå®šåˆé©çš„ top_k
        print(f"ğŸ¤– å‹•æ…‹æ±ºå®š top_k = {top_k}")

    query_vec = kb_model.encode([query])
    D, I = kb_index.search(np.array(query_vec), top_k)
    print(f"[RAG] ğŸ” æŸ¥è©¢å…§å®¹ï¼š{query}")
    print(f"[RAG] ğŸ§  å–å‡ºçŸ¥è­˜åº«è³‡æ–™ï¼š{[kb_texts[i][:50] for i in I[0]]}")
    return [kb_texts[i] for i in I[0]]






# ----------- æå•é¡å‹åˆ†é¡ -----------
def classify_query_type(message):
    print("ğŸ¤– åˆ¤æ–·æå•é¡å‹...")
    print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

    system_prompt = (
        "You are a classification assistant. Your task is to analyze the user's question and classify it as one of the following two types:\n\n"
        "1. Semantic Query â€“ The user is seeking similar past incidents, solution suggestions, or insights based on previous case knowledge.\n"
        "   Typical intents: find related issues, ask how a problem was resolved, request examples of solutions.\n\n"
        "2. Structured SQL â€“ The user is requesting structured or statistical data, such as record counts, field value filtering, time-based trends, or aggregated summaries.\n"
        "   Typical intents: show number of records, list unique values, filter by conditions, summarize results over time.\n\n"
        "Respond with exactly one of the following labels (no explanations):\n"
        "'Semantic Query' or 'Structured SQL'."
    )


    prompt = f"{system_prompt}\n\nUser: {message}"
    print(f"ğŸ“¤ ç™¼é€çµ¦æ¨¡å‹çš„ promptï¼ˆå‰ 300 å­—ï¼‰ï¼š\n{prompt[:300]}{'...' if len(prompt) > 300 else ''}")

    def try_model(model_name, timeout_sec):
        try:
            print(f"ğŸ§  å˜—è©¦ä½¿ç”¨æ¨¡å‹ï¼š{model_name}ï¼ˆtimeout={timeout_sec}sï¼‰")
            result = subprocess.run(
                ["ollama", "run", model_name],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=timeout_sec
            )
            print(f"ğŸ”š æ¨¡å‹ {model_name} å›å‚³ç¢¼ï¼š{result.returncode}")
            if result.returncode != 0:
                stderr = result.stderr.decode("utf-8")
                print(f"âš ï¸ æ¨¡å‹ {model_name} åŸ·è¡Œå¤±æ•—ï¼š{stderr}")
                return None
            reply = result.stdout.decode("utf-8").strip()
            print(f"[åˆ†é¡åˆ¤æ–·] ğŸ“¥ å›è¦†ï¼ˆå‰ 200 å­—ï¼‰ï¼š{reply[:200]}{'...' if len(reply) > 200 else ''}")
            return reply
        except subprocess.TimeoutExpired:
            print(f"â° æ¨¡å‹ {model_name} è¶…æ™‚ï¼ˆè¶…é {timeout_sec} ç§’ï¼‰")
            return None
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model_name} éŒ¯èª¤ï¼š{str(e)}")
            return None

    # å˜—è©¦å…ˆç”¨ command-r7b:latestï¼Œå† fallback ç”¨ openchat:7b
    reply = try_model("command-r7b:latest", timeout_sec=120)
    if not reply:
        print("âš ï¸ command-r7b:latest å›è¦†å¤±æ•—ï¼Œæ”¹ç”¨ openchat:7b")
        reply = try_model("openchat:7b", timeout_sec=120)
        print(f"ğŸ”„ å˜—è©¦ä½¿ç”¨ openchat:7b æ¨¡å‹é€²è¡Œåˆ†é¡...")

    if not reply:
        print("âš ï¸ ç„¡æ³•å–å¾—åˆ†é¡çµæœï¼Œé è¨­ç‚º Semantic Query")
        return "Semantic Query"

    print("ğŸ§ª åˆ†æå›è¦†å…§å®¹é€²è¡Œåˆ†é¡...")
    if "Semantic Query" in reply:
        print("âœ… é¡åˆ¥åˆ¤æ–·ï¼šSemantic Query")
        return "Semantic Query"
    if "Structured SQL" in reply:
        print("âœ… é¡åˆ¥åˆ¤æ–·ï¼šStructured SQL")
        return "Structured SQL"

    # â“ è½å…¥ fallback
    print("âš ï¸ å›å‚³å…§å®¹ä¸åœ¨å…è¨±é¡å‹ä¸­ï¼Œé è¨­ç‚º Semantic Query")
    return "Semantic Query"








# ----------- çµ±è¨ˆåˆ†ææŸ¥è©¢ -----------
# def analyze_metadata_query(message):
#     print("ğŸ“Š åŸ·è¡Œçµ±è¨ˆåˆ†æ...")
#     print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç¸½ç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print("âŒ metadata è¼‰å…¥éŒ¯èª¤")
#         return f"âš ï¸ ç„¡æ³•è¼‰å…¥ metadataï¼š{str(e)}"

#     system_prompt = (
#         "You are helping analyze a structured knowledge base.\n"
#         "From the user's question, choose ONE of the following fields to do statistical aggregation:\n"
#         " - subcategory\n - configurationItem\n - roleComponent\n - location\n"
#         "If the request is vague or unclear, respond with '__fallback__'.\n"
#         "Only return one word: the field name or '__fallback__'.\n"
#         "Do not return any explanation or code block. Just the field name."
#     )
#     prompt = f"{system_prompt}\n\nUser: {message}"
#     print(f"ğŸ“¤ ç™¼é€çµ¦æ¨¡å‹çš„ promptï¼š\n{prompt}")

#     try:
#         print("ğŸš€ å‘¼å«æ¨¡å‹ phi3:mini åˆ†ææ¬„ä½...")
#         result = subprocess.run(
#             ["ollama", "run", "phi3:mini"],
#             input=prompt.encode("utf-8"),
#             capture_output=True,
#             timeout=600
#         )
#         print(f"ğŸ”š æ¨¡å‹å›å‚³ç¢¼ï¼š{result.returncode}")
#         raw = result.stdout.decode("utf-8").strip()

#         # âœ… ç§»é™¤ markdown æ ¼å¼åŒ…è£¹
#         if raw.startswith("```"):
#             print("âš ï¸ åµæ¸¬åˆ° markdown åŒ…è£¹ï¼Œå˜—è©¦ç§»é™¤...")
#             raw = raw.strip("`").strip()
#             if "\n" in raw:
#                 raw = "\n".join(raw.split("\n")[1:-1])

#         field = raw.strip().strip('"').strip("'").lower()  # âœ… æ¨™æº–åŒ–å­—ä¸²
#         print(f"[æ¬„ä½åˆ¤æ–·] GPT å›è¦†æ¬„ä½ï¼š{field}")

#         allowed_fields = {"subcategory", "configurationItem", "roleComponent", "location"}

#         if field == "__fallback__":
#             print("ğŸ” å›å‚³ fallbackï¼Œæ”¹ç‚ºåˆ—å‡º subcategory å’Œ configurationItem çµ±è¨ˆ")
#             return "\n\n".join([
#                 summarize_field("subcategory", metadata),
#                 summarize_field("configurationItem", metadata)
#             ])

#         if field not in allowed_fields:
#             print("âš ï¸ æ¨¡å‹å›å‚³ä¸åœ¨å…è¨±æ¬„ä½ä¸­")
#             return f"âš ï¸ ç„¡æ³•åˆ¤æ–·è¦çµ±è¨ˆçš„æ¬„ä½ï¼ˆå›è¦†ç‚ºï¼š{field}ï¼‰"

#         print(f"âœ… é€²è¡Œæ¬„ä½ {field} çš„çµ±è¨ˆ")
#         return summarize_field(field, metadata)

#     except Exception as e:
#         print(f"âŒ å‘¼å«æ¨¡å‹éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
#         return f"âš ï¸ å‘¼å«æ¨¡å‹åˆ†é¡æ¬„ä½æ™‚å‡ºéŒ¯ï¼š{str(e)}"





# ----------- çµ±è¨ˆæ¬„ä½å€¼ -----------
# def summarize_field(field, metadata):
#     print(f"ğŸ“Š é–‹å§‹çµ±è¨ˆæ¬„ä½ï¼š{field}")
#     counts = {}
#     for item in metadata:
#         key = item.get(field, "æœªæ¨™è¨»")
#         counts[key] = counts.get(key, 0) + 1

#     print(f"ğŸ“ˆ çµ±è¨ˆå®Œæˆï¼Œå…±æœ‰ {len(counts)} ç¨®ä¸åŒå€¼")

#     sorted_counts = sorted(counts.items(), key=lambda x: -x[1])
#     for i, (k, v) in enumerate(sorted_counts[:5]):
#         print(f"  ğŸ”¢ Top{i+1}: {k} - {v} ç­†")

#     result_lines = [f"{i+1}. {k}ï¼š{v} ç­†" for i, (k, v) in enumerate(sorted_counts[:5])]
#     return f"ğŸ“Š çµ±è¨ˆçµæœï¼ˆä¾ {field}ï¼‰ï¼š\n" + "\n".join(result_lines)


# ----------- æ¬„ä½å€¼æ¸…å–®æŸ¥è©¢ -----------
# def list_field_values(message):
#     print("ğŸ” å•Ÿå‹•æ¬„ä½å€¼åˆ—èˆ‰ä»»å‹™...")
#     print(f"ğŸ“ ä½¿ç”¨è€…æå•ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print("âŒ metadata è¼‰å…¥å¤±æ•—")
#         return f"âš ï¸ Failed to load metadata: {str(e)}"

#     system_prompt = (
#         "You are a parser. The user is asking about what values are available in a certain field.\n"
#         "Please extract which field they want to list.\n"
#         "Return the field name only. Must be one of: configurationItem, subcategory, roleComponent, location"
#     )
#     prompt = f"{system_prompt}\n\nUser: {message}"
#     print(f"ğŸ“¤ ç™¼é€çµ¦æ¨¡å‹çš„ promptï¼š\n{prompt}")

#     try:
#         print("ğŸ§  ä½¿ç”¨æ¨¡å‹ phi3:mini åˆ¤æ–·æ¬„ä½åç¨±...")
#         result = subprocess.run(
#             ["ollama", "run", "phi3:mini"],
#             input=prompt.encode("utf-8"),
#             capture_output=True,
#             timeout=600
#         )

#         raw_reply = result.stdout.decode("utf-8").strip()
#         print(f"[å›æ‡‰] GPT å›å‚³ï¼š{raw_reply}")

#         field = raw_reply.strip()
#         if field not in ["configurationItem", "subcategory", "roleComponent", "location"]:
#             print("âš ï¸ æ¨¡å‹å›å‚³ç„¡æ•ˆæ¬„ä½")
#             return f"âš ï¸ Invalid field: {field}"

#         print(f"âœ… æ¬„ä½åˆ¤å®šæˆåŠŸï¼š{field}")
#         values = set()
#         for item in metadata:
#             value = item.get(field)
#             if value:
#                 values.add(value)

#         print(f"ğŸ“Š æ“·å–æ¬„ä½å€¼å®Œæˆï¼Œå…± {len(values)} ç¨®ä¸åŒå€¼")
#         sorted_vals = sorted(values)
#         for i, v in enumerate(sorted_vals[:5]):
#             print(f"  - Top {i+1}: {v}")

#         lines = [f"- {v}" for v in sorted_vals[:20]]
#         return f"ğŸ“‹ Values in '{field}' field:\n" + "\n".join(lines)

#     except Exception as e:
#         print(f"âŒ å‘¼å«æ¨¡å‹æˆ–è§£æéŒ¯èª¤ï¼š{str(e)}")
#         return f"âš ï¸ Failed to process: {str(e)}"

    


# ----------- æ¬„ä½æŸ¥è©¢åˆ†æ -----------

# def analyze_field_query(message):
#     print("ğŸ” å•Ÿå‹•å¤šæ¬„ä½æ¢ä»¶æŸ¥è©¢åˆ†æ...")
#     print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç¸½ç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print(f"âŒ metadata è¼‰å…¥å¤±æ•—ï¼š{str(e)}")
#         return f"âš ï¸ Failed to load metadata: {str(e)}"

#     # âœ… å–å¾—åˆæ³•å€¼æ¸…å–®ï¼ˆé™åˆ¶æ¨¡å‹è¼¸å‡ºï¼‰
#     allowed_fields = ["configurationItem", "subcategory", "roleComponent", "location"]
#     valid_values = {
#         field: sorted(set(str(item.get(field, "")).strip() for item in metadata if item.get(field)))
#         for field in allowed_fields
#     }
#     value_hint = "\n".join([f"{field}: {valid_values[field]}" for field in allowed_fields])

#     system_prompt = (
#         "You are a parser. Extract all field=value conditions from the user's message for filtering.\n"
#         "Only include fields: configurationItem, subcategory, roleComponent, location.\n"
#         "Use only values from these lists:\n"
#         f"{value_hint}\n"
#         "Return ONLY a raw JSON array like:\n"
#         '[{"field": "subcategory", "value": "Login/Access"}, {"field": "location", "value": "Taipei"}]\n'
#         "Do not include any explanation or markdown formatting.\n"
#         "The entire output must be a compact JSON array and must not exceed 500 characters in total."
#     )

#     prompt = f"{system_prompt}\n\nUser: {message}"
#     print(f"ğŸ“¤ ç™¼é€çµ¦æ¨¡å‹çš„ promptï¼š\n{prompt}")

#     try:
#         print("ğŸ§  å‘¼å«æ¨¡å‹ phi3:mini åˆ¤æ–·éæ¿¾æ¬„ä½æ¢ä»¶...")
#         result = subprocess.run(
#             ["ollama", "run", "phi3:mini"],
#             input=prompt.encode("utf-8"),
#             capture_output=True,
#             timeout=600
#         )
#         raw = result.stdout.decode("utf-8").strip()
#         print("[ğŸ” å¤šæ¬„ä½æŸ¥è©¢åŸå§‹å›è¦†]", raw)

#     # âœ… å»é™¤ markdown åŒ…è£¹èˆ‡æ¨™ç±¤å¹²æ“¾
#         if "```" in raw:
#             print("âš ï¸ åµæ¸¬åˆ° markdown æ ¼å¼ï¼Œæ­£åœ¨æ¸…ç†...")
#             raw = raw.split("```")[1].strip()
#         if raw.startswith("json"):
#             raw = raw[len("json"):].strip()

#         print("ğŸ“¥ æ¸…ç†å¾Œçš„ JSON å­—ä¸²ï¼š", raw)

#         # âœ… å˜—è©¦å¾åŸå§‹å­—ä¸²ä¸­æ“·å–åˆæ³• JSON é™£åˆ—
#         match = re.search(r'\[\s*{.*?}\s*\]', raw, re.DOTALL)
#         if not match:
#             return "âš ï¸ Failed to extract valid JSON array from model output."
#         json_part = match.group(0)

#         try:
#             parsed_conditions = json.loads(json_part)
#             print(f"âœ… æˆåŠŸè§£æç‚º JSON é™£åˆ—ï¼Œå…± {len(parsed_conditions)} ç­†æ¢ä»¶")
#         except Exception as e:
#             print(f"âŒ JSON è§£æå¤±æ•—ï¼š{e}")
#             return f"âš ï¸ JSON decode error: {str(e)}"


#         if not isinstance(parsed_conditions, list):
#             print("âŒ è§£æçµæœé list æ ¼å¼")
#             return "âš ï¸ Invalid parsed result format (not a list)."

#         filters = [(c["field"], c["value"]) for c in parsed_conditions if c.get("field") in allowed_fields]

#         print("ğŸ” éæ¿¾å¾Œçš„æœ‰æ•ˆæ¢ä»¶ï¼š")
#         for f, v in filters:
#             print(f"  â€¢ {f} = {v}")

#         if not filters:
#             print("âš ï¸ æ²’æœ‰æ“·å–åˆ°æœ‰æ•ˆæ¢ä»¶")
#             return "âš ï¸ No valid filters extracted from the query."

#         # ç¯©é¸è³‡æ–™ï¼ˆç¬¦åˆæ‰€æœ‰æ¢ä»¶ï¼Œå¤§å°å¯«ä¸æ•æ„Ÿï¼Œç©ºç™½å®¹éŒ¯ï¼‰
#         def match_all(item):
#             for field, value in filters:
#                 actual = str(item.get(field, "")).strip().lower()
#                 expected = str(value).strip().lower()
#                 if expected not in actual:  # âœ… æ”¹ç‚ºæ¨¡ç³Šæ¯”å°
#                     return False
#             return True

#         matches = [item for item in metadata if match_all(item)]

#         print(f"ğŸ“Š ç¬¦åˆæ¢ä»¶çš„çµæœç­†æ•¸ï¼š{len(matches)}")

#         if not matches:
#             print("ğŸ“­ æŸ¥ç„¡çµæœ")
#             return f"ğŸ” No results found for: " + " AND ".join([f"{f}={v}" for f, v in filters])

#         lines = [f"- {item.get('text', '')[:500]}" for item in matches[:5]]
#         # ğŸ” å¾ matches ä¸­å–å‡ºå¯¦éš›å‘½ä¸­çš„åŸå§‹å€¼
#         actual_values = {field: set() for field, _ in filters}
#         for item in matches:
#             for field, _ in filters:
#                 val = item.get(field, "").strip()
#                 if val:
#                     actual_values[field].add(val)

#         summary_lines = [
#             f"â€¢ {field} = {', '.join(sorted(actual_values[field])) or 'N/A'}"
#             for field in actual_values
#         ]

#         return (
#             "ğŸ” Top matches for:\n" +
#             "\n".join(summary_lines) +
#             "\n\n" + "\n".join(lines)
#         )
    
#     except Exception as e:
#         print(f"âŒ å‘¼å«æ¨¡å‹æˆ–è§£æéç¨‹å‡ºéŒ¯ï¼š{str(e)}")
#         return f"âš ï¸ Failed to parse or search: {str(e)}"




# ----------- æ™‚é–“è¶¨å‹¢åˆ†æ -----------
# def analyze_temporal_trend(message):
#     print("ğŸ“ˆ å•Ÿå‹•æ™‚é–“è¶¨å‹¢åˆ†æ...")
#     print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print("ğŸ“¦ ç¬¬ä¸€ç­†è³‡æ–™ï¼š", metadata[0])
#         print("ğŸ” æ˜¯å¦æœ‰ analysisTime æ¬„ä½ï¼š", "analysisTime" in metadata[0])
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç¸½ç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print(f"âŒ metadata è¼‰å…¥å¤±æ•—ï¼š{str(e)}")
#         return f"âš ï¸ ç„¡æ³•è¼‰å…¥è³‡æ–™ï¼š{str(e)}"

#     if not metadata:
#         print("âš ï¸ metadata ç‚ºç©º")
#         return "âš ï¸ ç„¡è³‡æ–™å¯åˆ†æã€‚"

#     if "analysisTime" not in metadata[0]:
#         print("âš ï¸ analysisTime æ¬„ä½ä¸å­˜åœ¨")
#         return "âš ï¸ ç¼ºå°‘ analysisTime æ¬„ä½ï¼Œç„¡æ³•åˆ†æè¶¨å‹¢ã€‚"

#     print("ğŸ§ª é–‹å§‹è½‰æ›ç‚º DataFrame ä¸¦è™•ç†æ™‚é–“æ¬„ä½...")
#     df = pd.DataFrame(metadata)
#     print(f"ğŸ“Š DataFrame æ¬„ä½ï¼š{list(df.columns)}")

#     df["analysisTime"] = pd.to_datetime(df["analysisTime"], errors="coerce")
#     initial_len = len(df)
#     df = df.dropna(subset=["analysisTime"])
#     print(f"ğŸ§¹ è™•ç†ç„¡æ•ˆæ™‚é–“å¾Œï¼Œå‰©é¤˜ç­†æ•¸ï¼š{len(df)} / åŸå§‹ {initial_len}")

#     print("ğŸ“† å»ºç«‹æœˆä»½æ¬„ä½ä¸¦è¨ˆç®—æ¯æœˆæ•¸é‡...")
#     df["month"] = df["analysisTime"].dt.to_period("M")
#     trend = df.groupby("month").size()
#     print("ğŸ“Š æ¯æœˆçµ±è¨ˆçµæœï¼š")
#     for month, count in trend.items():
#         print(f"  â€¢ {month}: {count} ç­†")

#     # âœ… ç”¨æ–‡å­—æ•˜è¿°æ¯æœˆè¶¨å‹¢
#     summary_lines = ["ğŸ“Š æ¯æœˆæ¡ˆä»¶è¶¨å‹¢ï¼š"]
#     for month, count in trend.items():
#         summary_lines.append(f"- {month.strftime('%Y-%m')}: {count} ç­†")
#     print("âœ… å·²è½‰æ›ç‚ºç´”æ–‡å­—æè¿°")

#     return "\n".join(summary_lines)




# ----------- å„²å­˜æŸ¥è©¢ä¸Šä¸‹æ–‡ -----------
def save_query_context(chat_id, query, result_type, filter_info=None, result_summary=None):
    filepath = f"chat_history/{chat_id}.json"
    print(f"ğŸ“ å˜—è©¦è®€å–å°è©±è¨˜éŒ„æª”ï¼š{filepath}")

    # å˜—è©¦è®€å–å°è©±æ­·å²
    try:
        with open(filepath, encoding="utf-8") as f:
            history = json.load(f)
        if not isinstance(history, list):
            print("âš ï¸ è¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼ˆé listï¼‰ï¼Œé‡æ–°åˆå§‹åŒ–æ­·å²")
            history = []
        else:
            print(f"ğŸ“– æˆåŠŸè®€å–æ­·å²è¨˜éŒ„ï¼Œç¾æœ‰ç­†æ•¸ï¼š{len(history)}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è®€å–æ­·å²ï¼Œåˆå§‹åŒ–ç‚ºç©ºï¼š{e}")
        history = []

    # æº–å‚™ context å…§å®¹
    context = {
        "type": result_type,
        "query": query,
        "filters": filter_info,
        "summary": result_summary
    }
    print(f"ğŸ§  æº–å‚™å„²å­˜çš„ contextï¼š{context}")

    # è‹¥ç„¡æ­·å²ï¼Œå»ºç«‹ä½”ä½å°è©±
    if not history:
        print("ğŸ“Œ å°šç„¡å°è©±æ­·å²ï¼Œè‡ªå‹•æ–°å¢ä¸€å‰‡ä½”ä½è¨Šæ¯ä¸¦é™„åŠ  contextã€‚")
        history.append({
            "role": "user",
            "content": query,
            "context": context
        })
    else:
        print("ğŸ” å·²æœ‰æ­·å²ï¼Œå°‡ context å¯«å…¥æœ€å¾Œä¸€å‰‡å°è©±...")
        history[-1]["context"] = context

    # é¡¯ç¤ºå°‡è¦å¯«å…¥çš„å®Œæ•´æ­·å²
    print("ğŸ“¦ å³å°‡å„²å­˜çš„å®Œæ•´æ­·å²å…§å®¹é è¦½ï¼ˆæœ€å¾Œ 1 ç­†ï¼‰ï¼š")
    print(json.dumps(history[-1], ensure_ascii=False, indent=2))

    # å„²å­˜å› JSON æª”æ¡ˆ
    try:
        os.makedirs("chat_history", exist_ok=True)  # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²æˆåŠŸå¯«å…¥æª”æ¡ˆï¼š{filepath}")
    except Exception as e:
        print(f"âŒ å„²å­˜è¨˜æ†¶å¤±æ•—ï¼š{e}")



# ----------- å»¶ä¼¸æŸ¥è©¢è™•ç† ----------- (not completed)
def is_follow_up_query(message: str) -> bool:
    print("ğŸ§  åˆ¤æ–·æ˜¯å¦ç‚ºè¿½å•æŸ¥è©¢...")
    print(f"ğŸ“ è¼¸å…¥è¨Šæ¯ï¼š{message}")

    keywords = ["previous", "last query", "those", "add filter", "now show", "continue", "follow up"]
    lowered = message.lower()
    print(f"ğŸ” è½‰ç‚ºå°å¯«å¾Œè¨Šæ¯ï¼š{lowered}")

    for kw in keywords:
        if kw in lowered:
            print(f"âœ… å‘½ä¸­é—œéµå­—ï¼š'{kw}' â†’ åˆ¤å®šç‚ºè¿½å•æŸ¥è©¢")
            return True

    print("âŒ ç„¡é—œéµå­—å‘½ä¸­ â†’ éè¿½å•æŸ¥è©¢")
    return False



# è™•ç†è¿½å•æŸ¥è©¢ (not completed)
def handle_follow_up(chat_id, message):
    filepath = f"chat_history/{chat_id}.json"
    print(f"ğŸ“‚ å˜—è©¦è®€å–æ­·å²è¨˜éŒ„ï¼š{filepath}")

    try:
        with open(filepath, encoding="utf-8") as f:
            history = json.load(f)
        print(f"ğŸ“– æ­·å²ç­†æ•¸ï¼š{len(history)}")
    except Exception as e:
        print(f"âŒ æ­·å²è®€å–å¤±æ•—ï¼š{e}")
        return "âš ï¸ ç„¡æ³•è®€å–å…ˆå‰å°è©±è¨˜éŒ„ï¼Œè«‹ç¢ºèª chat_id æ˜¯å¦æ­£ç¢ºã€‚"

    if not history or "context" not in history[-1]:
        print("âš ï¸ æœ€å¾Œä¸€ç­†æ­·å²ç„¡ context æ¬„ä½")
        return "âš ï¸ æŸ¥ç„¡å…ˆå‰æŸ¥è©¢æ¢ä»¶ï¼Œè«‹é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚"

    context = history[-1]["context"]
    result_type = context.get("type")
    print(f"ğŸ§  ä¸Šæ¬¡æŸ¥è©¢é¡å‹ç‚ºï¼š{result_type}")

    if result_type == "Field Filter":
        original = context.get("filters", {})
        print(f"ğŸ” åŸå§‹éæ¿¾æ¢ä»¶ï¼š{original}")

        new_filter_prompt = (
            "You are a filter parser. Based on this message, extract an additional field and value to add as a filter.\n"
            "Return JSON like: {\"field\": \"subcategory\", \"value\": \"Crash\"}\n"
            "The entire response must be in compact JSON format and must not exceed 500 characters."
        )

        prompt = f"{new_filter_prompt}\n\nUser: {message}"
        print("ğŸ“¤ ç™¼é€çµ¦æ¨¡å‹çš„å»¶ä¼¸éæ¿¾ promptï¼š")
        print(prompt)

        try:
            print("ğŸ§  å‘¼å«æ¨¡å‹ phi3:mini è§£ææ–°å¢æ¢ä»¶...")
            result = subprocess.run(
                ["ollama", "run", "phi3:mini"],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )
            raw_reply = result.stdout.decode("utf-8").strip()
            print(f"ğŸ“¥ GPT å›è¦†ï¼š{raw_reply}")

            new_filter = json.loads(raw_reply)
            field = new_filter.get("field")
            value = new_filter.get("value")
            print(f"âœ… æ“·å–åˆ°æ¬„ä½ï¼š{field}ï¼Œå€¼ï¼š{value}")

            if field not in ["configurationItem", "subcategory", "roleComponent", "location"]:
                print("âš ï¸ æ¬„ä½ä¸åœ¨å…è¨±æ¸…å–®ä¸­")
                return "âš ï¸ ç„¡æ•ˆçš„æ¬„ä½"

            filters = [original] + [new_filter]
            print(f"ğŸ”— åˆä½µéæ¿¾æ¢ä»¶ï¼š{filters}")

            with open("kb_metadata.json", encoding="utf-8") as f:
                metadata = json.load(f)
            print(f"ğŸ“¦ è¼‰å…¥ metadataï¼Œç¸½ç­†æ•¸ï¼š{len(metadata)}")

            matches = metadata
            for f in filters:
                matches = [m for m in matches if m.get(f["field"]) == f["value"]]
            print(f"ğŸ“Š ç¬¦åˆæ¢ä»¶ç­†æ•¸ï¼š{len(matches)}")

            lines = [f"- {item.get('text', '')[:500]}" for item in matches[:5]]
            return f"ğŸ” å»¶ä¼¸æŸ¥è©¢çµæœï¼ˆå…± {len(matches)} ç­†ï¼‰ï¼š\n" + "\n".join(lines)

        except Exception as e:
            print(f"âŒ å»¶ä¼¸æŸ¥è©¢éŒ¯èª¤ï¼š{str(e)}")
            return f"âš ï¸ å»¶ä¼¸æŸ¥è©¢å¤±æ•—ï¼š{str(e)}"

    print("âš ï¸ ç›®å‰åƒ…æ”¯æ´ Field Filter é¡å‹çš„è¿½å•")
    return "âš ï¸ ç›®å‰åªæ”¯æ´æ¬„ä½ç¯©é¸çš„å»¶ä¼¸æŸ¥è©¢ã€‚"

# ----------- è§£æ³•çµ±æ•´ -----------
# def summarize_solutions(message):
#     print("ğŸ› ï¸ å•Ÿå‹•ç›¸ä¼¼æ¡ˆä¾‹è™•ç†æ–¹æ¡ˆæ‘˜è¦æµç¨‹...")
#     print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")

#     try:
#         with open("kb_metadata.json", encoding="utf-8") as f:
#             metadata = json.load(f)
#         print(f"ğŸ“‚ æˆåŠŸè¼‰å…¥ metadataï¼Œç­†æ•¸ï¼š{len(metadata)}")
#     except Exception as e:
#         print(f"âŒ metadata è¼‰å…¥å¤±æ•—ï¼š{str(e)}")
#         return f"âš ï¸ Failed to load metadata: {str(e)}"

#     print("ğŸ” é–‹å§‹èªæ„æ¯”å°ç›¸é—œæ¡ˆä¾‹...")
#     related_cases = search_knowledge_base(message, top_k=5)
#     print(f"ğŸ§  å–å¾—ç›¸ä¼¼ç‰‡æ®µæ•¸é‡ï¼š{len(related_cases)}")
#     for i, r in enumerate(related_cases, 1):
#         print(f"  {i}. {r[:60]}{'...' if len(r) > 60 else ''}")

#     if not related_cases:
#         print("âš ï¸ ç„¡ç›¸é—œæ¡ˆä¾‹")
#         return "âš ï¸ No similar cases found to extract solutions."

#     print("ğŸ“¦ æ“·å–ç›¸é—œæ¡ˆä¾‹ä¸­çš„è§£æ±ºæ–¹æ¡ˆ...")
#     solutions = []
#     for item in metadata:
#         text = item.get("text", "")
#         if any(snippet in text for snippet in related_cases):
#             solution = item.get("solution", "")
#             if solution:
#                 solutions.append(solution)

#     print(f"âœ… æ“·å–åˆ° solution æ•¸é‡ï¼š{len(solutions)}")
#     if not solutions:
#         print("âš ï¸ ç„¡å°æ‡‰è§£æ±ºæ–¹æ¡ˆæ¬„ä½")
#         return "âš ï¸ No resolution data found for related cases."

#     print("ğŸ“ çµ„è£ prompt é€²è¡Œ GPT çµ±æ•´...")
#     prompt = "Please summarize the following resolution steps into a brief, clear list:\n\n"
#     prompt += "\n---\n".join(solutions[:10])
#     prompt += "\n\nSummary:"

#     print("ğŸ“¤ ç™¼é€ prompt çµ¦æ¨¡å‹ï¼ˆå‰ 300 å­—ï¼‰ï¼š")
#     print(prompt[:300] + ("..." if len(prompt) > 300 else ""))

#     try:
#         result = subprocess.run(
#             ["ollama", "run", "phi4"],
#             input=prompt.encode("utf-8"),
#             capture_output=True,
#             timeout=600
#         )
#         output = result.stdout.decode("utf-8").strip()
#         print("âœ… æ¨¡å‹æˆåŠŸå›æ‡‰ï¼ˆå‰ 200 å­—ï¼‰ï¼š")
#         print(output[:200] + ("..." if len(output) > 200 else ""))
#         return output

#     except Exception as e:
#         print(f"âŒ æ¨¡å‹æ‘˜è¦å¤±æ•—ï¼š{str(e)}")
#         return f"âš ï¸ Failed to summarize solutions: {str(e)}"
    

def build_sql_prompt(user_question):
    schema_info = """
        You are an expert data analyst.

        You are working with the following SQLite table named 'metadata':

        Columns:
        - id (integer): internal ID
        - text (text): full case description, includes issue and solution
        - subcategory (text): issue type, such as 'Login', 'Teams', etc.
        - configurationItem (text): module or system component
        - roleComponent (text): affected user role or feature
        - location (text): site or region where issue occurred
        - analysisTime (text): ISO timestamp when the issue was recorded

        Please write an SQL query (SELECT ...) to answer the user's question.
        Return only the SQL query, no explanation or formatting.
        """

    prompt = f"{schema_info}\n\nUser question: {user_question}\nSQL:"
    return prompt


#ç”¢ç”Ÿ SQL æŸ¥è©¢èªå¥

def generate_sql_with_llm(prompt, model="deepseek-coder-v2:latest"):
    print("ğŸ§  å˜—è©¦ä½¿ç”¨ LLM ç”¢ç”Ÿ SQL æŸ¥è©¢èªå¥...")
    print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{prompt}")
    if not prompt.strip():
        print("âš ï¸ æç¤ºèªå¥ç‚ºç©ºï¼Œç„¡æ³•ç”¢ç”Ÿ SQL")
        return None
    try:
        print("ğŸš€ å‘¼å«æ¨¡å‹ç”¢ç”Ÿ SQL ä¸­...")
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt.encode("utf-8"),
            capture_output=True,
            timeout=600
        )

        if result.returncode != 0:
            err = result.stderr.decode("utf-8")
            print("âŒ æ¨¡å‹éŒ¯èª¤ï¼š", err)
            return None

        output = result.stdout.decode("utf-8").strip()
        print("ğŸ“¥ æ¨¡å‹ç”¢å‡ºï¼ˆå‰ 200 å­—ï¼‰ï¼š", output[:200])
        return output

    except Exception as e:
        print(f"âŒ å‘¼å« LLM å¤±æ•—ï¼š{str(e)}")
        return None



def extract_sql_code(text):
    print("ğŸ” å˜—è©¦å¾æ¨¡å‹å›æ‡‰ä¸­èƒå– SQL æŒ‡ä»¤...")

    # å˜—è©¦æŠ“ ```sql å€å¡Šï¼ˆä¿®æ­£ \\s -> \sï¼‰
    code_block_match = re.search(r"```sql\s*(.*?)```", text, re.DOTALL | re.IGNORECASE)
    if code_block_match:
        sql_code = code_block_match.group(1).strip()
        print("âœ… åµæ¸¬åˆ° ```sql å€å¡Šï¼ŒæˆåŠŸæŠ½å– SQLã€‚")
        return sql_code

    # å¦å‰‡æŠ“ç¬¬ä¸€æ®µ SELECT èªå¥ï¼ˆä¹Ÿä¿®æ­£ \\sï¼‰
    select_match = re.search(r"(SELECT\s.+?;)", text, re.IGNORECASE | re.DOTALL)
    if select_match:
        sql_code = select_match.group(1).strip()
        print("âœ… æˆåŠŸæŠ½å– SELECT é–‹é ­çš„ SQLã€‚")
        return sql_code

    # æœ€å¾Œ fallbackï¼šè©¦è‘—æŠ“æ•´æ®µåŒ…å« FROM çš„æ®µè½
    fallback_match = re.search(r"(SELECT.+FROM.+?)(\n|$)", text, re.IGNORECASE | re.DOTALL)
    if fallback_match:
        sql_code = fallback_match.group(1).strip()
        print("âš ï¸ å¾ fallback æŠ½å– SQL æˆåŠŸï¼ˆä½†å¯èƒ½ä¸å®Œæ•´ï¼‰ã€‚")
        return sql_code

    print("âš ï¸ ç„¡æ³•æŠ½å– SQLï¼ŒåŸå§‹è¼¸å‡ºå¦‚ä¸‹ï¼š")
    print(text[:300])
    return None




def run_sql(query):
    try:
        print("ğŸ” æ­£åœ¨é€£ç·šåˆ° SQLite è³‡æ–™åº«...")
        conn = sqlite3.connect(DB_PATH)
        print("ğŸ” æ­£åœ¨æŸ¥è©¢ SQLite è³‡æ–™åº«...")
        df = pd.read_sql_query(query, conn)
        conn.close()
        print(f"âœ… æŸ¥è©¢æˆåŠŸï¼Œå…± {len(df)} ç­†çµæœã€‚")
        return df
    except Exception as e:
        print("âŒ æŸ¥è©¢å¤±æ•—ï¼š", e)
        return None

# ---------- äººé¡æ‘˜è¦ ----------
def summarize_sql_result(df, max_rows=5):
    if df.empty:
        return "ğŸ“­ No data found."

    preview_df = df.head(max_rows).copy()
    for col in preview_df.columns:
        if preview_df[col].dtype == "object":
            # å°‡å­—é¢ä¸Šçš„ \nï¼ˆ\\nï¼‰è½‰ç‚ºçœŸæ­£æ›è¡Œ
            preview_df[col] = (
                preview_df[col]
                .astype(str)
                .str.replace(r'\\n', '\n')
                .str.slice(0, 200)
            )
    summary = f"ğŸ“Š Query successful. Total {len(df)} records found.<br>"
    summary += f"ğŸ“‹ Preview of first {min(max_rows, len(df))} records:<br>"
    preview = preview_df.to_string(index=False)
    print("DEBUG preview====>")
    print(repr(preview))  # é€™æ™‚ä½ æœƒçœ‹åˆ°å¤šè¡Œè³‡æ–™ï¼Œæ²’æœ‰å­—é¢ \n
    print("<====DEBUG preview")
    return summary + "```\n" + preview + "\n```"





def estimate_tokens_per_row(df):
    csv_text = df.to_csv(index=False)
    avg_len = len(csv_text) / len(df) if len(df) > 0 else 1
    # ç²—ç•¥æ¨ä¼°ï¼š1 token â‰ˆ 4 å€‹å­—å…ƒ
    return int(avg_len / 4)


def calculate_dynamic_chunk_size(df, model_name="phi4-mini", prompt_reserve_tokens=500):
    model_token_limits = {
        "phi4-mini": 4096,
        "phi3:mini": 4096,
        "mistral": 8192,
        "orca2": 8192,
        "deepseek-coder:latest": 16384,
        "orca2-13b": 8192,
        "deepseek-coder-v2:latest": 16384,
    }
    max_tokens = model_token_limits.get(model_name, 4096)
    tokens_per_row = estimate_tokens_per_row(df)
    available_tokens = max_tokens - prompt_reserve_tokens
    # ğŸ›¡ï¸ ç‚ºäº†è¼¸å‡ºå“è³ªï¼Œå†æ¸› 10 ç­†
    chunk_size = max(5, int(available_tokens / tokens_per_row)-10)
    return min(chunk_size, len(df))

def estimate_token_count(text):
    return len(text) // 4


def split_and_merge_summaries(summaries, primary_model="orca2:13b", token_limit=8192, prompt_reserve=500):
    fallback_models = ["nous-hermes2:10.7b", "mistral", "phi3:mini"]
    available_tokens = token_limit - prompt_reserve
    grouped = []
    group = []
    token_count = 0
 
    for s in summaries:
        s_tokens = estimate_token_count(s)
        if token_count + s_tokens > available_tokens:
            grouped.append(group)
            group = [s]
            token_count = s_tokens
        else:
            group.append(s)
            token_count += s_tokens
    if group:
        grouped.append(group)
 
    def run_with_model(m, prompt):
        try:
            result = subprocess.run(
                ["ollama", "run", m],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=300
            )
            if result.returncode == 0:
                return result.stdout.decode("utf-8").strip()
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {m} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None
 
    merged_chunks = []
    for i, group in enumerate(grouped, 1):
        merge_prompt = f"You are a data analyst. Please summarize the key points from the following group {i} of summaries:\n\n"
        for idx, s in enumerate(group, 1):
            merge_prompt += f"ï¼ˆæ‘˜è¦ {idx}ï¼‰{s}\n\n"
        merge_prompt += "Please consolidate the main observations:"
 
        # æ¨¡å‹è¼ªæ›¿å‘¼å«
        reply = run_with_model(primary_model, merge_prompt)
        for fallback in fallback_models:
            if reply:
                break
            print(f"ğŸ” ä½¿ç”¨ fallback æ¨¡å‹ï¼š{fallback}")
            reply = run_with_model(fallback, merge_prompt)
 
        merged_chunks.append(reply if reply else "âŒ æœ¬æ®µæ‘˜è¦å¤±æ•—")
 
    if len(merged_chunks) == 1:
        return f"ğŸ“Š GPT æ•´åˆæ‘˜è¦å¦‚ä¸‹ï¼š\n{merged_chunks[0]}"
    else:
        return split_and_merge_summaries(merged_chunks, primary_model, token_limit, prompt_reserve)




def summarize_sql_result_with_llm(df, model="orca2:13b"):
    print("ğŸ§  å˜—è©¦ä½¿ç”¨ LLM é€²è¡Œ SQL çµæœæ‘˜è¦...")
    print(f"ğŸ“ è³‡æ–™ç­†æ•¸ï¼š{len(df)}")
    if df.empty:
        return "ğŸ“­ æŸ¥ç„¡è³‡æ–™çµæœã€‚"

    chunk_size = calculate_dynamic_chunk_size(df, model)
    print(f"ğŸ“ é ä¼° chunk_size = {chunk_size} ç­†ï¼ˆæ¨¡å‹ï¼š{model}ï¼‰")

    chunk_summaries = []

    for i in range(0, len(df), chunk_size):
        chunk = df.iloc[i:i+chunk_size]
        sample_csv = chunk.to_csv(index=False)
        prompt = f"You are a data analyst. The following is data chunk {i//chunk_size+1}. Please summarize its characteristics and trends:\n\n{sample_csv}\n\nSummary:"

        try:
            result = subprocess.run(
                ["ollama", "run", model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )

            if result.returncode == 0:
                reply = result.stdout.decode("utf-8").strip()
                chunk_summaries.append(reply)
                print(f"âœ… ç¬¬ {i//chunk_size+1} æ®µå®Œæˆæ‘˜è¦")
            else:
                print(f"âš ï¸ ç¬¬ {i//chunk_size+1} æ®µæ‘˜è¦å¤±æ•—ï¼Œè·³é")

        except Exception as e:
            print(f"âŒ ç¬¬ {i//chunk_size+1} æ®µå‘¼å« LLM å¤±æ•—ï¼š{e}")

    if not chunk_summaries:
        return summarize_sql_result(df)
    
    # é–‹å§‹æ•´åˆ
    print("ğŸ§  é–‹å§‹æ•´åˆæ‰€æœ‰æ®µè½æ‘˜è¦...")

    merge_prompt = "You are a data analyst. Based on the following multiple summaries, please provide an overall conclusion:\n\n"
    for idx, s in enumerate(chunk_summaries, 1):
        merge_prompt += f"ï¼ˆç¬¬ {idx} æ®µæ‘˜è¦ï¼‰{s}\n\n"
    merge_prompt += "Please provide the key insights and observations:"

    def run_with_fallback(prompt, primary_model, fallback_model="deepseek-coder-v2:latest"):
        print(f"ğŸ” å˜—è©¦ä½¿ç”¨ä¸»æ¨¡å‹ {primary_model} é€²è¡Œæ•´åˆæ‘˜è¦...")
        try:
            result = subprocess.run(
                ["ollama", "run", primary_model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )
            if result.returncode == 0:
                return result.stdout.decode("utf-8").strip()
            else:
                print(f"âš ï¸ ä¸»æ¨¡å‹ {primary_model} å¤±æ•—ï¼Œå˜—è©¦ fallback æ¨¡å‹ {fallback_model}")
        except Exception as e:
            print(f"âŒ ä¸»æ¨¡å‹ {primary_model} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        # å˜—è©¦ fallback æ¨¡å‹
        try:
            print(f"ğŸ” ä½¿ç”¨ fallback æ¨¡å‹ {fallback_model} é€²è¡Œæ•´åˆæ‘˜è¦...")
            result = subprocess.run(
                ["ollama", "run", fallback_model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )
            if result.returncode == 0:
                return result.stdout.decode("utf-8").strip()
            else:
                print(f"âš ï¸ fallback æ¨¡å‹ {fallback_model} ä¹Ÿå¤±æ•—")
        except Exception as e:
            print(f"âŒ fallback æ¨¡å‹ {fallback_model} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        return None

    final_summary = run_with_fallback(merge_prompt, model)
    if final_summary:
        return f"ğŸ“Š GPT æ•´åˆæ‘˜è¦å¦‚ä¸‹ï¼š\n{final_summary}"
    else:
        print("âš ï¸ åˆä½µæ‘˜è¦å¤±æ•—ï¼Œå›å‚³å„æ®µæ‘˜è¦é›†åˆ")
        return "\n\n".join(chunk_summaries)




# ----------- GPT ä¸»å‡½å¼ -----------
def run_offline_gpt(message, model="mistral", history=[], chat_id=None):
    print("ğŸŸ¢ å•Ÿå‹• GPT å›ç­”æµç¨‹...")
    print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")
    print(f"ğŸ§  ä½¿ç”¨æ¨¡å‹ï¼š{model} / chat_id: {chat_id}")
    query_type = classify_query_type(message)
    print(f"ğŸ” åˆ¤æ–·çµæœï¼š{query_type}")




    if is_follow_up_query(message) and chat_id:
        print("ğŸ” åµæ¸¬ç‚ºè¿½å•æŸ¥è©¢ï¼Œè½‰äº¤ handle_follow_up è™•ç†...")
        return handle_follow_up(chat_id, message)
    
    if query_type == "Structured SQL":
        print("ğŸ§¾ é¡å‹ç‚º SQL çµæ§‹åŒ–æŸ¥è©¢ï¼Œé–‹å§‹ç”Ÿæˆ SQL...")
        refined_prompt = build_sql_prompt(message)
        raw_sql = generate_sql_with_llm(refined_prompt)
        sql_code = extract_sql_code(raw_sql)

        if not sql_code:
            return "âš ï¸ ç„¡æ³•å¾ LLM å›è¦†ä¸­æŠ½å–æœ‰æ•ˆçš„ SQL æŒ‡ä»¤ã€‚"

        df = run_sql(sql_code)
        if df is None or df.empty:
            return "ğŸ“­ æŸ¥ç„¡è³‡æ–™çµæœï¼Œè«‹èª¿æ•´æ¢ä»¶å¾Œå†è©¦ã€‚"

        summary = summarize_sql_result(df)
        summaryByLLM = summarize_sql_result_with_llm(df)

        combined_summary = (
            "ğŸ“‹ [ç³»çµ±æ‘˜è¦]\n" + summary.strip() +
            "\n\nğŸ§  [GPT æ‘˜è¦]\n" + summaryByLLM.strip()
        )

        save_query_context(chat_id, message, query_type, result_summary=combined_summary[:500])

        return f"{summary}\n\n{summaryByLLM}"



    # if query_type == "Statistical Analysis":
    #     print("ğŸ“Š é¡å‹ç‚ºçµ±è¨ˆåˆ†æï¼Œé–‹å§‹è™•ç†...")
    #     reply = analyze_metadata_query(message)
    #     print("ğŸ“¦ çµ±è¨ˆåˆ†æå®Œæˆï¼Œæ‘˜è¦å‰ 200 å­—ï¼š", reply[:200])
    #     save_query_context(chat_id, message, query_type, result_summary=reply[:200])
    #     return reply

    # if query_type == "Field Filter":
    #     print("ğŸ”„ é¡å‹ç‚ºæ¬„ä½éæ¿¾ï¼Œé–‹å§‹é€²è¡Œéæ¿¾...")
    #     reply = analyze_field_query(message)
    #     print("ğŸ“¦ éæ¿¾æŸ¥è©¢å®Œæˆï¼Œå‰ 200 å­—ï¼š", reply[:200])
    #     filters = []
    #     for line in reply.splitlines():
    #         if line.strip().startswith("â€¢ "):
    #             try:
    #                 field_part = line.replace("â€¢", "").strip()
    #                 field, value = field_part.split("=", 1)
    #                 filters.append({"field": field.strip(), "value": value.strip()})
    #             except Exception as e:
    #                 print(f"âš ï¸ ç„¡æ³•è§£ææ¢ä»¶è¡Œï¼š{line}ï¼ŒéŒ¯èª¤ï¼š{e}")
    #                 continue
    #     print("ğŸ§¾ æ“·å–åˆ°çš„ filtersï¼š", filters)
    #     save_query_context(chat_id, message, query_type, filter_info=filters if filters else None, result_summary=reply[:200])
    #     return reply

    # if query_type == "Field Values":
    #     print("ğŸ“‹ é¡å‹ç‚ºæ¬„ä½å€¼æ¸…å–®ï¼Œé–‹å§‹åˆ—å‡ºæ¬„ä½å€¼...")
    #     reply = list_field_values(message)
    #     print("ğŸ“¦ æ¬„ä½å€¼æŸ¥è©¢å®Œæˆï¼Œå‰ 200 å­—ï¼š", reply[:200])
    #     save_query_context(chat_id, message, query_type, result_summary=reply[:200])
    #     return reply

    # if query_type == "Temporal Trend":
    #     print("ğŸ“ˆ é¡å‹ç‚ºæ™‚é–“è¶¨å‹¢æŸ¥è©¢ï¼Œé–‹å§‹ç¹ªè£½åœ–è¡¨...")
    #     reply = analyze_temporal_trend(message)
    #     print("ğŸ“¦ è¶¨å‹¢åœ–å®Œæˆï¼ˆHTML ç‰‡æ®µï¼‰")
    #     save_query_context(chat_id, message, query_type, result_summary=reply[:200])
    #     return reply

    # if query_type == "Solution Summary":
    #     print("ğŸ›  é¡å‹ç‚ºè§£æ³•çµ±æ•´ï¼Œé–‹å§‹å½™æ•´è™•ç†æ–¹å¼...")
    #     reply = summarize_solutions(message)
    #     print("ğŸ“¦ è§£æ³•çµ±æ•´å®Œæˆï¼Œå‰ 200 å­—ï¼š", reply[:200])
    #     save_query_context(chat_id, message, query_type, result_summary=reply[:200])
    #     return reply

    # é è¨­ç‚º Semantic Query
    print("ğŸ”„ é¡å‹ç‚ºèªæ„æŸ¥è©¢ï¼Œé–‹å§‹æª¢ç´¢çŸ¥è­˜åº«...")
    
    # âœ… å‹•æ…‹æ±ºå®š top_k ç­†æ•¸ï¼ˆé è¨­ fallback=3ï¼‰
    top_k = determine_top_k_with_llm(message, fallback=3)
    retrieved = search_knowledge_base(message, top_k=top_k)
    if retrieved:
        print(f"[RAG] âœ… æ‰¾åˆ° {len(retrieved)} ç­†ç›¸ä¼¼è³‡æ–™ï¼š")
        for i, chunk in enumerate(retrieved, 1):
            preview = chunk.replace('\n', ' ')[:100]
            print(f"    {i}. {preview}...")
    else:
        print("[RAG] âš ï¸ æœªæ‰¾åˆ°ç›¸ä¼¼è³‡æ–™")

    print(f"[ğŸ”§ å£“ç¸®ç”¨æ¨¡å‹] ä½¿ç”¨æ¨¡å‹ï¼š orca2:13b")
    print(f"[ğŸ¯ å›ç­”ç”¨æ¨¡å‹] ä½¿ç”¨æ¨¡å‹ï¼š{model}")

    kb_context = summarize_retrieved_kb(retrieved, model="orca2:13b")
    print("ğŸ“š çŸ¥è­˜åº«æ‘˜è¦å®Œæˆ")

    # çµ„åˆå°è©±æ­·å²
    context = ""
    if not isinstance(history, list):
        print("âš ï¸ å°è©±æ­·å²æ ¼å¼éŒ¯èª¤ï¼Œåˆå§‹åŒ–ç‚ºç©º list")
        history = []
    for turn in history[-5:]:
        role = "User" if turn["role"] == "user" else "Assistant"
        context += f"{role}: {turn['content']}\n"

    prompt = f"{kb_context}\n\n{context}User: {message}\nAssistant:"
    print("\n[Prompt Preview] ğŸ§¾ ç™¼é€çµ¦æ¨¡å‹çš„ Prompt å‰ 300 å­—ï¼š")
    print(prompt[:300] + ("..." if len(prompt) > 300 else ""))

    try:
        print("ğŸš€ ç™¼é€ prompt çµ¦æ¨¡å‹ä¸­...")
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt.encode("utf-8"),
            capture_output=True,
            timeout=600
        )

        if result.returncode != 0:
            err = result.stderr.decode('utf-8')
            print("âŒ æ¨¡å‹éŒ¯èª¤ï¼š", err)
            return f"âš ï¸ Ollama éŒ¯èª¤ï¼š{err}"

        reply = result.stdout.decode("utf-8").strip()
        print("ğŸ“¥ æ¨¡å‹å›è¦†ï¼ˆå‰ 300 å­—ï¼‰ï¼š")
        print(reply[:300] + ("..." if len(reply) > 300 else ""))

        save_query_context(chat_id, message, query_type, result_summary=reply[:200])
        return reply if reply else "âš ï¸ æ²’æœ‰æ”¶åˆ°æ¨¡å‹å›æ‡‰ã€‚"

    except Exception as e:
        print(f"âŒ å‘¼å«æ¨¡å‹å¤±æ•—ï¼š{str(e)}")
        return f"âš ï¸ å‘¼å«æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
