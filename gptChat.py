

import subprocess
import os
import pickle
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd
import matplotlib.pyplot as plt
import re
import io
import base64
import sqlite3
import requests
from agents.sql_agent import SQLAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from agents.semantic_agent import SemanticAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from agents.query_classifier_agent import QueryClassifierAgent
from agents.followup_agent import FollowUpAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from utils.kb_loader import load_kb  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½å‡½å¼
import json
import hashlib
from autogen_core.tools import FunctionTool
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_core import ClosureAgent,ClosureContext,DefaultSubscription, MessageContext, SingleThreadedAgentRuntime
from datetime import datetime

# ----------- å…¨åŸŸè¨­å®š -----------
POWERAUTOMATE_URL = "https://prod-08.southeastasia.logic.azure.com:443/workflows/a9de89a708674755923e900665994521/triggers/manual/paths/invoke?api-version=2016-06-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=Eo8rgY9JHLAqYDQCYTjWYiufiHq3LYQ_kZXWmGjqLhw"  # ğŸ” è«‹æ›æˆä½ çš„å¯¦éš›ç¶²å€

DB_PATH = "resultDB.db"  # ä½ åœ¨ build_kb.py è£¡è¨­å®šçš„ DB åç¨±
# âœ… è¼‰å…¥ metadataï¼ˆè‹¥ä¸å­˜åœ¨å‰‡ç‚ºç©ºï¼‰
metadata_path = "kb_metadata.json"

metadata_path = "kb_metadata.json"
if os.path.exists(metadata_path):
    with open(metadata_path, encoding="utf-8") as f:
        metadata = json.load(f)
    print("ğŸ“¦ æˆåŠŸè¼‰å…¥ kb_metadata.json")
else:
    metadata = []
    print("âš ï¸ æ‰¾ä¸åˆ° kb_metadata.jsonï¼Œå°‡ä½¿ç”¨ç©º metadata")

# âœ… å»ºç«‹ FAISS ID â†’ åŸå§‹æ–‡å­—å¥å­ çš„å°ç…§è¡¨
faiss_id_to_text = {
    int(hashlib.sha256(item["id"].encode("utf-8")).hexdigest(), 16) % (1 << 63): item["text"]
    for item in metadata if "id" in item and "text" in item
}

kb_model, kb_index, kb_texts = load_kb() # è¼‰å…¥çŸ¥è­˜åº«æ¨¡å‹ã€ç´¢å¼•å’Œæ–‡æœ¬
classifier = QueryClassifierAgent() # âœ… åˆå§‹åŒ–æŸ¥è©¢åˆ†é¡ä»£ç†
sql_agent = SQLAgent() # âœ… åˆå§‹åŒ– SQLAgent
semantic_agent = SemanticAgent(
    kb_model=kb_model, 
    kb_index=kb_index, 
    kb_texts=kb_texts, 
    metadata=metadata, 
    faiss_id_to_text=faiss_id_to_text
) # âœ… åˆå§‹åŒ–èªæ„ä»£ç†
followup_agent = FollowUpAgent()  # âœ… åˆå§‹åŒ–è¿½å•ä»£ç†



# ----------- ä»¥ä¸‹é–‹å§‹å˜—è©¦ç”¨AutoGen -----------

# å…¨åŸŸè®Šæ•¸ï¼ˆæˆ–è¨­ä¸€å€‹ class propertyï¼‰
_last_autogen_result = None


def semantic_agent_handle(message: str) -> str:
    """
    Invokes the SemanticAgent to perform semantic retrieval and synthesis based on the user's natural language question.

    This function accepts a single message as input and:
      - Encodes the question using a sentence-transformer model.
      - Searches a FAISS vector index and SQLite database for the most relevant historic case records, dynamically determining the number of matches to retrieve based on query specificity.
      - Groups and summarizes multiple retrieved records using a local LLM (with automatic model fallback and chunked/recursive summarization to handle token limits).
      - Returns a synthesized summary of key findings, typical solutions, or recurring incident patterns in plain text.

    Use this function when the user request is in natural language and requires semantic understanding, case similarity search, incident clustering, or aggregation of unstructured problem/solution experiences.

    Parameters:
        message (str): The user's question or problem statement in natural language.

    Returns:
        str: An LLM-generated summary or best-practice overview synthesized from the most relevant historic cases.

    Example:
        reply = semantic_agent_handle("What are common solutions for Teams connectivity issues in the past six months?")
        print(reply)
    """
    global _last_autogen_result
    result =  semantic_agent.handle(message)
    _last_autogen_result = result  # å„²å­˜æœ€å¾Œä¸€æ¬¡çµæœ
    print(f"ğŸ” SemanticAgent è™•ç†çµæœï¼š{result[:1000]}{'...' if len(result) > 1000 else ''}")
    return result


def sql_agent_handle(message: str) -> str:
    """
    Invokes the SQLAgent to handle structured data requests based on a user's natural language query.

    This function:
      - Uses an LLM to decompose the user's question into a SQL query prompt and an analysis prompt.
      - Generates an appropriate SQL query to aggregate or summarize data from a SQLite database of case records.
      - Executes the SQL query and retrieves structured results as a pandas DataFrame.
      - Optionally summarizes large query results in multiple chunks using an LLM, then recursively merges these summaries for a concise overview.
      - Returns a combined human-readable summary and a model-generated insight covering statistics, trends, or patterns found in the data.

    Use this function for queries involving statistics, record counts, field-based filtering, database aggregation, or when the user's intent is to obtain structured, tabular, or quantitative information.

    Parameters:
        message (str): The user's question or command describing the data requirement in natural language.

    Returns:
        str: Combined summary and LLM-generated analysis of the queried data, suitable for direct presentation or reporting.

    Example:
        reply = sql_agent_handle("Show the number of incidents by subcategory for the past year")
        print(reply)
    """
    global _last_autogen_result
    # å‘¼å« SQLAgent è™•ç† SQL æŸ¥è©¢
    result = sql_agent.handle(message)
    _last_autogen_result = result  # å„²å­˜æœ€å¾Œä¸€æ¬¡çµæœ
    print(f"ğŸ” SQLAgent è™•ç†çµæœï¼š{result[:1000]}{'...' if len(result) > 1000 else ''}")
    return result


def get_last_autogen_result():
    return _last_autogen_result


sql_tool = FunctionTool(
    func=sql_agent_handle,
    name="SQLAgentTool",
    description="Handles requests for structured data, such as statistical summaries, filtering records by specific fields, or generating tables from the database."
)

semantic_tool = FunctionTool(
    func=semantic_agent_handle,
    name="SemanticAgentTool",
    description="Handles requests stated in natural language to retrieve similar past incidents, summarize multiple case solutions, or provide insights based on previous experience."
)


# AutoGen Agent è¨­å®š
# é€™è£¡ä½¿ç”¨ OllamaChatCompletionClient ä½œç‚ºæ¨¡å‹å®¢æˆ¶ç«¯
ollama_client = OllamaChatCompletionClient(
    model="mistral-nemo:latest"
)

classifier_agent = AssistantAgent(
    name="Classifier",
    model_client=ollama_client,
    tools=[sql_tool, semantic_tool],
    system_message=(
            "You are a classification assistant. Your task is to analyze the user's question and classify it as one of the following two types:\n\n"
            "1. Semantic Query â€“ The user is seeking similar past incidents, solution suggestions, or insights based on previous case knowledge.\n"
            "   Typical intents: find related issues, ask how a problem was resolved, request examples of solutions.\n\n"
            "2. Structured SQL â€“ The user is requesting structured or statistical data, such as record counts, field value filtering, time-based trends, or aggregated summaries.\n"
            "   Typical intents: show number of records, list unique values, filter by conditions, summarize results over time.\n\n"
            "Respond with exactly one of the following labels (no explanations):\n"
            "'SemanticAgentTool' or 'SQLAgentTool'."
    )
)

# ç”¨ UserProxy ä»£è¡¨äººé¡ä½¿ç”¨è€…
user_proxy = UserProxyAgent(name="user_proxy")
team = RoundRobinGroupChat([classifier_agent, user_proxy], max_turns=2)


async def autogen_dispatch(message):
    # ç›´æ¥ä¸Ÿè¨Šæ¯çµ¦ classifier_agentï¼Œè®“ä»–è‡ªå‹•åˆ†é¡ä¸¦èª¿ç”¨å·¥å…·
    stream = classifier_agent.run_stream(
        task=message,
    )
    # result.summary å°±æ˜¯ SQLAgent æˆ– SemanticAgent çš„å›å‚³å…§å®¹
    await Console(stream)  # é¡¯ç¤ºå°è©±éç¨‹ï¼ˆå¦‚éœ€è¦ï¼‰

    return get_last_autogen_result()  # å›å‚³æœ€å¾Œä¸€æ¬¡çš„çµæœ

# ----------- AutoGen Agent è¨­å®šçµæŸ -----------

# -----------------------------------------------------------------------------------------------------------
# ----------- å„²å­˜æŸ¥è©¢ä¸Šä¸‹æ–‡ -----------

def save_query_context(chat_id, query, result_type, filter_info=None, result_summary=None):
    filepath = f"chat_history/{chat_id}.json"
    print(f"ğŸ“ å˜—è©¦è®€å–å°è©±è¨˜éŒ„æª”ï¼š{filepath}")

    try:
        with open(filepath, encoding="utf-8") as f:
            chat_data = json.load(f)

        if not isinstance(chat_data, dict) or "history" not in chat_data or not isinstance(chat_data["history"], list):
            print("âš ï¸ chat è¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼Œé‡æ–°åˆå§‹åŒ–")
            chat_data = {
                "id": chat_id,
                "title": chat_id,
                "edit_title": "",
                "model": "unknown",
                "timestamp": datetime.now().isoformat(),
                "history": []
            }
        else:
            print(f"ğŸ“– æˆåŠŸè®€å–æ­·å²è¨˜éŒ„ï¼Œç¾æœ‰ç­†æ•¸ï¼š{len(chat_data['history'])}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è®€å–æ­·å²ï¼Œåˆå§‹åŒ–ç‚ºç©ºï¼š{e}")
        chat_data = {
            "id": chat_id,
            "title": chat_id,
            "edit_title": "",
            "model": "unknown",
            "timestamp": datetime.now().isoformat(),
            "history": []
        }

    context = {
        "type": result_type,
        "query": query,
        "filters": filter_info,
        "summary": result_summary
    }
    print(f"ğŸ§  æº–å‚™å„²å­˜çš„ contextï¼š{context}")

    if not chat_data["history"]:
        print("ğŸ“Œ å°šç„¡å°è©±æ­·å²ï¼Œè‡ªå‹•æ–°å¢ä¸€å‰‡ä½”ä½è¨Šæ¯ä¸¦é™„åŠ  contextã€‚")
        chat_data["history"].append({
            "role": "user",
            "content": query,
            "context": context
        })
    else:
        print("ğŸ” å·²æœ‰æ­·å²ï¼Œå°‡ context å¯«å…¥æœ€å¾Œä¸€å‰‡å°è©±...")
        chat_data["history"][-1]["context"] = context

    print("ğŸ“¦ å³å°‡å„²å­˜çš„å®Œæ•´æ­·å²å…§å®¹é è¦½ï¼ˆæœ€å¾Œ 1 ç­†ï¼‰ï¼š")
    print(json.dumps(chat_data["history"][-1], ensure_ascii=False, indent=2))

    try:
        os.makedirs("chat_history", exist_ok=True)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(chat_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²æˆåŠŸå¯«å…¥æª”æ¡ˆï¼š{filepath}")
    except Exception as e:
        print(f"âŒ å„²å­˜è¨˜æ†¶å¤±æ•—ï¼š{e}")

# -----------------------------------------------------------------------------------------------------------

async def run_offline_gpt(message, model="orca2:13b", history=[], chat_id=None):
    print("ğŸŸ¢ å•Ÿå‹• GPT å›ç­”æµç¨‹...")
    print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")
    print(f"ğŸ§  chat_id: {chat_id}")

    if followup_agent.is_follow_up(message) and chat_id:
        print("ğŸ” åµæ¸¬ç‚ºè¿½å•æŸ¥è©¢ï¼Œè½‰äº¤ FollowUpAgent è™•ç†...")
        return followup_agent.handle(chat_id, message)

    try:
        kb_context = await autogen_dispatch(message)
    except Exception as e:
        print(f"âŒ autogen_dispatch ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        kb_context = ""

    print("ğŸ§  çŸ¥è­˜åº«æ‘˜è¦è™•ç†å®Œæˆ")
    print(f"ğŸ“š çŸ¥è­˜åº«æ‘˜è¦ï¼ˆå‰ 1000 å­—ï¼‰ï¼š{kb_context[:1000]}{'...' if len(kb_context) > 1000 else ''}")

    # æ•´ç†å°è©±æ­·å²
    context = ""
    if not isinstance(history, list):
        print("âš ï¸ å°è©±æ­·å²æ ¼å¼éŒ¯èª¤ï¼Œåˆå§‹åŒ–ç‚ºç©º list")
        history = []
    for turn in history[-2:]:
        role = "User" if turn["role"] == "user" else "Assistant"
        context += f"{role}: {turn['content']}\n"

    # å»ºç«‹ PowerAutomate å°ˆç”¨ Promptï¼ˆåˆ†é–‹ template + userInputï¼‰
    template = (
        "You are a knowledgeable helpdesk assistant. "
        "You will first review some relevant case entries, then answer the user's question based on context and your reasoning.\n\n"
        "Please limit your answer to no more than 1000 English words.\n\n"
        "==== [Retrieved Knowledge Base Entries] ====\n"
    )

    user_input_section = (
        f"{kb_context.strip()}\n\n"
        "==== [Conversation History] ====\n"
        f"{context.strip()}\n"
        "==== [User's Current Question] ====\n"
        f"User: {message}\n"
        "==== [Your Response] ====\n"
        "Assistant:"
    )

    full_prompt = template + user_input_section

    print("\n[Prompt Preview] ğŸ§¾ ç™¼é€çµ¦æ¨¡å‹çš„ Prompt å‰ 5000 å­—ï¼š")
    print(full_prompt[:5000] + ("..." if len(full_prompt) > 5000 else ""))
    print("testing")

    try:
        print("ğŸ“¡ å‚³é€ prompt è‡³ Power Automate...")
        payload = {
            "template": template,
            "userInput": user_input_section
        }
        res = requests.post(POWERAUTOMATE_URL, json=payload, timeout=360)
        if res.status_code == 200:
            reply = res.json().get("response", "").strip()
            print("ğŸ“¥ AI Builder å›æ‡‰ï¼ˆå‰ 1000 å­—ï¼‰ï¼š")
            print(reply[:1000] + ("..." if len(reply) > 1000 else ""))
            if reply:
                save_query_context(chat_id, message, "RAG-Integrated", result_summary=reply)
                return reply
            else:
                print("âš ï¸ AI Builder å›æ‡‰ç‚ºç©ºï¼Œå°‡é€²è¡Œ fallback")
        else:
            print(f"âŒ AI Builder å›æ‡‰éŒ¯èª¤ï¼Œç‹€æ…‹ç¢¼ï¼š{res.status_code}")
    except Exception as e:
        print(f"âŒ AI Builder å‘¼å«å¤±æ•—æˆ–é€¾æ™‚ï¼š{e}")

    # âœ… fallbackï¼šæœ¬åœ° ollama æ¨¡å‹æ¨è«–
    try:
        print("ğŸš€ ç™¼é€ prompt çµ¦æœ¬åœ°æ¨¡å‹ä¸­...")
        result = subprocess.run(
            ["ollama", "run", model],
            input=full_prompt.encode("utf-8"),
            capture_output=True,
            timeout=600
        )
        if result.returncode != 0:
            err = result.stderr.decode('utf-8')
            print("âŒ æ¨¡å‹éŒ¯èª¤ï¼š", err)
            return f"âš ï¸ Ollama éŒ¯èª¤ï¼š{err}"

        reply = result.stdout.decode("utf-8").strip()
        print("ğŸ“¥ æ¨¡å‹å›è¦†ï¼ˆå‰ 1000 å­—ï¼‰ï¼š")
        print(reply[:1000] + ("..." if len(reply) > 1000 else ""))

        save_query_context(chat_id, message, "RAG-Integrated", result_summary=reply)
        print("------------------------------------------------------------------çµæŸè™•ç†---------------------------------------------------------------------------------")
        return reply if reply else "âš ï¸ æ²’æœ‰æ”¶åˆ°æ¨¡å‹å›æ‡‰ã€‚"

    except Exception as e:
        print(f"âŒ å‘¼å«æ¨¡å‹å¤±æ•—ï¼š{str(e)}")
        return f"âš ï¸ å‘¼å«æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"



