import subprocess
import os
import pickle
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd
import matplotlib.pyplot as plt
import re
import io
import base64
import sqlite3
import requests
from agents.sql_agent import SQLAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from agents.semantic_agent import SemanticAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from agents.query_classifier_agent import QueryClassifierAgent
from agents.followup_agent import FollowUpAgent  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½ class
from utils.kb_loader import load_kb  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½å‡½å¼
import json
# ----------- å…¨åŸŸè¨­å®š -----------
DB_PATH = "resultDB.db"  # ä½ åœ¨ build_kb.py è£¡è¨­å®šçš„ DB åç¨±
kb_model, kb_index, kb_texts = load_kb() # è¼‰å…¥çŸ¥è­˜åº«æ¨¡å‹ã€ç´¢å¼•å’Œæ–‡æœ¬
classifier = QueryClassifierAgent() # âœ… åˆå§‹åŒ–æŸ¥è©¢åˆ†é¡ä»£ç†
sql_agent = SQLAgent() # âœ… åˆå§‹åŒ– SQLAgent
semantic_agent = SemanticAgent(kb_model=kb_model, kb_index=kb_index, kb_texts=kb_texts) # âœ… åˆå§‹åŒ–èªæ„ä»£ç†
followup_agent = FollowUpAgent()  # âœ… åˆå§‹åŒ–è¿½å•ä»£ç†

# ----------- å„²å­˜æŸ¥è©¢ä¸Šä¸‹æ–‡ -----------
def save_query_context(chat_id, query, result_type, filter_info=None, result_summary=None):
    filepath = f"chat_history/{chat_id}.json"
    print(f"ğŸ“ å˜—è©¦è®€å–å°è©±è¨˜éŒ„æª”ï¼š{filepath}")

    # å˜—è©¦è®€å–å°è©±æ­·å²
    try:
        with open(filepath, encoding="utf-8") as f:
            history = json.load(f)
        if not isinstance(history, list):
            print("âš ï¸ è¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼ˆé listï¼‰ï¼Œé‡æ–°åˆå§‹åŒ–æ­·å²")
            history = []
        else:
            print(f"ğŸ“– æˆåŠŸè®€å–æ­·å²è¨˜éŒ„ï¼Œç¾æœ‰ç­†æ•¸ï¼š{len(history)}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è®€å–æ­·å²ï¼Œåˆå§‹åŒ–ç‚ºç©ºï¼š{e}")
        history = []

    # æº–å‚™ context å…§å®¹
    context = {
        "type": result_type,
        "query": query,
        "filters": filter_info,
        "summary": result_summary
    }
    print(f"ğŸ§  æº–å‚™å„²å­˜çš„ contextï¼š{context}")

    # è‹¥ç„¡æ­·å²ï¼Œå»ºç«‹ä½”ä½å°è©±
    if not history:
        print("ğŸ“Œ å°šç„¡å°è©±æ­·å²ï¼Œè‡ªå‹•æ–°å¢ä¸€å‰‡ä½”ä½è¨Šæ¯ä¸¦é™„åŠ  contextã€‚")
        history.append({
            "role": "user",
            "content": query,
            "context": context
        })
    else:
        print("ğŸ” å·²æœ‰æ­·å²ï¼Œå°‡ context å¯«å…¥æœ€å¾Œä¸€å‰‡å°è©±...")
        history[-1]["context"] = context

    # é¡¯ç¤ºå°‡è¦å¯«å…¥çš„å®Œæ•´æ­·å²
    print("ğŸ“¦ å³å°‡å„²å­˜çš„å®Œæ•´æ­·å²å…§å®¹é è¦½ï¼ˆæœ€å¾Œ 1 ç­†ï¼‰ï¼š")
    print(json.dumps(history[-1], ensure_ascii=False, indent=2))

    # å„²å­˜å› JSON æª”æ¡ˆ
    try:
        os.makedirs("chat_history", exist_ok=True)  # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²æˆåŠŸå¯«å…¥æª”æ¡ˆï¼š{filepath}")
    except Exception as e:
        print(f"âŒ å„²å­˜è¨˜æ†¶å¤±æ•—ï¼š{e}")




# ----------- GPT ä¸»å‡½å¼ -----------
def run_offline_gpt(message, model="orca2:13b", history=[], chat_id=None):
    print("ğŸŸ¢ å•Ÿå‹• GPT å›ç­”æµç¨‹...")
    print(f"ğŸ“ ä½¿ç”¨è€…è¼¸å…¥ï¼š{message}")
    print(f"ğŸ§  chat_id: {chat_id}")
    print("------------------------------------------------------------------é–‹å§‹åˆ†é¡---------------------------------------------------------------------------------")
    query_type = classifier.handle(message)
    print("------------------------------------------------------------------çµæŸåˆ†é¡---------------------------------------------------------------------------------")
    print(f"ğŸ” åˆ¤æ–·çµæœï¼š{query_type}")
    # å¦‚æœæ˜¯è¿½å•æŸ¥è©¢ï¼Œç›´æ¥è½‰äº¤è™•ç†(å°šæœªå®Œæˆ) æ‡‰è©²è¦ä½µåˆ° classify_query_type è£¡é¢
    # é€™è£¡å‡è¨­è¿½å•æŸ¥è©¢æœƒæœ‰ chat_idï¼Œå¦å‰‡ç„¡æ³•æ‰¾åˆ°å°æ‡‰çš„æ­·å²è¨˜éŒ„
    # åœ¨ run_offline_gpt è£¡é¢é€™æ®µæ”¹å¯«ï¼š
    print("------------------------------------------------------------------é–‹å§‹åˆ¤æ–·æ˜¯å¦ç‚ºè¿½å•æŸ¥è©¢---------------------------------------------------------------------------------")
    if followup_agent.is_follow_up(message) and chat_id:
        print("ğŸ” åµæ¸¬ç‚ºè¿½å•æŸ¥è©¢ï¼Œè½‰äº¤ FollowUpAgent è™•ç†...")
        return followup_agent.handle(chat_id, message)
    print("------------------------------------------------------------------çµæŸåˆ¤æ–·æ˜¯å¦ç‚ºè¿½å•æŸ¥è©¢---------------------------------------------------------------------------------")
    # å¦‚æœæ˜¯sql çµæ§‹åŒ–æŸ¥è©¢ï¼Œèµ°sqlæŸ¥è©¢æµç¨‹
    print("------------------------------------------------------------------é–‹å§‹åˆ¤æ–·æ˜¯å¦ç‚º SQL çµæ§‹åŒ–æŸ¥è©¢---------------------------------------------------------------------------------")
    if query_type == "Structured SQL":        
        print("ğŸ§¾ é¡å‹ç‚º SQL çµæ§‹åŒ–æŸ¥è©¢ï¼Œæ”¹ç”± SQLAgent è™•ç†...")
        sql_agent.set_model("deepseek-coder-v2:latest")  # âœ… è¨­å®š SQLAgent ä½¿ç”¨çš„æ¨¡å‹
        print("ğŸ§  ä½¿ç”¨ SQLAgent è™•ç†æŸ¥è©¢...")
        reply = sql_agent.handle(message)
        print(f"ğŸ“¥ SQLAgent å›è¦†ï¼ˆå‰ 500 å­—ï¼‰ï¼š{reply[:500]}{'...' if len(reply) > 500 else ''}")
        if not reply:
            print("âš ï¸ SQLAgent å›è¦†ç‚ºç©ºï¼Œå¯èƒ½æ˜¯æŸ¥è©¢èªå¥ç”Ÿæˆå¤±æ•—")
            return "âš ï¸ ç„¡æ³•ç”Ÿæˆæœ‰æ•ˆçš„ SQL æŸ¥è©¢èªå¥ã€‚è«‹æª¢æŸ¥æ‚¨çš„å•é¡Œæè¿°ã€‚"
        print("------------------------------------------------------------------çµæŸ SQL çµæ§‹åŒ–æŸ¥è©¢è™•ç†---------------------------------------------------------------------------------")
        # âœ… è¨˜å¾—å­˜å›æŸ¥è©¢ç´€éŒ„
        save_query_context(chat_id, message, query_type, result_summary=reply[:500])
        return reply
    # é è¨­ç‚º Semantic Query
    print("------------------------------------------------------------------é–‹å§‹åˆ¤æ–·æ˜¯å¦ç‚ºèªæ„æŸ¥è©¢---------------------------------------------------------------------------------")
    semantic_agent.model = model  # âœ… å‹•æ…‹æŒ‡å®šä½¿ç”¨è€…ç•¶å‰é¸æ“‡çš„æ¨¡å‹
    print("ğŸ”„ é¡å‹ç‚ºèªæ„æŸ¥è©¢ï¼Œæ”¹ç”± SemanticAgent è™•ç†...")
    kb_context = semantic_agent.handle(message)
    print("ğŸ§  çŸ¥è­˜åº«æ‘˜è¦è™•ç†å®Œæˆ")
    print(f"ğŸ“š çŸ¥è­˜åº«æª¢ç´¢çµæœç­†æ•¸ï¼š{len(kb_context)}")
    print(f"ğŸ“š çŸ¥è­˜åº«æ‘˜è¦ï¼ˆå‰ 300 å­—ï¼‰ï¼š{kb_context[:300]}{'...' if len(kb_context) > 300 else ''}")
    if not kb_context:
        print("âš ï¸ çŸ¥è­˜åº«æª¢ç´¢çµæœç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦")
        return "âš ï¸ ç„¡æ³•å¾çŸ¥è­˜åº«æª¢ç´¢åˆ°ç›¸é—œè³‡æ–™ã€‚è«‹å˜—è©¦æ›´æ”¹æ‚¨çš„å•é¡Œæè¿°ã€‚"
    print("ğŸ“š çŸ¥è­˜åº«æ‘˜è¦å®Œæˆ")
    print("------------------------------------------------------------------çµæŸèªæ„æŸ¥è©¢---------------------------------------------------------------------------------")
    


    print("------------------------------------------------------------------é–‹å§‹æŠŠè¨Šæ¯åšçµ„åˆ---------------------------------------------------------------------------------")
    # çµ„åˆå°è©±æ­·å²
    context = ""
    if not isinstance(history, list):
        print("âš ï¸ å°è©±æ­·å²æ ¼å¼éŒ¯èª¤ï¼Œåˆå§‹åŒ–ç‚ºç©º list")
        history = []
    # åƒ…å–æœ€å¾Œ 5 ç­†
    # æ ¹æ“šå‰å¹¾è¼ªçš„å°è©±ç´€éŒ„ historyï¼Œçµ„åˆæˆä¸€æ®µæ–‡å­— contextï¼Œè®“æ¨¡å‹èƒ½çœ‹æ‡‚ä¸Šä¸‹æ–‡è„ˆçµ¡ï¼ˆå¤šè¼ªå°è©±ï¼‰ã€‚
    for turn in history[-5:]:
        
        role = "User" if turn["role"] == "user" else "Assistant" # å°‡ role è½‰ç‚º User/Assistant
        context += f"{role}: {turn['content']}\n" # åƒ…å–æœ€å¾Œ 5 ç­†

    prompt = (
        "You are a knowledgeable helpdesk assistant. "
        "You will first review some relevant case entries, then answer the user's question based on context and your reasoning.\n\n"
        
        "==== [Retrieved Knowledge Base Entries] ====\n"
        f"{kb_context.strip()}\n\n"
        
        "==== [Conversation History] ====\n"
        f"{context.strip()}\n"
        
        "==== [User's Current Question] ====\n"
        f"User: {message}\n"
        
        "==== [Your Response] ====\n"
        "Assistant:"
    )
    print("\n[Prompt Preview] ğŸ§¾ ç™¼é€çµ¦æ¨¡å‹çš„ Prompt å‰ 300 å­—ï¼š")
    print(prompt[:300] + ("..." if len(prompt) > 300 else ""))
    try:
        print("ğŸš€ ç™¼é€ prompt çµ¦æ¨¡å‹ä¸­...")
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt.encode("utf-8"),
            capture_output=True,
            timeout=600
        )

        if result.returncode != 0:
            err = result.stderr.decode('utf-8')
            print("âŒ æ¨¡å‹éŒ¯èª¤ï¼š", err)
            return f"âš ï¸ Ollama éŒ¯èª¤ï¼š{err}"

        reply = result.stdout.decode("utf-8").strip()
        print("ğŸ“¥ æ¨¡å‹å›è¦†ï¼ˆå‰ 300 å­—ï¼‰ï¼š")
        print(reply[:300] + ("..." if len(reply) > 300 else ""))

        save_query_context(chat_id, message, query_type, result_summary=reply[:200])
        print("------------------------------------------------------------------çµæŸè™•ç†---------------------------------------------------------------------------------")
        return reply if reply else "âš ï¸ æ²’æœ‰æ”¶åˆ°æ¨¡å‹å›æ‡‰ã€‚"

    except Exception as e:
        print(f"âŒ å‘¼å«æ¨¡å‹å¤±æ•—ï¼š{str(e)}")
        return f"âš ï¸ å‘¼å«æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
    