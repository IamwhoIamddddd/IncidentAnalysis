import os
import sys
import json
import faiss
import pickle
import hashlib
import sqlite3
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import numpy as np
from datetime import datetime
from dateutil.parser import parse
# ========== âœ… æª¢æŸ¥ç’°å¢ƒèˆ‡ä¾è³´ ==========
print("âœ… [DEBUG] ä½ æœ‰æˆåŠŸå‘¼å« build_kb.py")

# ========== âœ… åŠ å…¥ log èˆ‡é–å®šæª¢æŸ¥ ==========
LOCK_FILE = "kb_building.lock"
LOG_FILE = "kb_log.txt"
KB_INDEX = "kb_index.faiss"
KB_TEXTS = "kb_texts.pkl"
KB_METADATA = "kb_metadata.json"
PROCESSED_LOG = "processed_files.json"
DATA_DIR = "json_data"
MODEL_NAME = "all-MiniLM-L6-v2"
SQLITE_DB = "resultDB.db"

def id_to_int64(uid):
    return int(hashlib.sha256(uid.encode('utf-8')).hexdigest(), 16) % (1 << 63)


def fix_datetime(value):
    try:
        # å˜—è©¦è½‰ç‚º datetime ç‰©ä»¶ï¼Œä¸¦è½‰æˆ ISO æ ¼å¼å­—ä¸²
        return parse(value).isoformat()
    except Exception:
        return "æ™‚é–“æœªå¡«å…¥"

def log(msg):
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(msg + "\n")

log("âœ… [LOG] build_kb.py è¢«åŸ·è¡Œï¼")

if os.path.exists(LOCK_FILE):
    print("â—çŸ¥è­˜åº«æ­£åœ¨å»ºç«‹ä¸­ï¼Œè«‹ç¨å€™")
    log("â— [LOG] åµæ¸¬åˆ° lock fileï¼Œå·²ä¸­æ­¢å»ºç«‹")
    sys.exit(0)

with open(LOCK_FILE, "w") as f:
    f.write("building")

def load_processed_files():
    if not os.path.exists(PROCESSED_LOG):
        return set()
    with open(PROCESSED_LOG, "r", encoding="utf-8") as f:
        data = json.load(f)
        return set(entry["file"] for entry in data)
    
def save_processed_file(file):
    now = datetime.now().isoformat()
    if os.path.exists(PROCESSED_LOG):
        with open(PROCESSED_LOG, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = []
    data.append({"file": file, "processedAt": now})
    with open(PROCESSED_LOG, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def save_to_sqlite(metadata_list):
    conn = sqlite3.connect(SQLITE_DB)
    c = conn.cursor()

    # å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    c.execute("""
            CREATE TABLE IF NOT EXISTS metadata (
                internalId INTEGER PRIMARY KEY AUTOINCREMENT,
                id TEXT UNIQUE,
                text TEXT,
                subcategory TEXT,
                configurationItem TEXT,
                roleComponent TEXT,
                location TEXT,
                opened TEXT,
                analysisTime TEXT
            )
    """)


    # æ’å…¥è³‡æ–™
    for item in metadata_list:
        c.execute("""
            INSERT OR REPLACE INTO metadata (id, text, subcategory, configurationItem, roleComponent, location, opened,analysisTime)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            item.get("id"),
            item["text"],
            item["subcategory"],
            item["configurationItem"],
            item["roleComponent"],
            item["location"],
            item["opened"],
            item["analysisTime"]
        ))


    conn.commit()
    conn.close()
    print(f"ğŸ—ƒï¸ å·²åŒæ­¥å„²å­˜ {len(metadata_list)} ç­†è³‡æ–™åˆ° SQLiteï¼š{SQLITE_DB}")



def extract_texts_and_metadata(json_file):
    with open(json_file, encoding="utf-8") as f:
        data = json.load(f)
        items = data["data"] if isinstance(data, dict) and "data" in data else data
        if not isinstance(items, list):
            items = [items]
        kb_texts = []
        metadata = []
        for item in items:
            summary = item.get("aiSummary") or item.get("problemSummary") or "(AI æ“·å–å¤±æ•—)"
            solution = item.get("solution") or "(AI æ“·å–å¤±æ•—)"
            ci = item.get("configurationItem") or "æœªçŸ¥æ¨¡çµ„"
            role = item.get("roleComponent") or "æœªæŒ‡å®šå…ƒä»¶"
            sub = item.get("subcategory") or "æœªåˆ†é¡"
            loc = item.get("location") or "æœªæä¾›"
            open_raw = item.get("opened") or "æ™‚é–“æœªå¡«å…¥"
            analysisTime_raw = item.get("analysisTime") or "æ™‚é–“æœªå¡«å…¥"
            uid = item.get("id") or "æœªæä¾›"
            analysis_Time = fix_datetime(analysisTime_raw)
            open_time = fix_datetime(open_raw)
            text = f"""äº‹ä»¶é¡åˆ¥ï¼š{sub}ï½œæ¨¡çµ„ï¼š{ci}ï½œè§’è‰²ï¼š{role}\nåœ°é»ï¼š{loc}\nå•é¡Œæè¿°ï¼š{summary}\nè™•ç†æ–¹å¼ï¼š{solution}"""
            kb_texts.append(text)
            metadata.append({
                "id": uid,
                "text": text,
                "subcategory": sub,
                "configurationItem": ci,
                "roleComponent": role,
                "location": loc,
                "opened": open_time,
                "analysisTime": analysis_Time,
            })
        return kb_texts, metadata

def build_kb():
    processed_files = load_processed_files()
    all_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".json") and f not in processed_files]
    if not all_files:
        print("ğŸ“­ æ²’æœ‰æ–°æª”æ¡ˆï¼Œè·³éå»ºåº«")
        return

    print(f"ğŸ“‚ æœ‰ {len(all_files)} å€‹æ–° JSON æª”è¦åŠ å…¥çŸ¥è­˜åº«")
    model = SentenceTransformer(MODEL_NAME)

    if os.path.exists(KB_INDEX) and os.path.exists(KB_TEXTS) and os.path.exists(KB_METADATA):
        print("ğŸ”„ è¼‰å…¥èˆŠæœ‰ FAISS indexã€æ–‡å­—åº«èˆ‡ metadata")
        index = faiss.read_index(KB_INDEX)
        with open(KB_TEXTS, "rb") as f:
            kb_texts = pickle.load(f)
        with open(KB_METADATA, "r", encoding="utf-8") as f:
            metadata = json.load(f)
    else:
        print("ğŸ†• å»ºç«‹å…¨æ–°çŸ¥è­˜åº«")
        index = None
        kb_texts = []
        metadata = []


    for file in tqdm(all_files, desc="ğŸ“¥ åŠ å…¥æ–°çŸ¥è­˜æª”æ¡ˆ"):
        path = os.path.join(DATA_DIR, file)
        print(f"ğŸ“‘ è™•ç†æª”æ¡ˆï¼š{file}")
        new_texts, new_metadata = extract_texts_and_metadata(path)
        metadata.extend(new_metadata)
        save_processed_file(file)

    # ğŸ”„ è‹¥æœ‰èˆŠçš„ metadataï¼Œå…ˆè¼‰å…¥ä¸¦è½‰æˆ dict ä»¥ id ç‚º key
    print("ğŸ“‚ è¼‰å…¥èˆŠçš„ metadata.json ä¸¦æº–å‚™æ¯”å° ID...")
    if os.path.exists(KB_METADATA):
        with open(KB_METADATA, "r", encoding="utf-8") as f:
            old_metadata = json.load(f)
    else:
        old_metadata = []

    metadata_dict = {item["id"]: item for item in old_metadata if "id" in item}

    print(f"ğŸ” èˆŠ metadata å…± {len(metadata_dict)} ç­†ï¼Œæº–å‚™èˆ‡æ–°è³‡æ–™åˆä½µ")

    # ğŸ†• æ›´æ–°æˆ–æ–°å¢æ¯ä¸€ç­†æ–° metadataï¼ˆç”¨æ–°è³‡æ–™è¦†è“‹åŒ idï¼‰
    print(f"â• åˆä½µæ–° metadataï¼Œå…± {len(metadata)} ç­†æ–°è³‡æ–™")
    for item in metadata:
        uid = item.get("id")
        if not uid or uid == "æœªæä¾›":
            continue
        metadata_dict[uid] = item

    # ğŸ’¾ å¯«å›æª”æ¡ˆï¼ˆè½‰å› listï¼‰
    merged_metadata = list(metadata_dict.values())
    print(f"ğŸ’¾ å¯«å…¥åˆä½µå¾Œçš„ metadataï¼Œå…± {len(merged_metadata)} ç­†")
    with open(KB_METADATA, "w", encoding="utf-8") as f:
        json.dump(merged_metadata, f, ensure_ascii=False, indent=2)

    # âœ… é€™è£¡æ”¹ç‚ºé‡å»º FAISS index å’Œæ–‡å­—åº«
    print("ğŸ“ é–‹å§‹é‡å»º FAISS å‘é‡åº«")
    texts_for_embedding = [item["text"] for item in merged_metadata]
    embeddings = model.encode(texts_for_embedding, show_progress_bar=True)

    index_flat = faiss.IndexFlatL2(embeddings.shape[1])         # å»ºç«‹ base index
    index = faiss.IndexIDMap(index_flat)                        # åŒ…æˆ ID map
    print("ğŸ”¢ æº–å‚™åŠ å…¥å‘é‡åˆ° FAISS index...")
    ids = []
    for item in merged_metadata:
        uid = item["id"]
        fid = id_to_int64(uid)
        print(f"ğŸ†” åŸå§‹ ID: {uid} -> FAISS ID: {fid}")
        ids.append(fid)

    ids = np.array(ids, dtype=np.int64)
    index.add_with_ids(np.array(embeddings), ids)
    print(f"ğŸ”¢ å‘é‡åº«å»ºç«‹å®Œæˆï¼Œå…± {len(embeddings)} ç­†å‘é‡")
    print(f"ğŸ”¢ å‘é‡åº«å»ºç«‹å®Œæˆï¼Œå…± {len(texts_for_embedding)} ç­†æ–‡æœ¬")
    print("âœ… å‘é‡å»ºç«‹å®Œæˆï¼Œæº–å‚™å„²å­˜ FAISS index")

    faiss.write_index(index, KB_INDEX)
    with open(KB_TEXTS, "wb") as f:
        pickle.dump(texts_for_embedding, f)
    print(f"ğŸ’¾ å‘é‡åº«èˆ‡æ–‡å­—åº«å·²å„²å­˜ï¼Œå…± {len(texts_for_embedding)} ç­†")

    print("ğŸ—ƒï¸ å¯«å…¥ SQLite è³‡æ–™åº«ä¸­...")
    save_to_sqlite(merged_metadata)

    print(f"âœ… çŸ¥è­˜åº«æ›´æ–°å®Œæˆï¼ˆç¸½å…± {len(texts_for_embedding)} ç­†ï¼‰")
    log(f"âœ… [LOG] æˆåŠŸå»ºç«‹çŸ¥è­˜åº«ï¼Œå…± {len(texts_for_embedding)} ç­†")


if __name__ == "__main__":
    try:
        build_kb()
    finally:
        if os.path.exists(LOCK_FILE):
            os.remove(LOCK_FILE)
        print("ğŸ—‚ï¸ é–å®šæª”å·²åˆªé™¤ï¼ŒçµæŸå»ºåº«æµç¨‹")
        log("âœ… [LOG] çŸ¥è­˜åº«æµç¨‹çµæŸï¼Œlock å·²æ¸…é™¤")
        print("ğŸ“œ æ—¥èªŒå·²æ›´æ–°ï¼Œè«‹æª¢æŸ¥ kb_log.txt ç²å–è©³ç´°è³‡è¨Š")