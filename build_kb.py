import os
import sys
import json
import faiss
import pickle
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import numpy as np
from datetime import datetime
from dateutil.parser import parse
# ========== âœ… æª¢æŸ¥ç’°å¢ƒèˆ‡ä¾è³´ ==========
print("âœ… [DEBUG] ä½ æœ‰æˆåŠŸå‘¼å« build_kb.py")

# ========== âœ… åŠ å…¥ log èˆ‡é–å®šæª¢æŸ¥ ==========
LOCK_FILE = "kb_building.lock"
LOG_FILE = "kb_log.txt"

KB_INDEX = "kb_index.faiss"
KB_TEXTS = "kb_texts.pkl"
KB_METADATA = "kb_metadata.json"
PROCESSED_LOG = "processed_files.json"
DATA_DIR = "json_data"
MODEL_NAME = "all-MiniLM-L6-v2"



def fix_datetime(value):
    try:
        # å˜—è©¦è½‰ç‚º datetime ç‰©ä»¶ï¼Œä¸¦è½‰æˆ ISO æ ¼å¼å­—ä¸²
        return parse(value).isoformat()
    except Exception:
        return "æ™‚é–“æœªå¡«å…¥"

def log(msg):
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(msg + "\n")

log("âœ… [LOG] build_kb.py è¢«åŸ·è¡Œï¼")

if os.path.exists(LOCK_FILE):
    print("â—çŸ¥è­˜åº«æ­£åœ¨å»ºç«‹ä¸­ï¼Œè«‹ç¨å€™")
    log("â— [LOG] åµæ¸¬åˆ° lock fileï¼Œå·²ä¸­æ­¢å»ºç«‹")
    sys.exit(0)

with open(LOCK_FILE, "w") as f:
    f.write("building")

def load_processed_files():
    if not os.path.exists(PROCESSED_LOG):
        return set()
    with open(PROCESSED_LOG, "r", encoding="utf-8") as f:
        data = json.load(f)
        return set(entry["file"] for entry in data)

def save_processed_file(file):
    now = datetime.now().isoformat()
    if os.path.exists(PROCESSED_LOG):
        with open(PROCESSED_LOG, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = []
    data.append({"file": file, "processedAt": now})
    with open(PROCESSED_LOG, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def extract_texts_and_metadata(json_file):
    with open(json_file, encoding="utf-8") as f:
        data = json.load(f)
        items = data["data"] if isinstance(data, dict) and "data" in data else data
        if not isinstance(items, list):
            items = [items]
        kb_texts = []
        metadata = []
        for item in items:
            summary = item.get("aiSummary") or item.get("problemSummary") or "(AI æ“·å–å¤±æ•—)"
            solution = item.get("solution") or "(AI æ“·å–å¤±æ•—)"
            ci = item.get("configurationItem") or "æœªçŸ¥æ¨¡çµ„"
            role = item.get("roleComponent") or "æœªæŒ‡å®šå…ƒä»¶"
            sub = item.get("subcategory") or "æœªåˆ†é¡"
            loc = item.get("location") or "æœªæä¾›"
            open_raw = item.get("opened") or "æ™‚é–“æœªå¡«å…¥"
            open_time = fix_datetime(open_raw)
            text = f"""äº‹ä»¶é¡åˆ¥ï¼š{sub}ï½œæ¨¡çµ„ï¼š{ci}ï½œè§’è‰²ï¼š{role}\nåœ°é»ï¼š{loc}\nå•é¡Œæè¿°ï¼š{summary}\nè™•ç†æ–¹å¼ï¼š{solution}"""
            kb_texts.append(text)
            metadata.append({
                "text": text,
                "subcategory": sub,
                "configurationItem": ci,
                "roleComponent": role,
                "location": loc,
                "analysisTime": open_time,
            })
        return kb_texts, metadata

def build_kb():
    processed_files = load_processed_files()
    all_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".json") and f not in processed_files]
    if not all_files:
        print("ğŸ“­ æ²’æœ‰æ–°æª”æ¡ˆï¼Œè·³éå»ºåº«")
        return

    print(f"ğŸ“‚ æœ‰ {len(all_files)} å€‹æ–° JSON æª”è¦åŠ å…¥çŸ¥è­˜åº«")
    model = SentenceTransformer(MODEL_NAME)

    if os.path.exists(KB_INDEX) and os.path.exists(KB_TEXTS) and os.path.exists(KB_METADATA):
        print("ğŸ”„ è¼‰å…¥èˆŠæœ‰ FAISS indexã€æ–‡å­—åº«èˆ‡ metadata")
        index = faiss.read_index(KB_INDEX)
        with open(KB_TEXTS, "rb") as f:
            kb_texts = pickle.load(f)
        with open(KB_METADATA, "r", encoding="utf-8") as f:
            metadata = json.load(f)
    else:
        print("ğŸ†• å»ºç«‹å…¨æ–°çŸ¥è­˜åº«")
        index = None
        kb_texts = []
        metadata = []

    for file in tqdm(all_files, desc="ğŸ“¥ åŠ å…¥æ–°çŸ¥è­˜æª”æ¡ˆ"):
        path = os.path.join(DATA_DIR, file)
        new_texts, new_metadata = extract_texts_and_metadata(path)
        new_embeddings = model.encode(new_texts, show_progress_bar=False)
        if index is None:
            index = faiss.IndexFlatL2(new_embeddings.shape[1])
        index.add(np.array(new_embeddings))
        kb_texts.extend(new_texts)
        metadata.extend(new_metadata)
        save_processed_file(file)

    faiss.write_index(index, KB_INDEX)
    with open(KB_TEXTS, "wb") as f:
        pickle.dump(kb_texts, f)
    with open(KB_METADATA, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"âœ… çŸ¥è­˜åº«æ›´æ–°å®Œæˆï¼ˆç¸½å…± {len(kb_texts)} ç­†ï¼‰")
    log(f"âœ… [LOG] æˆåŠŸå»ºç«‹çŸ¥è­˜åº«ï¼Œå…± {len(kb_texts)} ç­†")

if __name__ == "__main__":
    try:
        build_kb()
    finally:
        if os.path.exists(LOCK_FILE):
            os.remove(LOCK_FILE)
        print("ğŸ—‚ï¸ é–å®šæª”å·²åˆªé™¤ï¼ŒçµæŸå»ºåº«æµç¨‹")
        log("âœ… [LOG] çŸ¥è­˜åº«æµç¨‹çµæŸï¼Œlock å·²æ¸…é™¤")
        print("ğŸ“œ æ—¥èªŒå·²æ›´æ–°ï¼Œè«‹æª¢æŸ¥ kb_log.txt ç²å–è©³ç´°è³‡è¨Š")