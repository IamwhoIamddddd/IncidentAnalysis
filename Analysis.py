# åŒ¯å…¥ Flask æ¡†æ¶åŠç›¸é—œæ¨¡çµ„
from flask import Flask, request, jsonify, render_template, session, send_file
from typer import prompt
from gpt_utils import extract_resolution_suggestion
from gpt_utils import extract_problem_with_custom_prompt
from gpt_utils import analyze_with_ai_builder_then_fallback
from gptChat import run_offline_gpt
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from collections import Counter
import hashlib
import subprocess
import sys
from pathlib import Path
import umap
import hdbscan
# åŒ¯å…¥æ•¸å­¸é‹ç®—æ¨¡çµ„
import math
import json
# åŒ¯å…¥ pandas ç”¨æ–¼è™•ç† Excel è³‡æ–™
import pandas as pd
# åŒ¯å…¥ os æ¨¡çµ„è™•ç†æª”æ¡ˆèˆ‡è·¯å¾‘
import os
import shutil
# åŒ¯å…¥æ­£å‰‡è¡¨é”å¼æ¨¡çµ„
import re
import glob
# åŒ¯å…¥ webbrowser ç”¨æ–¼é–‹å•Ÿç¶²é 
import webbrowser
import socket
# åŒ¯å…¥ traceback ç”¨æ–¼éŒ¯èª¤è¿½è¹¤
import traceback
# åŒ¯å…¥ Werkzeug çš„å·¥å…·å‡½æ•¸ç¢ºä¿æª”æ¡ˆåç¨±å®‰å…¨
from werkzeug.utils import secure_filename
# âœ… åŒ¯å…¥èªæ„åˆ†ææ¨¡çµ„
from SmartScoring import is_high_risk, is_escalated, is_multi_user, extract_keywords, recommend_solution, is_actionable_resolution, load_embeddings, load_examples_from_json
# âœ… é å…ˆ encode ä¸€ç­†è³‡æ–™ä»¥åŠ é€Ÿé¦–æ¬¡è«‹æ±‚
from SmartScoring import bert_model  # ç¢ºä¿ä½ æœ‰å¾ SmartScoring è¼‰å…¥æ¨¡å‹
from SmartScoring import extract_cluster_name  # åŒ¯å…¥è‡ªå®šçš„ cluster å‘½åå‡½å¼
from tqdm import tqdm
from sentence_transformers import util
# âœ… åŒ¯å…¥é—œéµå­—æŠ½å–æ¨¡çµ„
from datetime import datetime
from sklearn.cluster import KMeans
import numpy as np
from datetime import datetime
import time
# --- åˆ†ç¾¤å•Ÿç”¨æ¢ä»¶ï¼ˆå¯ä¾è³‡æ–™èª¿æ•´ï¼‰---
import asyncio
import aiohttp
import math
import requests
import threading
import json
import tempfile
from jsonschema import validate, ValidationError
from datetime import datetime
from collections import Counter
import openpyxl
from openpyxl.worksheet.table import Table, TableStyleInfo
import traceback
from flask_socketio import SocketIO, emit



POWERAUTOMATE_CLASSIFY_URL = "https://prod-26.southeastasia.logic.azure.com:443/workflows/651f88b4e548481ba38d129c30af1cae/triggers/manual/paths/invoke?api-version=2016-06-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=1kjXdbN7QsORisL6sdEA1IRXRef_bstLqZjmRjp9c6E"  # ğŸ” æ”¹æˆä½ è‡ªå·±çš„åˆ†é¡æµç¨‹ URL
POWERAUTOMATE_SUMMARY_URL = "https://prod-71.southeastasia.logic.azure.com:443/workflows/d70056c4f2c044b9a297164c9f98d1b6/triggers/manual/paths/invoke?api-version=2016-06-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=L_qVgz5s0bLvz20lmI3RsoEmClvbTJfy7v99Ai38Xpw"  # ğŸ” æ”¹æˆä½ è‡ªå·±çš„æ‘˜è¦æµç¨‹ URL

KMEANS_MIN_COUNT = 4         # æœ€å°‘è³‡æ–™ç­†æ•¸
KMEANS_MIN_RANGE = 5.0       # åˆ†æ•¸æœ€å¤§æœ€å°å€¼å·®
KMEANS_MIN_STDDEV = 3.0      # æ¨™æº–å·®ä¸‹é™

progress_log = ""  # å…¨åŸŸè®Šæ•¸ï¼Œå°ˆé–€ç”¨ä¾†å­˜é€²åº¦è¨Šæ¯




start = time.time()
print("ğŸ”¥ é ç†±èªæ„æ¨¡å‹ä¸­...")
bert_model.encode("warmup")  # é ç†±ä¸€æ¬¡ï¼Œé¿å…ç¬¬ä¸€æ¬¡ä½¿ç”¨å¤ªæ…¢
print(f"âœ… æ¨¡å‹é ç†±å®Œæˆï¼Œç”¨æ™‚ï¼š{time.time() - start:.2f} ç§’")

# å»ºç«‹ Flask æ‡‰ç”¨
app = Flask(__name__)
# è¨­å®šæ‡‰ç”¨çš„å¯†é‘°ï¼Œç”¨æ–¼ session åŠ å¯†
app.secret_key = 'gwegweqgt22e'
# è¨­å®š session å„²å­˜æ–¹å¼ç‚ºæª”æ¡ˆç³»çµ±
app.config['SESSION_TYPE'] = 'filesystem'

# ------------------------------------------------------------------------------
# è¨­å®šä¸Šå‚³è³‡æ–™å¤¾èˆ‡å¤§å°é™åˆ¶ï¼ˆ10MBï¼‰
UPLOAD_FOLDER = 'uploads'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # é™åˆ¶æª”æ¡ˆå¤§å°ç‚º 10MB
ALLOWED_EXTENSIONS = {'xlsx'}  # åƒ…å…è¨±ä¸Šå‚³ xlsx æª”æ¡ˆ


basedir = os.path.abspath(os.path.dirname(__file__))  # å–å¾—ç•¶å‰ app.py çš„çµ•å°ç›®éŒ„
# ç¢ºä¿ä¸Šå‚³è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(os.path.join(basedir, 'json_data'), exist_ok=True)
os.makedirs(os.path.join(basedir, 'excel_result_Unclustered'), exist_ok=True)  # æ–°å¢æœªåˆ†ç¾¤è³‡æ–™å¤¾
os.makedirs(os.path.join(basedir, 'excel_result_Clustered'), exist_ok=True) # æ–°å¢åˆ†ç¾¤è³‡æ–™å¤¾

# ------------------------------------------------------------------------------

# åˆ¤æ–·æ˜¯å¦å…è¨±çš„æª”æ¡ˆæ ¼å¼
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# ------------------------------------------------------------------------------

# å®šç¾©å‡½æ•¸ï¼šè™•ç†ç‰¹æ®Šå€¼ï¼ˆå¦‚ NaNã€None ç­‰ï¼‰
def safe_value(val):
    if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
        return 0
    elif val is None:
        return None
    elif isinstance(val, str):
        return val
    else:
        return val

# ------------------------------------------------------------------------------

def append_progress(msg):
    global progress_log
    progress_log += msg + "\n"
    print(msg)  # åŒæ™‚åœ¨æ§åˆ¶å°è¼¸å‡º
    
# ------------------------------------------------------------------------------




@app.route('/check-unclustered', methods=['GET'])
def check_unclustered_files():
    folder = 'excel_result_Unclustered'
    if not os.path.exists(folder):
        return jsonify({'exists': False}), 200
    files = [f for f in os.listdir(folder) if f.endswith('.xlsx')]
    return jsonify({'exists': len(files) > 0}), 200


@app.route('/clustered-files', methods=['GET'])
def list_clustered_files():
    clustered_folder = os.path.join('excel_result_Clustered', 'Details')

    if not os.path.exists(clustered_folder):
        return jsonify({'files': [], 'total': 0})

    # æ”¯æ´åˆ†æ‰¹åƒæ•¸
    offset = int(request.args.get('offset', 0))
    limit = int(request.args.get('limit', 100))

    pattern = re.compile(r"^Cluster_\[CI\].+_\[AI\].+\.xlsx$")  # ç¬¦åˆå‘½åè¦å‰‡
    all_files = [
        f for f in os.listdir(clustered_folder)
        if f.endswith('.xlsx') and pattern.match(f)
    ]
    all_files.sort()  # ä½ ä¹Ÿå¯ä»¥æŒ‰æ—¥æœŸæˆ–ç­†æ•¸æ’åº

    total = len(all_files)  # å…¨éƒ¨æª”æ¡ˆç¸½æ•¸

    # åˆ†æ‰¹å–å‡ºæœ¬æ¬¡è¦å›å‚³çš„æª”æ¡ˆ
    page_files = all_files[offset:offset + limit]

    files_info = []
    for f in page_files:
        filepath = os.path.join(clustered_folder, f)
        try:
            df = pd.read_excel(filepath)
            row_count = len(df)
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å– {f}ï¼š{e}")
            row_count = 0

        files_info.append({
            'name': f,
            'rows': row_count
        })

    return jsonify({'files': files_info, 'total': total})


@app.route('/download-clustered', methods=['GET'])
def download_clustered_file():
    filename = request.args.get('file')
    # ä¸€å®šè¦åŠ  Details å­è³‡æ–™å¤¾
    path = os.path.join('excel_result_Clustered', 'Details', filename)
    if os.path.exists(path):
        return send_file(path, as_attachment=True)
    return jsonify({'error': 'æ‰¾ä¸åˆ°æª”æ¡ˆ'}), 404


# æ ¹æ“šåˆ†æ•¸åˆ¤æ–·é¢¨éšªç­‰ç´šï¼ˆæ”¯æ´ KMeans åˆ†ç¾¤ï¼‰
kmeans_thresholds = None  # å…¨åŸŸè®Šæ•¸ï¼Œå­˜å„² KMeans åˆ†ç¾¤é–€æª»


def get_risk_level(score):
    global kmeans_thresholds
    level = ''

    if kmeans_thresholds and len(kmeans_thresholds) == 4:
        thresholds = sorted(kmeans_thresholds)
        if score >= thresholds[3]:
            level = 'é«˜é¢¨éšª'
        elif score >= thresholds[2]:
            level = 'ä¸­é¢¨éšª'
        elif score >= thresholds[1]:
            level = 'ä½é¢¨éšª'
        else:
            level = 'å¿½ç•¥'
        print(f"ğŸ“Š KMeansï¼šimpactScore: {score} â†’ åˆ†ç´šï¼š{level}ï¼ˆä½¿ç”¨å‹•æ…‹é–€æª»ï¼‰")
    else:
        if score >= 18:
            level = 'é«˜é¢¨éšª'
        elif score >= 12:
            level = 'ä¸­é¢¨éšª'
        elif score >= 6:
            level = 'ä½é¢¨éšª'
        else:
            level = 'å¿½ç•¥'
        print(f"ğŸ“Š å›ºå®šé–€æª»ï¼šimpactScore: {score} â†’ åˆ†ç´šï¼š{level}")

    return level

# åœ¨åˆ†æå®Œæˆå¾Œè‡ªå‹•è¨­å®š kmeans_thresholds
# ï¼ˆè«‹æ”¾åœ¨ KMeans åˆ†ç¾¤å®Œæˆå¾Œï¼‰
def set_kmeans_thresholds_from_centroids(centroids):
    global kmeans_thresholds
    kmeans_thresholds = sorted(centroids)
    print(f"âœ… å·²è¨­å®š KMeans åˆ†ç¾¤é–€æª»ï¼ˆsortedï¼‰ï¼š{kmeans_thresholds}")

# ------------------------------------------------------------------------------

def append_cluster_progress(msg, progress, total):
    with open("cluster_progress.json", "w", encoding="utf-8") as f:
        json.dump({
            "progress": progress,
            "total": total,
            "status": msg
        }, f, ensure_ascii=False)


def clean_filename(name):
    # ç§»é™¤ Windows ç¦ç”¨å­—å…ƒ < > : " / \ | ? *
    return re.sub(r'[<>:"/\\|?*]', '_', str(name)).strip()



def load_or_create_category_json(ci_name):
    safe_ci_name = clean_filename(ci_name)
    category_path = f"cluster_excels/{safe_ci_name}_categories.json"
    if os.path.exists(category_path):
        print(f"ğŸ”„ å·²è®€å–åˆ†é¡è¨˜æ†¶ JSONï¼š{category_path}")
        with open(category_path, "r", encoding="utf-8") as f:
            return json.load(f), category_path
    else:
        print(f"ğŸ†• å°šç„¡åˆ†é¡è¨˜æ†¶ï¼Œå°‡å»ºç«‹æ–°æª”ï¼š{category_path}")
        return [], category_path  # ç©ºåˆ†é¡è¡¨

    
    

def classify_summary_with_ai(summary, existing_categories, config_item):
    # âœ… æ ¼å¼åŒ–æˆé€—è™Ÿåˆ†éš”å­—ä¸²çµ¦ prompt ç”¨
    formatted_categories = ', '.join([str(c.get("category", "")) for c in existing_categories])

    print("-----------------------------------------------------")
    print("ğŸ“‹ é–‹å§‹é€²è¡Œåˆ†é¡")
    print(f"  - Summaryï¼š{summary[:80]}{'...' if len(summary) > 80 else ''}")
    print(f"  - ç¾æœ‰åˆ†é¡æ¸…å–®ï¼š{formatted_categories if formatted_categories else '(ç„¡)'}")
    
    prompt = f"""You are an AI assistant helping to classify IT incident summaries.

    The purpose of this classification is to produce more fine-grained and meaningful categories than the Configuration Item itself, so that IT incidents can be managed and analyzed more effectively.

    IMPORTANT:
    - The category **must be more specific than the Configuration Item**.
    - Do NOT use the Configuration Item name or any similar/derived name as the category.
    - For example, if the Configuration Item is "Email System", the category should be things like "login failure", "delivery delay", "mailbox full", "spam filtering", NOT just "Email System".

    Please read the following summary and determine the most appropriate category name from the given list.
    If none of the existing categories match well, create a new concise category that best represents the issue.

    Configuration Item:
    {config_item}

    Incident Summary:
    {summary}

    Existing Categories:
    {formatted_categories}

    Output Rule:
    - Return only the final category name.
    - Do NOT include any explanation, formatting, or punctuation.
    - The response must be exactly one line with the category name only.

    Your answer:"""


    try:
        payload = {
            "prompt": prompt
        }
        print("  - å·²é€å‡º API è«‹æ±‚ï¼Œç­‰å¾…åˆ†é¡çµæœ...")
        response = requests.post(POWERAUTOMATE_CLASSIFY_URL, json=payload, timeout=120)
        result = response.json()
        print("  - API å›å‚³å…§å®¹ï¼š", result)
        category = result.get("category", "").strip().lower()
        print(f"  - åˆ†é¡çµæœï¼š{category if category else 'ï¼ˆuncategorizedï¼‰'}")
        print("-----------------------------------------------------")
        return category if category else "uncategorized"
    except Exception as e:
        print(f"âŒ åˆ†é¡å¤±æ•—ï¼ˆsummary: {summary[:80]}{'...' if len(summary) > 80 else ''}ï¼‰ï¼š", e)
        print("-----------------------------------------------------")
        return "uncategorized"
# ------------------------------------------------------------------------------

@app.route('/cluster-excel', methods=['POST'])
def cluster_excel():
    unclustered_dir = 'excel_result_Unclustered'
    clustered_dir = 'excel_result_Clustered'
    os.makedirs(clustered_dir, exist_ok=True)  # âœ… ç¢ºä¿ Clustered è³‡æ–™å¤¾å­˜åœ¨

    files = [f for f in os.listdir(unclustered_dir) if f.endswith('_Unclustered.xlsx')]
    
    total_files = len(files)

    append_cluster_progress("â³ é–‹å§‹åˆ†ç¾¤...", 0, total_files)
    
    
    
    print("="*60)
    print(f"ğŸ” åµæ¸¬åˆ° {len(files)} ç­†å¾…åˆ†ç¾¤æª”æ¡ˆï¼š{files if files else 'ï¼ˆç„¡ï¼‰'}")
    print("="*60)

    for i, filename in enumerate(files, 1):
        
        # ğŸŸ¡ ä¸€é–‹å§‹å°±å¯«ã€Œæ­£åœ¨è™•ç†...ã€
        append_cluster_progress(
            f"æ­£åœ¨è™•ç†ç¬¬ {i}/{total_files} å€‹æª”æ¡ˆï¼š{filename}",
            i - 1,
            total_files
        )
        
        uid = filename.replace('_Unclustered.xlsx', '')
        excel_path = os.path.join(unclustered_dir, filename)
        print(f"\nğŸŸ¦ [{i}/{len(files)}] é–‹å§‹è™•ç†æª”æ¡ˆï¼š{excel_path}")

        df = pd.read_excel(excel_path)
        print(f"  ğŸ“‘ å…± {len(df)} ç­†è³‡æ–™å¾…åˆ†é¡")

        # âœ… ä¸€ç­†ä¸€ç­†åˆ†é¡
        for idx, row in df.iterrows():
            config_item = row.get("configurationItem", "Unknown")
            summary = row.get("aiSummary", "").strip()
            print(f"    â”œâ”€ [{idx+1}/{len(df)}] åˆ†é¡ configurationItemï¼š{config_item} | Summary å‰ 40 å­—ï¼š{summary[:40]}{'...' if len(summary)>40 else ''}")

            # ğŸ” è®€å–åˆ†é¡è¨˜æ†¶ JSON
            categories, cat_path = load_or_create_category_json(config_item)

            # âœ¨ ä¸Ÿçµ¦ GPT åˆ†é¡ï¼Œé€™è£¡è¦å¤šå¸¶ config_item
            category = classify_summary_with_ai(summary, categories, config_item)

            # âœ… å¦‚æœåˆ†é¡é‚„æ²’å‡ºç¾åœ¨è¨˜æ†¶æ¸…å–®ï¼Œæ‰åŠ å…¥
            if category not in [c["category"].lower() for c in categories]:  # âœ… å°å¯«æ¯”è¼ƒé¿å…é‡è¤‡
                new_cat = {"category": category}
                categories.append(new_cat)
                with open(cat_path, "w", encoding="utf-8") as f:
                    json.dump(categories, f, indent=2, ensure_ascii=False)
                print(f"    â”‚   ğŸ†• æ–°åˆ†é¡å·²åŠ å…¥ {cat_path}ï¼š{category}")
            else:
                print(f"    â”‚   åˆ†é¡ {category} å·²å­˜åœ¨è¨˜æ†¶æ¸…å–®")

            # âœ… å¯«å…¥é€™ç­†äº‹ä»¶çš„ aiCategory
            df.at[idx, "aiCategory"] = category.lower()  # âœ… ä¿éšªè½‰å°å¯«

        # âœ… è¦†è“‹å›åŸ Excelï¼ˆUnclusteredï¼‰
        df.to_excel(excel_path, index=False)
        print(f"  ğŸ“„ å·²è¦†è“‹æ›´æ–° Unclustered Excelï¼š{excel_path}")

        # âœ… ç¹¼çºŒç…§åŸé‚è¼¯åˆ†ç¾¤ï¼ˆæ­¤æ™‚æ¯ç­†éƒ½æœ‰ aiCategoryï¼‰
        results = df.to_dict(orient='records')
        print(f"  ğŸ”— é€²è¡Œåˆ†ç¾¤ä¸¦åŒ¯å‡ºè‡³ Clustered ...")
        cluster_excel_export(results)

        # âœ… æ¬åˆ° Clustered è³‡æ–™å¤¾
        clustered_path = os.path.join(clustered_dir, uid + '_Clustered.xlsx')
        shutil.move(excel_path, clustered_path)
        print(f"  ğŸ“ å·²ç§»å‹•æª”æ¡ˆä¸¦æ”¹åï¼š{clustered_path}")
                # ...åˆ†é¡ã€åˆ†ç¾¤ã€summary...
        # â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
        append_cluster_progress(
            f"å·²å®Œæˆç¬¬ {i}/{total_files} å€‹æª”æ¡ˆï¼š{filename}",
            i,
            total_files
        )
        
        
        
    # æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼Œæœ€å¾Œå¯«ä¸€æ¬¡é€²åº¦
    append_cluster_progress(f"ğŸ‰ æ‰€æœ‰åˆ†ç¾¤ Excel æª”æ¡ˆå·²å®Œæˆæ¬ç§»èˆ‡åˆ†ç¾¤ï¼Œå…±è™•ç† {total_files} æª”ï¼", total_files, total_files)

    print("="*60)
    print(f"ğŸ‰ æ‰€æœ‰åˆ†ç¾¤ Excel æª”æ¡ˆå·²å®Œæˆæ¬ç§»èˆ‡åˆ†ç¾¤ï¼Œå…±è™•ç† {len(files)} æª”ï¼")
    print("="*60)

    return jsonify({'message': f'å·²æˆåŠŸè™•ç† {len(files)} ç­† Excel æª”æ¡ˆä¸¦å®Œæˆåˆ†ç¾¤'}), 200

# ------------------------------------------------------------------------------

def cluster_excel_export(results, export_dir="excel_result_Clustered/Details"):
    def clean(text):
        return re.sub(r'[^\w\-_.]', '_', str(text).strip())[:30] or "Unknown"

    print("\nğŸŸ¦ [cluster_excel_export] é–‹å§‹åˆ†ç¾¤èˆ‡åŒ¯å‡º ...")
    cluster_data = defaultdict(list)
    for r in results:
        config_item = r.get("configurationItem") or "Unknown"
        ai_category = r.get("aiCategory") or "æœªåˆ†é¡"
        cluster_key = f"{config_item}_{ai_category}"
        r['cluster'] = cluster_key
        cluster_data[cluster_key].append(r)

    os.makedirs(export_dir, exist_ok=True)
    print(f"ğŸ“ åŒ¯å‡ºè³‡æ–™å¤¾è·¯å¾‘ï¼š{export_dir}")
    print(f"ğŸ”¢ å…±ç™¼ç¾ {len(cluster_data)} å€‹åˆ†ç¾¤")
    
    # ====== åŠ é€™æ®µå–å¾—ç›®å‰ç¸½æª”æ¡ˆæ•¸ï¼Œè®“é€²åº¦åˆç† ======
    total_clusters = len(cluster_data)
    # ==========================================
    

    for idx, (key, group) in enumerate(cluster_data.items(), 1):
        
        

        
        cluster_df = pd.DataFrame(group)
        try:
            config_item, ai_category = key.split('_', 1)
        except ValueError:
            config_item, ai_category = key, "æœªåˆ†é¡"

        filename = f"{export_dir}/Cluster_[CI]{clean(config_item)}_[AI]{clean(ai_category)}.xlsx"
        print(f"\n  [{idx}/{len(cluster_data)}] åˆ†ç¾¤ Keyï¼š{key}")
        print(f"    â”œâ”€ åŒ¯å‡ºæª”åï¼š{filename}")
        print(f"    â”œâ”€ æœ¬ç¾¤å…± {len(group)} ç­†è³‡æ–™")

        if os.path.exists(filename):
            old_df = pd.read_excel(filename)
            print(f"    â”œâ”€ æª”æ¡ˆå·²å­˜åœ¨ï¼ŒåŸæœ‰ {len(old_df)} ç­†è³‡æ–™ï¼Œå°‡åˆä½µ")
            cluster_df = pd.concat([old_df, cluster_df], ignore_index=True)
        else:
            print("    â”œâ”€ æª”æ¡ˆä¸å­˜åœ¨ï¼Œå°‡æ–°å»ºæª”æ¡ˆ")

        cluster_df = cluster_df.sort_values(by="analysisTime", ascending=False)
        cluster_df.to_excel(filename, index=False)
        print(f"    â”œâ”€ å·²è¼¸å‡º Excelï¼Œåˆä½µå¾Œå…± {len(cluster_df)} ç­†")
        
        # ====== æ–°å¢ï¼šå†å­˜ä¸€ä»½åˆ° OneDrive ç›®çš„è³‡æ–™å¤¾ (é€™å€‹è³‡æ–™å¤¾åœ¨MSTC ITG sharepoint ç¶²ç«™ä¸Š)======
        custom_export_dir = r"C:\Users\tachang\Microsoft\MSTC ITG - Timmy\IncidentAnalysis_Clustered_File\Details"
        os.makedirs(custom_export_dir, exist_ok=True)
        custom_filename = os.path.join(custom_export_dir, os.path.basename(filename))
        cluster_df.to_excel(custom_filename, index=False)
        print(f"    â”œâ”€ å·²é¡å¤–å‚™ä»½åˆ°ï¼š{custom_filename}")
        # ==============================================
        
        
        
        print(f"    â”œâ”€ ç”¢ç”Ÿç¾¤çµ„æ‘˜è¦ ...")
        summarize_group_to_excel(config_item, ai_category, group)

        high_count = sum(1 for e in group if e.get('riskLevel') == 'é«˜é¢¨éšª')
        total = len(group)
        if total > 0 and (high_count / total) >= 0.5:
            print(f"    ğŸš¨ é è­¦ï¼šCluster {key} æœ‰ {high_count}/{total} ç­†é«˜é¢¨éšªäº‹ä»¶")
    print("\nâœ… [cluster_excel_export] æ‰€æœ‰åˆ†ç¾¤ Excel æª”æ¡ˆå·²å„²å­˜ï¼")
    
    
    
    
    
# ------------------------------------------------------------------------------
def run_gpt_summary(text, instruction):
    if not text.strip():
        print("âš ï¸ ç„¡å…§å®¹å¯æ‘˜è¦")
        return "ï¼ˆç„¡å…§å®¹ï¼‰"

    prompt = f"""{instruction}

    Content:
    {text}

    Please summarize concisely in one paragraph:"""
    
    print("ğŸŸ¦ [run_gpt_summary] é–‹å§‹å‘¼å« GPT æ‘˜è¦")
    print(f"  - æŒ‡ä»¤ instructionï¼š{instruction}")
    print(f"  - æ‘˜è¦å…§å®¹å‰ 50 å­—ï¼š{text[:50]}{'...' if len(text)>50 else ''}")

    try:
        response = requests.post(POWERAUTOMATE_SUMMARY_URL, json={"prompt": prompt}, timeout=120)
        result = response.json()
        summary = result.get("summary", "").strip()
        print(f"  - GPT æ‘˜è¦çµæœï¼š{summary[:80]}{'...' if len(summary)>80 else ''}")
        print("ğŸŸ¦ [run_gpt_summary] å®Œæˆ\n")
        return summary
    except Exception as e:
        print("âŒ æ‘˜è¦å¤±æ•—ï¼š", e)
        return "ï¼ˆæ‘˜è¦å¤±æ•—ï¼‰"


def summarize_group_to_excel(config_item, ai_category, group, output_dir="excel_result_Clustered/Summaries"):
    def clean(text):
        return re.sub(r'[^\w\-_.]', '_', str(text).strip())[:30] or "Unknown"
    os.makedirs(output_dir, exist_ok=True)

    print("\nğŸŸ© [summarize_group_to_excel] é–‹å§‹ç¾¤çµ„æ‘˜è¦ ...")
    print(f"  - configItemï¼š{config_item}")
    print(f"  - aiCategoryï¼š{ai_category}")
    print(f"  - ç¾¤çµ„äº‹ä»¶æ•¸é‡ï¼š{len(group)}")

    summaries = [str(r.get("aiSummary", "")).strip() for r in group if r.get("aiSummary")]
    solutions = [str(r.get("solution", "")).strip() for r in group if r.get("solution")]

    summary_text = "\n".join(summaries)
    solution_text = "\n".join(solutions)

    print(f"  - å•é¡Œ summaries æ¢æ•¸ï¼š{len(summaries)}")
    print(f"  - solution æ¢æ•¸ï¼š{len(solutions)}")
    print("  - æ­£åœ¨å‘¼å« GPT ç”¢ç”Ÿã€summary_summaryã€ ...")
    summary_summary = run_gpt_summary(summary_text, "Summarize the problem summaries in a concise form.")
    print("  - æ­£åœ¨å‘¼å« GPT ç”¢ç”Ÿã€solution_summaryã€ ...")
    solution_summary = run_gpt_summary(solution_text, "Summarize the solutions in a concise and helpful form.")

    # âœ… å„²å­˜æˆ Excel
    df = pd.DataFrame([{
        "category": ai_category,
        "configItem": config_item,
        "summary_summary": summary_summary,
        "solution_summary": solution_summary
    }])
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    filename = f"{output_dir}/Summary_[CI]{clean(config_item)}_[AI]{clean(ai_category)}.xlsx"

    df.to_excel(filename, index=False)
    print(f"ğŸ“„ å·²è¼¸å‡ºæ‘˜è¦æª”æ¡ˆï¼š{filename}")
    
    # ====== æ–°å¢ï¼šå†å­˜ä¸€ä»½åˆ° OneDrive ç›®çš„è³‡æ–™å¤¾ (é€™å€‹è³‡æ–™å¤¾åœ¨MSTC ITG sharepoint ç¶²ç«™ä¸Š)======
    custom_export_dir = r"C:\Users\tachang\Microsoft\MSTC ITG - Timmy\IncidentAnalysis_Clustered_File\Summaries"
    os.makedirs(custom_export_dir, exist_ok=True)
    custom_filename = os.path.join(custom_export_dir, os.path.basename(filename))
    df.to_excel(custom_filename, index=False)
    print(f"ğŸ“„ å·²é¡å¤–å‚™ä»½æ‘˜è¦åˆ°ï¼š{custom_filename}")
    # ==================================================    

    print("ğŸŸ© [summarize_group_to_excel] å®Œæˆ\n")






@app.route('/download-summary', methods=['GET'])
def download_summary():
    filename = request.args.get('file')
    path = os.path.join('excel_result_Clustered', 'Summaries', filename)
    if os.path.exists(path):
        return send_file(path, as_attachment=True)
    return jsonify({'error': 'æ‰¾ä¸åˆ°æª”æ¡ˆ'}), 404


@app.route('/summary-files', methods=['GET'])
def list_summary_files():
    # ç¢ºä¿ summary è³‡æ–™å¤¾å­˜åœ¨
    summary_folder = os.path.join('excel_result_Clustered', 'Summaries')
    if not os.path.exists(summary_folder):
        return jsonify({'files': [], 'total': 0})

    # å–å¾—åˆ†æ‰¹åƒæ•¸ï¼Œé è¨­ç¬¬ä¸€æ¬¡ offset=0, limit=100
    offset = int(request.args.get('offset', 0))
    limit = int(request.args.get('limit', 100))

    # å–å¾—å…¨éƒ¨ç¬¦åˆæ¢ä»¶çš„ .xlsx æª”æ¡ˆï¼ˆå»ºè­°æŒ‰æª”åæ’åºï¼Œæ–¹ä¾¿ç”¨æˆ¶æ‰¾ï¼‰
    all_files = [
        f for f in os.listdir(summary_folder)
        if f.endswith('.xlsx')
    ]
    all_files.sort()  # ä½ ä¹Ÿå¯ä»¥æ”¹æˆç”¨æ—¥æœŸã€å¤§å°æ’åº

    total = len(all_files)  # å…¨éƒ¨çš„ summary æª”æ¡ˆæ•¸
    page_files = all_files[offset:offset + limit]  # é€™æ¬¡è¦å›å‚³çš„é€™ä¸€æ‰¹

    files_info = []
    for f in page_files:
        filepath = os.path.join(summary_folder, f)
        try:
            df = pd.read_excel(filepath)
            row_count = len(df)
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å– {f}ï¼š{e}")
            row_count = 0
        files_info.append({
            'name': f,
            'rows': row_count
        })

    return jsonify({'files': files_info, 'total': total})




# ------------------------------------------------------------------------------



# ç”¨æ–¼åŒæ­¥ Flask è·¯ç”±å‘¼å« async åˆ†æé‚è¼¯
def analyze_excel(filepath, weights=None, resolution_priority=None, summary_priority=None):
    return asyncio.run(analyze_excel_async(filepath, weights, resolution_priority, summary_priority))



# ç”¨æ–¼åŒæ­¥ Flask è·¯ç”±å‘¼å« async åˆ†æé‚è¼¯
async def analyze_excel_async(filepath, weights=None, resolution_priority=None, summary_priority=None):
    start_time = time.time()
    append_progress("ğŸŸ© é–‹å§‹åˆ†æ Excel æª”æ¡ˆ...")
    default_weights = {
        'keyword': 5.0,
        'multi_user': 3.0,
        'escalation': 2.0,
        'config_item': 5.0,
        'role_component': 3.0,
        'time_cluster': 2.0
    }
    weights = {**default_weights, **(weights or {})}
    print(f"ğŸŸ© æœ¬æ¬¡åˆ†æé–‹å§‹ï¼Œå°‡å³æ™‚è®€å–ä¸‰é¡èªå¥ json æª”æ¡ˆ...")
    # â­ è®€å–èªå¥å’Œ embedding
    high_risk_examples, high_risk_embeddings = load_embeddings("high_risk")
    escalation_examples, escalation_embeddings = load_embeddings("escalate")
    multi_user_examples, multi_user_embeddings = load_embeddings("multi_user")
    append_progress("âœ… èªå¥åº«èˆ‡åµŒå…¥å‘é‡è¼‰å…¥å®Œæˆ")


    df = pd.read_excel(filepath)
    append_progress(f"âœ… æˆåŠŸè¼‰å…¥æª”æ¡ˆï¼Œå…± {len(df)} ç­†è³‡æ–™ï¼Œé–‹å§‹æ¬„ä½å‰è™•ç†")
    


    # âœ… æ¬„ä½é †ä½ fallback é è¨­
    resolution_priority = resolution_priority or ['Description', 'Short description', 'Close notes']
    summary_priority = summary_priority or ['Short description', 'Description']

    def combine_fields_with_priority(row, field_order, limit):
        parts = []
        for f in field_order:
            if f in row and pd.notna(row[f]):
                value = str(row[f]).strip()
                parts.append(f"{f}: {value}")
        combined = "\n".join(parts)
        while len(combined) > limit and len(parts) > 1:
            removed = parts.pop()
            print(f"ğŸ” ç§»é™¤æ¬„ä½ï¼š{removed[:20]}...")
            combined = "\n".join(parts)

        # é¡å¤–ï¼šå°å‡ºå¯¦éš›ä½¿ç”¨çš„æ¬„ä½åç¨±
        used_fields = field_order[:len(parts)]
        print(f"âœ… å¯¦éš›ä½¿ç”¨æ¬„ä½ï¼š{used_fields}ï¼Œåˆä½µé•·åº¦ï¼š{len(combined)}")
        print("ğŸ” åˆä½µçµæœå…§å®¹ï¼š\n", combined)   # ğŸŸ¢ æ–°å¢é€™è¡Œï¼
        return combined.strip()

    # âœ… ç”¢ç”Ÿ resolution_input / summary_input çµ¦ GPT ç”¨
    df['resolution_input'] = df.apply(lambda row: combine_fields_with_priority(row, resolution_priority, 16000), axis=1)
    df['summary_input'] = df.apply(lambda row: combine_fields_with_priority(row, summary_priority, 16000), axis=1)
    append_progress("âœ… Resolution / Summary æ¬„ä½å…§å®¹åˆä½µå®Œç•¢ï¼Œæº–å‚™é€²è¡Œé€åˆ—åˆ†æ")



    component_counts = df['Role/Component'].value_counts()
    configuration_item_counts = df['Configuration item'].value_counts()
    configuration_item_max = configuration_item_counts.max()
    df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')
    analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    append_progress("â³ æ­£åœ¨åˆ†ææ‰€æœ‰è³‡æ–™ï¼Œè«‹ç¨å€™â€¦")
    print(f"ğŸ“Š è§’è‰²/çµ„ä»¶æ•¸é‡ï¼š{len(component_counts)}ï¼Œæœ€å¤§å€¼ï¼š{configuration_item_max}")
    # éåŒæ­¥è™•ç†
    tasks = [
        analyze_row_async(
            row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time,
            high_risk_examples, high_risk_embeddings,
            escalation_examples, escalation_embeddings,
            multi_user_examples, multi_user_embeddings,
            row['resolution_input'], row['summary_input']  # âœ… æ–°å¢é€™å…©æ¬„
        )
        for idx, row in df.iterrows()
    ]
    # results_raw = await asyncio.gather(*tasks, return_exceptions=True)
    # results = [r for r in results_raw if r and not isinstance(r, Exception)]
    
    results = []
    total = len(tasks)
    finished = 0
    for coro in asyncio.as_completed(tasks):
        res = await coro
        finished += 1
        append_progress(f"ğŸ”„ åˆ†æé€²åº¦ï¼šç¬¬ {finished} / {total} ç­†")
        if res:
            results.append(res)

    # âœ… é˜²å‘†ï¼šæ²’æœ‰ä»»ä½•æˆåŠŸçš„çµæœå°±ç›´æ¥å›å‚³é¿å…å´©æ½°
    if not results:
        print("âš ï¸ æ‰€æœ‰è³‡æ–™éƒ½ç„¡æ³•åˆ†æï¼Œè«‹æª¢æŸ¥æ¬„ä½æ˜¯å¦ç¼ºå¤±")
        return {
            'data': [],
            'analysisTime': analysis_time
        }

    all_scores = [r['impactScore'] for r in results]
    score_range = max(all_scores) - min(all_scores)
    score_std = np.std(all_scores)

    print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

    if (
        len(all_scores) >= KMEANS_MIN_COUNT and
        score_range >= KMEANS_MIN_RANGE and
        score_std >= KMEANS_MIN_STDDEV
    ):
        kmeans = KMeans(n_clusters=4, random_state=42)
        labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
        centroids = kmeans.cluster_centers_.flatten()
        set_kmeans_thresholds_from_centroids(centroids)
        print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
        label_map = {}
        for i, idx in enumerate(np.argsort(centroids)[::-1]):
            label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
        for i, r in enumerate(results):
            r['riskLevel'] = label_map[labels[i]]
        print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
        
        append_progress(f"ğŸ“ˆ KMeans åˆ†ç¾¤å®Œæˆï¼Œä¸­å¿ƒå€¼: {sorted(centroids, reverse=True)}")

    else:
        print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
        append_progress("âš ï¸ KMeans æœªå•Ÿç”¨ï¼Œä½¿ç”¨å›ºå®šé–€æª»é€²è¡Œåˆ†ç´š")

        for r in results:
            r['riskLevel'] = get_risk_level(r['impactScore'])

    total_time = time.time() - start_time
    avg_time = total_time / len(results)
    append_progress(f"âœ… åˆ†æå®Œæˆï¼ç¸½è€—æ™‚ {total_time:.2f} ç§’ï¼Œå¹³å‡æ¯ç­† {avg_time:.2f} ç§’")
    print(f"\nğŸ¯ æ‰€æœ‰åˆ†æç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’")
    print(f"ğŸ“Š å–®ç­†å¹³å‡è€—æ™‚ï¼š{avg_time:.2f} ç§’")
    print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
    return {
        'data': results,
        'analysisTime': analysis_time
    }
    
    
async def analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time,     
    high_risk_examples, high_risk_embeddings,
    escalation_examples, escalation_embeddings,
    multi_user_examples, multi_user_embeddings,
    resolution_text, summary_input):
    print(f"[åˆ†æ Row#{idx+1}] æœ¬æ¬¡ç”¨çš„é«˜é¢¨éšªèªå¥æ•¸ï¼š{len(high_risk_examples)}ï¼Œå€’æ•¸å…©å¥ï¼š{high_risk_examples[-2:] if high_risk_examples else 'ç©º'}")
    print(f"[åˆ†æ Row#{idx+1}] æœ¬æ¬¡ç”¨çš„å‡ç´šèªå¥æ•¸ï¼š{len(escalation_examples)}ï¼Œå€’æ•¸å…©å¥ï¼š{escalation_examples[-2:] if escalation_examples else 'ç©º'}")
    print(f"[åˆ†æ Row#{idx+1}] æœ¬æ¬¡ç”¨çš„å½±éŸ¿å¤šä½¿ç”¨è€…èªå¥æ•¸ï¼š{len(multi_user_examples)}ï¼Œå€’æ•¸å…©å¥ï¼š{multi_user_examples[-2:] if multi_user_examples else 'ç©º'}")
    
    
    
    
    try:
        # åŸå§‹æ¬„ä½ä¿ç•™
        description_text = row.get('Description', 'not filled')
        short_description_text = row.get('Short Description', 'not filled')
        close_note_text = row.get('Close notes', 'not filled')

        # å­—ä¸²æ¸…ç†ï¼ˆä¿ç•™è®Šæ•¸å‘½åï¼‰
        desc = str(description_text).strip()
        short_desc = str(short_description_text).strip()
        close_notes = str(close_note_text).strip()
        print("-------------------------------------------------------------------test----------------------------------------------------------------------------------------")
        print(desc, short_desc, close_notes)

        # è‹¥å…¨éƒ¨å…§å®¹çš†ç‚ºç©ºï¼Œç›´æ¥è·³éæ­¤ç­†
        if not (desc or short_desc or close_notes):
            print(f"âš ï¸ ç¬¬ {idx+1} ç­†å…§å®¹å…¨ç‚ºç©ºç™½ï¼Œç•¥éåˆ†æ")
            return None

        # âœ… æ”¹ç”¨åˆä½µå¾Œæ¬„ä½ï¼ˆå·²ç”±å‰æ®µ fallback è™•ç†ï¼‰
        print(f"ğŸ§  [Row#{idx+1}] ä½¿ç”¨ resolution_textï¼ˆé•·åº¦ï¼š{len(resolution_text)}ï¼‰")
        print(f"ğŸ“Œ Resolution æ¬„ä½åŸå§‹åˆä½µå…§å®¹ï¼š\n{resolution_text[:1000]}")
        print(f"ğŸ§  [Row#{idx+1}] ä½¿ç”¨ summary_inputï¼ˆé•·åº¦ï¼š{len(summary_input)}ï¼‰")
        print(f"ğŸ“Œ Summary æ¬„ä½åŸå§‹åˆä½µå…§å®¹ï¼š\n{summary_input[:1000]}")
        print("-------------------------------------------------------------------test----------------------------------------------------------------------------------------")
        print(desc, short_desc, close_notes)


        keyword_score = is_high_risk(summary_input, high_risk_examples, high_risk_embeddings)
        user_impact_score = is_multi_user(summary_input, multi_user_examples, multi_user_embeddings)
        escalation_score = is_escalated(resolution_text, escalation_examples, escalation_embeddings)

        config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)
        configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0

        role_comp = row.get('Role/Component', 'not filled')
        count = component_counts.get(role_comp, 0)
        role_component_freq = 3 if count >= 5 else 2 if count >= 3 else 1 if count == 2 else 0

        this_time = row.get('Opened', 'not filled')
        if pd.isnull(this_time):
            time_cluster_score = 1
        else:
            others = df[df['Role/Component'] == role_comp]
            close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
                                  (others['Opened'] <= this_time + pd.Timedelta(hours=24))]
            count_cluster = len(close_events)
            time_cluster_score = 3 if count_cluster >= 3 else 2 if count_cluster == 2 else 1

        severity_score = round(
            keyword_score * weights['keyword'] +
            user_impact_score * weights['multi_user'] +
            escalation_score * weights['escalation'], 2
        )
        frequency_score = round(
            configuration_item_freq * weights['config_item'] +
            role_component_freq * weights['role_component'] +
            time_cluster_score * weights['time_cluster'], 2
        )
        impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
        risk_level = get_risk_level(impact_score)
        
        
        
        ai_suggestion, ai_summary = await analyze_with_ai_builder_then_fallback(
        resolution_text, summary_input, source_id=f"Row#{idx+1}"
        )

        return {
            'id': safe_value(row.get('Incident') or row.get('Number')),
            'configurationItem': safe_value(row.get('Configuration item')),
            'roleComponent': safe_value(row.get('Role/Component')),
            'subcategory': safe_value(row.get('Subcategory')),
            'aiSummary': safe_value(ai_summary),
            'originalShortDescription': safe_value(short_desc),
            'originalDescription': safe_value(desc),
            'severityScore': safe_value(severity_score),
            'frequencyScore': safe_value(frequency_score),
            'impactScore': safe_value(impact_score),
            'severityScoreNorm': round(severity_score / 10, 2),
            'frequencyScoreNorm': round(frequency_score / 20, 2),
            'impactScoreNorm': round(impact_score / 30, 2),
            'riskLevel': risk_level,
            'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
            'location': safe_value(row.get('Location')),
            'opened': row.get('Opened'),  # âœ… æ–°å¢é€™è¡Œ
            'analysisTime': analysis_time,
            'weights': {k: round(v / 10, 2) for k, v in weights.items()},
            'usedResolutionInput': safe_value(resolution_text),
            'usedSummaryInput': safe_value(summary_input),

        }

    except Exception as e:
        append_progress(f"âŒ åˆ†æç¬¬ {idx+1} ç­†å¤±æ•—ï¼š{str(e)[:60]}")
        print(f"âŒ åˆ†æç¬¬ {idx + 1} ç­†å¤±æ•—ï¼š", e)
        return None





@app.route('/cluster-progress')
def cluster_progress():
    try:
        with open("cluster_progress.json", encoding="utf-8") as f:
            return jsonify(json.load(f))
    except Exception:
        return jsonify({"progress": 0, "total": 1, "status": "å°šæœªé–‹å§‹"})




SENTENCE_DIR = os.path.join("data", "sentences")
os.makedirs(SENTENCE_DIR, exist_ok=True)

def get_file_path(tag):
    return os.path.join(SENTENCE_DIR, f"{tag}.json")

@app.route("/get-sentence-db")
def get_sentence_db():
    result = []
    for tag in ["high_risk", "escalate", "multi_user"]:
        path = get_file_path(tag)
        if os.path.exists(path):
            with open(path, encoding='utf-8') as f:
                sentences = json.load(f)
                for entry in sentences:
                    result.append({"text": entry["text"], "tag": tag})
    return jsonify(result)

@app.route("/save-sentence-db", methods=["POST"])
def save_sentence():
    new_entry = request.get_json()
    tag = new_entry.get("tag")
    if tag not in ["high_risk", "escalate", "multi_user"]:
        return jsonify({"message": "invalid tag"}), 400

    path = get_file_path(tag)
    if os.path.exists(path):
        with open(path, encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = []

    if any(d['text'] == new_entry['text'] for d in data):
        return jsonify({"message": "duplicate"}), 409

    data.append({"text": new_entry['text']})
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return get_sentence_db()

@app.route("/delete-sentence", methods=["POST"])
def delete_sentence():
    req = request.get_json()
    tag = req.get("tag")
    text = req.get("text")
    if tag not in ["high_risk", "escalate", "multi_user"] or not text:
        return jsonify({"message": "invalid input"}), 400

    path = get_file_path(tag)
    if not os.path.exists(path):
        return jsonify({"message": "not found"}), 404

    with open(path, encoding='utf-8') as f:
        data = json.load(f)

    new_data = [d for d in data if d['text'] != text]
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(new_data, f, ensure_ascii=False, indent=2)

    return get_sentence_db()

@app.route("/edit-sentence", methods=["POST"])
def edit_sentence():
    req = request.get_json()
    tag = req.get("tag")
    old_text = req.get("oldText")
    new_text = req.get("newText")

    if tag not in ["high_risk", "escalate", "multi_user"] or not old_text or not new_text:
        return jsonify({"message": "invalid input"}), 400

    path = get_file_path(tag)
    if not os.path.exists(path):
        return jsonify({"message": "not found"}), 404

    with open(path, encoding='utf-8') as f:
        data = json.load(f)

    updated = False
    for d in data:
        if d['text'] == old_text:
            d['text'] = new_text
            updated = True
            break

    if not updated:
        return jsonify({"message": "not found"}), 404

    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return get_sentence_db()








GPT_DATA_DIR = "gpt_data"
PROMPT_FILE = os.path.join(GPT_DATA_DIR, "gpt_prompts.json")
MAP_FILE = os.path.join(GPT_DATA_DIR, "gpt_prompt_map.json")

os.makedirs(GPT_DATA_DIR, exist_ok=True)

def read_json(path, default=None):
    if os.path.exists(path):
        with open(path, encoding='utf-8') as f:
            return json.load(f)
    return default if default is not None else {}

def write_json(path, obj):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

@app.route('/get-gpt-prompts')
def get_gpt_prompts():
    all_data = read_json(PROMPT_FILE, {})
    # åŒ…æˆ {ç”¨é€”: {"prompts": [...]} }
    wrapped = {k: {"prompts": v if isinstance(v, list) else [v]} for k, v in all_data.items()}
    return jsonify(wrapped)

@app.route('/save-gpt-prompt', methods=['POST'])
def save_gpt_prompt():
    """
    æ–°å¢ä¸€ç­† prompt åˆ°æŸå€‹åˆ†é¡ã€‚body: { "task": "solution", "prompt": "xxx" }
    """
    data = request.get_json()
    task = data.get('task')
    prompt = data.get('prompt', '').strip()

    if not task or not prompt:
        return jsonify(success=False, message='âŒ ç¼ºå°‘ task æˆ– prompt'), 400

    all_prompts = read_json(PROMPT_FILE, {})
    prompt_list = all_prompts.get(task, [])
    if not isinstance(prompt_list, list):
        prompt_list = [prompt_list]

    if prompt not in prompt_list:
        prompt_list.append(prompt)
    else:
        return jsonify(success=False, message='âš ï¸ è©² prompt å·²å­˜åœ¨'), 409

    all_prompts[task] = prompt_list
    write_json(PROMPT_FILE, all_prompts)

    return jsonify(success=True, allPrompts=all_prompts)

@app.route('/delete-gpt-prompt', methods=['POST'])
def delete_gpt_prompt():
    """
    æ”¯æ´åˆªé™¤åˆ†é¡æˆ–åˆ†é¡ä¸‹çš„å–®ä¸€å¥å­ã€‚
    body: { "task": "solution", "prompt": "xxx" } æˆ–åªçµ¦ task ä»£è¡¨æ•´é¡åˆªé™¤ã€‚
    """
    data = request.get_json()
    task = data.get('task')
    prompt = data.get('prompt', '').strip()

    all_prompts = read_json(PROMPT_FILE, {})

    if task not in all_prompts:
        return jsonify(success=False, message=f'æ‰¾ä¸åˆ°ç”¨é€” {task}'), 404

    if prompt:
        prompt_list = all_prompts[task]
        if prompt in prompt_list:
            prompt_list.remove(prompt)
            if prompt_list:
                all_prompts[task] = prompt_list
            else:
                del all_prompts[task]
        else:
            return jsonify(success=False, message='æ‰¾ä¸åˆ°è©² prompt'), 404
    else:
        del all_prompts[task]  # åˆªæ•´é¡

    write_json(PROMPT_FILE, all_prompts)

    # åˆªæ‰ mapping ä¸­çš„å°æ‡‰
    mapping = read_json(MAP_FILE, {})
    if task in mapping:
        del mapping[task]
        write_json(MAP_FILE, mapping)

    return jsonify(success=True, allPrompts=all_prompts)

@app.route('/get-gpt-prompt-map')
def get_gpt_prompt_map():
    return jsonify(read_json(MAP_FILE, {}))

@app.route('/save-gpt-prompt-map', methods=['POST'])
def save_gpt_prompt_map():
    data = request.get_json()

    solution_prompt = data.get("solution")
    summary_prompt = data.get("ai_summary")
    models = data.get("models", {})

    new_mapping = {
        "solution": {
            "prompt": solution_prompt,
            "model": models.get("solution", "")
        },
        "ai_summary": {
            "prompt": summary_prompt,
            "model": models.get("ai_summary", "")
        }
    }

    write_json(MAP_FILE, new_mapping)
    return jsonify(success=True, mapping=new_mapping)

def get_prompt_for_use(use_type):
    mapping = read_json(MAP_FILE, {})
    all_prompts = read_json(PROMPT_FILE, {})
    mapped_key = mapping.get(use_type, use_type)
    prompt_list = all_prompts.get(mapped_key, [])
    if isinstance(prompt_list, list):
        return {'prompt': prompt_list[0] if prompt_list else '', 'model': mapping.get(mapped_key, {}).get("model", "")}
    return {'prompt': prompt_list, 'model': mapping.get(mapped_key, {}).get("model", "")}

# å®šç¾©é¦–é è·¯ç”±
@app.route('/')
def index():
    return render_template('FrontEnd.html')  # æ¸²æŸ“é¦–é æ¨¡æ¿

# å®šç¾©çµæœé é¢è·¯ç”±
@app.route('/result')
def result_page():
    return render_template('result.html')  # æ¸²æŸ“çµæœé é¢

# å®šç¾©æ­·å²ç´€éŒ„é é¢è·¯ç”±
@app.route('/history')
def history_page():
    return render_template('history.html')  # æ¸²æŸ“æ­·å²ç´€éŒ„é é¢


@app.route('/generate_cluster')
def generate_cluster_page():
    return render_template('generate_cluster.html')  # æ¸²æŸ“ç”Ÿæˆåˆ†ç¾¤é é¢

@app.route("/manual_input")
def manual_input_page():
    return render_template("manual_input.html")

@app.route("/gpt_prompt")
def gpt_prompt_page():
    return render_template("gpt_prompt.html")

@app.route("/chat_ui")
def helpdesk_ui():
    return render_template("chat.html")



# ------------------------------------------------------------------------------

@app.route("/chat", methods=["POST"])
def chat_with_model():
    data = request.get_json()
    message = data.get("message", "")
    model = data.get("model", "mistral") # model é è¨­æ˜¯ mistral
    history = data.get("history", [])
    chat_id = data.get("chatId", "")  # âœ… å‰ç«¯å‚³å…¥çš„å”¯ä¸€ ID

    if not chat_id:
        return jsonify({"error": "Missing chatId"}), 400

    # å»ºç«‹è³‡æ–™å¤¾èˆ‡æª”æ¡ˆè·¯å¾‘
    os.makedirs("chat_history", exist_ok=True)
    file_path = os.path.join("chat_history", f"{chat_id}.json")

    try:
        # âœ… å‘¼å« GPT æ¨¡å‹è™•ç†ï¼ˆä½ çš„æ ¸å¿ƒé‚è¼¯ï¼‰
        reply = asyncio.run(run_offline_gpt(message, model=model, history=history, chat_id=chat_id))
        
        print("ğŸ§¾ reply é¡å‹ï¼š", type(reply))
        print("ğŸ§¾ reply å…§å®¹é è¦½ï¼š", str(reply)[:1000])

        # âœ… å®‰å…¨è™•ç†ï¼šä¿è­‰ reply ä¸€å®šæ˜¯ strï¼Œé¿å… list/dict éŒ¯èª¤
        if not isinstance(reply, str):
            print("âš ï¸ reply ä¸æ˜¯å­—ä¸²ï¼Œè‡ªå‹•è½‰æ›ç‚ºå®‰å…¨æ ¼å¼")
            try:
                reply = json.dumps(reply, ensure_ascii=False)
            except Exception as e:
                reply = f"âš ï¸ å›å‚³æ ¼å¼éŒ¯èª¤ï¼š{e}"

        # âœ… åˆ¤æ–·æ˜¯æ–°è©±é¡Œé‚„æ˜¯ç¹¼çºŒèŠ
        if not os.path.exists(file_path):
            print("ğŸ†• æ–°çš„å°è©±ç´€éŒ„ï¼Œå»ºç«‹æ–°æª”æ¡ˆ")
            # ğŸ†• é¦–æ¬¡å»ºç«‹æ–°æª”æ¡ˆ
            chat_record = {
                "id": chat_id,
                "title": chat_id,     # å›ºå®šç‚ºæª”å
                "edit_title": "",     # é è¨­ç©º
                "model": model,
                "timestamp": datetime.now().isoformat(),
                "history": history + [
                    {"role": "user", "content": message},
                    {"role": "assistant", "content": reply}
                ]
            }
        else:
            # ğŸ” è¼‰å…¥åŸæª”æ¡ˆä¸¦è¿½åŠ 
            with open(file_path, "r", encoding="utf-8") as f: # æ‰“é–‹æ—¢æœ‰çš„å°è©±ç´€éŒ„æª”æ¡ˆï¼ˆJSON æ ¼å¼ï¼‰
                chat_record = json.load(f) 
                
            # ğŸ›¡ï¸ é˜²å‘†ï¼šå¦‚æœ history æ˜¯ç©ºçš„æˆ–æ ¼å¼éŒ¯èª¤ï¼Œå¼·åˆ¶åˆå§‹åŒ–ç‚º list
            if "history" not in chat_record or not isinstance(chat_record["history"], list):
                print("âš ï¸ history æ ¼å¼éŒ¯èª¤ï¼Œè‡ªå‹•åˆå§‹åŒ–ç‚ºç©º list")
                chat_record["history"] = []

            chat_record["history"].append({"role": "user", "content": message}) # è¿½åŠ ä½¿ç”¨è€…çš„è¨Šæ¯
            chat_record["history"].append({"role": "assistant", "content": reply}) # è¿½åŠ åŠ©æ‰‹çš„å›è¦†
            print("ğŸ“ å·²è¼‰å…¥æ—¢æœ‰å°è©±ç´€éŒ„ï¼Œä¸¦è¿½åŠ æ–°çš„è¨Šæ¯ã€‚")

        # âœ… å¯«å›æª”æ¡ˆ
        with open(file_path, "w", encoding="utf-8") as f: # æ‰“é–‹æª”æ¡ˆæº–å‚™å¯«å…¥,å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œæœƒè¦†è“‹åŸå…§å®¹ã€‚
            # å°‡å°è©±ç´€éŒ„å¯«å…¥ JSON æª”æ¡ˆ
            json.dump(chat_record, f, ensure_ascii=False, indent=2)

        print(f"âœ… å°è©±ç´€éŒ„å·²å„²å­˜åˆ° {file_path}")
        return jsonify({"reply": reply}) # å›å‚³åŠ©æ‰‹çš„å›è¦†ç”¨jsonå½¢å¼

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤test: {str(e)}")  # âœ… åŠ ä¸ŠéŒ¯èª¤å…§å®¹
        return jsonify({"error": str(e)}), 500 # å¦‚æœç™¼ç”ŸéŒ¯èª¤ï¼Œå›å‚³éŒ¯èª¤è¨Šæ¯



@app.route("/rename-chat", methods=["POST"])
def rename_chat():
    data = request.get_json()
    chat_id = data.get("chatId")
    new_title = data.get("newTitle")

    if not chat_id or new_title is None:
        return jsonify({"error": "ç¼ºå°‘åƒæ•¸"}), 400

    file_path = Path(f"chat_history/{chat_id}.json")
    if not file_path.exists():
        return jsonify({"error": "æ‰¾ä¸åˆ°æª”æ¡ˆ"}), 404

    try:
        with open(file_path, encoding="utf-8") as f:
            record = json.load(f)
        record["edit_title"] = new_title  # âœ… å¯«å…¥æ–°æ¨™é¡Œ
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    

@app.route("/delete-chat/<chat_id>", methods=["DELETE"])
def delete_chat(chat_id):
    file_path = Path(f"chat_history/{chat_id}.json")
    if file_path.exists():
        try:
            file_path.unlink()  # åˆªé™¤æª”æ¡ˆ
            return jsonify({"success": True})
        except Exception as e:
            return jsonify({"error": f"ç„¡æ³•åˆªé™¤ï¼š{e}"}), 500
    return jsonify({"error": "æª”æ¡ˆä¸å­˜åœ¨"}), 404



@app.route("/chat-history-list")
def get_chat_history_list():
    records = []
    path = Path("chat_history")
    if not path.exists():
        return jsonify([])

    for file in path.glob("*.json"):
        with open(file, encoding="utf-8") as f:
            try:
                obj = json.load(f)
                records.append({
                    "id": obj.get("id"),
                    "title": obj.get("edit_title") or obj.get("title"),
                    "timestamp": obj.get("timestamp"),
                    "model": obj.get("model")
                })
            except:
                continue
    return jsonify(sorted(records, key=lambda x: x["timestamp"], reverse=True))

@app.route("/chat-history/<id>")
def get_chat_history_by_id(id):
    file_path = Path(f"chat_history/{id}.json")
    if not file_path.exists():
        return jsonify({"error": "not found"}), 404
    with open(file_path, encoding="utf-8") as f:
        return jsonify(json.load(f))




@app.route('/preview-excel', methods=['POST'])
def preview_excel():
    file = request.files.get('file')
    if not file:
        return jsonify({'error': 'æœªæä¾›æª”æ¡ˆ'})

    try:
        df = pd.read_excel(file)
        columns = df.columns.tolist()
        rows = df.head(50).fillna('').astype(str).to_dict(orient='records')  # é è¦½å‰ 50 ç­†è³‡æ–™
        return jsonify({'columns': columns, 'rows': rows})
    except Exception as e:
        return jsonify({'error': str(e)})
    
    
@app.route('/get-progress')
def get_progress():
    global progress_log
    return jsonify({"progress": progress_log})






@app.route('/ping')
def ping():
    return "pong", 200


@app.route('/upload', methods=['POST'])
def upload_file():
    global progress_log
    progress_log = ""     # æ¯æ¬¡æ–°ä¸Šå‚³éƒ½å…ˆæ¸…ç©ºé€²åº¦
    append_progress("âœ… é–‹å§‹åˆ†æ ...")
    print("ğŸ“¥ æ”¶åˆ°ä¸Šå‚³è«‹æ±‚")

    if 'file' not in request.files:
        print("âŒ æ²’æœ‰ file æ¬„ä½")
        return jsonify({'error': 'æ²’æœ‰æ‰¾åˆ°æª”æ¡ˆæ¬„ä½'}), 400

    file = request.files['file']
    if file.filename == '':
        print("âš ï¸ æª”æ¡ˆåç¨±ç‚ºç©º")
        return jsonify({'error': 'æœªé¸æ“‡æª”æ¡ˆ'}), 400

    if not allowed_file(file.filename):
        print("âš ï¸ æª”æ¡ˆé¡å‹ä¸ç¬¦")
        return jsonify({'error': 'è«‹ä¸Šå‚³ .xlsx æª”æ¡ˆ'}), 400

    # æ¥æ”¶æ¬Šé‡è¨­å®š
    weights = None
    weights_raw = request.form.get('weights')
    if weights_raw:
        try:
            weights = json.loads(weights_raw)
            print("ğŸ“¥ æ”¶åˆ°æ¬Šé‡è¨­å®šï¼š", weights)
        except Exception as e:
            print(f"âš ï¸ æ¬Šé‡è§£æå¤±æ•—ï¼š{e}")
            return jsonify({'error': 'æ¬Šé‡è§£æå¤±æ•—'}), 400
    else:
        print("â„¹ï¸ æœªæä¾›è‡ªè¨‚æ¬Šé‡ï¼Œä½¿ç”¨é è¨­å€¼åˆ†æ")

    # è§£æ resolution/summary æ¬„ä½é †ä½
    try:
        resolution_priority = json.loads(request.form.get('resolution_priority', '[]'))
        summary_priority = json.loads(request.form.get('summary_priority', '[]'))
        print("ğŸ“Œ Resolution é †ä½æ¬„ä½ï¼š", resolution_priority)
        print("ğŸ“Œ Summary é †ä½æ¬„ä½ï¼š", summary_priority)
    except Exception as e:
        print(f"âš ï¸ æ¬„ä½é †ä½è§£æå¤±æ•—ï¼š{e}")
        return jsonify({'error': 'æ¬„ä½é †ä½è§£æå¤±æ•—'}), 400

    # ç”¢ç”Ÿæ™‚é–“æˆ³è¨˜èˆ‡æª”å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    uid = f"result_{timestamp}"
    original_filename = f"original_{timestamp}.xlsx"
    original_path = os.path.join('uploads', original_filename)

    try:
        file.save(original_path)
        print(f"ğŸ“ åŸå§‹æª”å·²å„²å­˜ï¼š{original_path}")
    except Exception as e:
        return jsonify({'error': f'å„²å­˜åŸå§‹æª”å¤±æ•—ï¼š{str(e)}'}), 500

    try:
        # å‘¼å«åˆ†æä¸»é‚è¼¯
        analysis_result = analyze_excel(
            original_path,
            weights=weights,
            resolution_priority=resolution_priority,
            summary_priority=summary_priority
        )
        results = analysis_result['data']
        # â¬‡ï¸â¬‡ï¸â¬‡ï¸ å°±åŠ åœ¨é€™ï¼
        analysis_result['file'] = file.filename

        save_analysis_files(analysis_result, uid)
        print(f"âœ… åˆ†æå®Œæˆï¼Œå…± {len(results)} ç­†")




        # è‡ªå‹•è§¸ç™¼å»ºåº«è…³æœ¬
        print("ğŸš€ è‡ªå‹•åŸ·è¡Œ build_kb.py å»ºç«‹çŸ¥è­˜åº«")
        # å‘¼å«æœ¬åœ°çš„ Python åŸ·è¡Œ build_kb.pyï¼ˆä¿è­‰å’Œ Flask ç”¨åŒä¸€å€‹è§£è­¯å™¨ï¼‰
        script_path = os.path.join(os.path.dirname(__file__), "build_kb.py")
        print("ğŸš€ å˜—è©¦ç”¨ sys.executable åŸ·è¡Œï¼š", script_path)
        subprocess.Popen([sys.executable, script_path])

        # é€™è£¡ç›´æ¥è®€æœ€æ–°çš„åˆ†ææª”
        json_path = os.path.join('json_data', f"{uid}.json")
        if os.path.exists(json_path):
            with open(json_path, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
                results = loaded.get('data', [])
        else:
            results = []
        print(f"ğŸ“ æœ€æ–°åˆ†ææª”å·²è®€å–ï¼š{json_path}")
        
        return jsonify({
            'data': results[:100],   # åªå›å‚³å‰ 100 ç­†
            'uid': uid,
            'weights': weights,
            'jsonFilename': f"{uid}.json"  # æ–°å¢é€™ä¸€è¡Œ
        }), 200
    except Exception as e:
        print(f"âŒ åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500
    


def make_json_serializable(obj):
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    if isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [make_json_serializable(v) for v in obj]
    return obj




import os
import pandas as pd
import openpyxl
from openpyxl.worksheet.table import Table, TableStyleInfo
from openpyxl.utils import get_column_letter
from openpyxl import load_workbook



def append_df_to_excel(df, excel_path, sheet_name="Sheet1"):
    if not os.path.exists(excel_path):
        print(f"ğŸ“ æª”æ¡ˆä¸å­˜åœ¨ï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆï¼š{excel_path}")
        # å»ºç«‹æ–°æª” + è¡¨æ ¼æ ¼å¼
        with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)
        wb = load_workbook(excel_path)
        ws = wb[sheet_name]
        table = Table(displayName="IncidentTable",
                      ref=f"A1:{get_column_letter(ws.max_column)}{ws.max_row}")
        table.tableStyleInfo = TableStyleInfo(name="TableStyleMedium9", showRowStripes=True)
        ws.add_table(table)
        wb.save(excel_path)
        print("âœ… æ–°æª”æ¡ˆå·²å»ºç«‹ä¸¦è¡¨æ ¼åŒ–")
        return

    # æª”æ¡ˆå­˜åœ¨ï¼šå…ˆè®€ç¾æœ‰è³‡æ–™åˆ—æ•¸èˆ‡ workbook
    reader = pd.read_excel(excel_path, sheet_name=sheet_name, engine="openpyxl")
    book = load_workbook(excel_path)

    # è‹¥ sheet ä¸å­˜åœ¨ï¼Œæ‰‹å‹•å»ºç«‹è¡¨é ­+
    if sheet_name not in book.sheetnames:
        ws = book.create_sheet(title=sheet_name)
        for idx, col in enumerate(df.columns, start=1):
            ws.cell(row=1, column=idx, value=col)
        table = Table(displayName="IncidentTable",
                      ref=f"A1:{get_column_letter(len(df.columns))}1")
        table.tableStyleInfo = TableStyleInfo(name="TableStyleMedium9", showRowStripes=True)
        ws.add_table(table)
        book.save(excel_path)
        print("âœ… å·²å»ºç«‹è¡¨é ­èˆ‡è¡¨æ ¼æ¨£å¼")

    startrow = len(reader)
    print(f"ğŸ“Œ ç›®æ¨™æª”æ¡ˆå·²æœ‰ {startrow} åˆ—ï¼Œå¾ç¬¬ {startrow + 1} åˆ—é–‹å§‹è¿½åŠ ")

    with pd.ExcelWriter(excel_path, engine="openpyxl", mode="a", if_sheet_exists="overlay") as writer:

        # âœ… ä½¿ç”¨ update() è€Œéé‡æ–°è³¦å€¼
        writer.sheets.update({ws.title: ws for ws in book.worksheets})
        df.to_excel(writer, sheet_name=sheet_name, startrow=startrow, index=False, header=False)
    print("âœ… æˆåŠŸ append åˆ†æçµæœåˆ°ç›®æ¨™ Excelï¼")

    # ğŸ“ è‡ªå‹•å»¶å±•è¡¨æ ¼ç¯„åœï¼ˆåŠ å¼·ç©©å®šæ€§ï¼‰
    try:
        wb2 = load_workbook(excel_path)
        ws2 = wb2[sheet_name]

        if ws2.tables:
            table = list(ws2.tables.values())[0]
            new_ref = f"A1:{get_column_letter(ws2.max_column)}{ws2.max_row}"
            print(f"ğŸ“ å»¶å±•è¡¨æ ¼ç¯„åœï¼š{table.ref} âœ {new_ref}")
            table.ref = new_ref
            wb2.save(excel_path)
            print("ğŸ“ è¡¨æ ¼ç¯„åœå·²æ›´æ–°")
            
        else:
            print("âš ï¸ æ‰¾ä¸åˆ°ä»»ä½•è¡¨æ ¼ï¼Œå°‡è£œå»ºè¡¨æ ¼")
            new_ref = f"A1:{get_column_letter(ws2.max_column)}{ws2.max_row}"
            new_table = Table(displayName="IncidentTable", ref=new_ref)
            style = TableStyleInfo(name="TableStyleMedium9", showRowStripes=True)
            new_table.tableStyleInfo = style
            ws2.add_table(new_table)
            wb2.save(excel_path)
            print(f"ğŸ†• å·²è£œå»ºè¡¨æ ¼ä¸¦è¨­å®šç¯„åœï¼š{new_ref}")
            
            
        # âœ… ğŸ” å†æ¬¡è®€å–æª”æ¡ˆï¼Œé€™æ¬¡çœŸæ­£è¨­å®šæ¬„å¯¬ï¼ˆç¢ºä¿å…ˆ appendã€å†èª¿æ•´ï¼‰
        wb3 = load_workbook(excel_path)
        ws3 = wb3[sheet_name]

        # ğŸ§© è‡ªå‹•èª¿æ•´æ¯ä¸€æ¬„æ¬„å¯¬
        print("ğŸ“ æ­£åœ¨è‡ªå‹•èª¿æ•´æ¬„å¯¬...")
        for i in range(1, ws3.max_column + 1):
            max_width = 1
            for j in range(1, ws3.max_row + 1):
                cell_value = ws3.cell(row=j, column=i).value
                if isinstance(cell_value, (int, float)):
                    width = len(str(format(cell_value, ',')))
                elif cell_value is None:
                    width = 0
                else:
                    width = len(str(cell_value).encode('gbk'))  # ä¸­æ–‡ 1 å­— = 2 bytesï¼ˆgbk ç·¨ç¢¼ï¼‰
                max_width = max(max_width, width)
            col_letter = get_column_letter(i)
            ws3.column_dimensions[col_letter].width = min(max_width, 20) + 2  # æœ€å¤š 20ï¼ŒåŠ  2 é¡¯å¾—å¯¬é¬†

        wb3.save(excel_path)
        print("ğŸ“ è¡¨æ ¼èˆ‡æ¬„ä½å¯¬åº¦å·²æ›´æ–°")
        

    except Exception as e:
        print(f"âš ï¸ å»¶å±•è¡¨æ ¼ç¯„åœå¤±æ•—ï¼š{e}")



def deduplicate_by_id_and_time(df: pd.DataFrame) -> pd.DataFrame:
    if "id" not in df.columns or "analysisTime" not in df.columns:
        print("âš ï¸ ç¼ºå°‘ id æˆ– analysisTime æ¬„ä½ï¼Œç„¡æ³•å»é‡")
        return df

    df["id"] = df["id"].astype(str).str.strip()
    df["analysisTime_parsed"] = pd.to_datetime(df["analysisTime"], errors="coerce", utc=True)

    before = len(df)
    df = df.dropna(subset=["id", "analysisTime_parsed"])
    df = df.sort_values("analysisTime_parsed").drop_duplicates(subset="id", keep="last")
    after = len(df)

    print(f"ğŸ§¹ å»é‡å®Œæˆï¼šåŸå§‹ {before} ç­† âœ å»é‡å¾Œ {after} ç­†")
    return df.drop(columns=["analysisTime_parsed"])

# âœ… æ¬„ä½åç¨±å°ç…§ï¼šåç¨± â†’ è¦é€å‡ºçš„åç¨±
FIELD_MAPPING = {
    "id": "id",
    "configurationItem": "configurationItem",
    "roleComponent": "roleComponent",
    "subcategory": "subcategory",
    "aiSummary": "problem",  # â† æ”¹é€™è¡Œ
    "solution": "solution",
    "severityScoreNorm": "severityScoreNorm",   # âœ… ä¿ç•™åŸå
    "frequencyScoreNorm": "frequencyScoreNorm",
    "impactScoreNorm": "impactScoreNorm",
    "riskLevel": "riskLevel",
    "location": "location",
    "opened": "opened",
    "analysisTime": "analysisTime"  # âœ… åˆ¥å¿˜äº†åŠ ä¸Šæ™‚é–“    
}

def apply_excel_formatting(path, sheet_name="Sheet1"):
    wb = load_workbook(path)
    ws = wb[sheet_name]
    new_ref = f"A1:{get_column_letter(ws.max_column)}{ws.max_row}"

    # ğŸ“Š å»ºç«‹è¡¨æ ¼æ¨£å¼
    table = Table(displayName="IncidentTable", ref=new_ref)
    style = TableStyleInfo(name="TableStyleMedium9", showRowStripes=True)
    table.tableStyleInfo = style
    ws.add_table(table)

    # ğŸ“ è‡ªå‹•èª¿æ•´æ¬„å¯¬
    for i in range(1, ws.max_column + 1):
        max_width = 1
        for j in range(1, ws.max_row + 1):
            val = ws.cell(row=j, column=i).value
            if isinstance(val, (int, float)):
                width = len(str(format(val, ',')))
            else:
                width = len(str(val or "").encode('gbk'))
            max_width = max(max_width, width)
        ws.column_dimensions[get_column_letter(i)].width = min(max_width, 20) + 2

    wb.save(path)
    print("ğŸ¨ è¡¨æ ¼æ¨£å¼èˆ‡æ¬„å¯¬å·²è‡ªå‹•èª¿æ•´")



def save_analysis_files(result, uid):
    os.makedirs('json_data', exist_ok=True)
    os.makedirs('excel_result_Unclustered', exist_ok=True)  # âœ… ä½¿ç”¨æ–°çš„è³‡æ–™å¤¾

    # å„²å­˜ JSON
    json_path = os.path.join(basedir, 'json_data', f"{uid}.json")
    print(f"ğŸ“ é è¨ˆå„²å­˜ JSONï¼š{json_path}")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(make_json_serializable(result), f, ensure_ascii=False, indent=2)
    print("âœ… JSON æª”æ¡ˆå·²å¯«å…¥æˆåŠŸ")

    # å„²å­˜åˆ†æå ±è¡¨ Excelï¼ˆåªå„²å­˜ result['data']ï¼‰
    df = pd.DataFrame(result['data'])
    
    print(f"åŒ¯å…¥Excel æª”æ¡ˆï¼š{df}")

    # âœ… åƒ…ä¿ç•™ä¸¦æ’åºç›®æ¨™æ¬„ä½ï¼ˆåŸå§‹æ¬„ä½åç¨±ï¼‰
    columns_to_keep = list(FIELD_MAPPING.keys())
    for col in columns_to_keep:
        if col not in df.columns:
            df[col] = ""
    df = df[columns_to_keep]

    # âœ… å»é‡è™•ç†
    df = deduplicate_by_id_and_time(df)

    # âœ… æ¬„ä½è½‰æ›ï¼ˆå·¦é‚Šæ¬„ä½ âœ Sync æ¬„ä½åç¨±ï¼‰
# âœ… æ¬„ä½è½‰æ›
    df = df.rename(columns=FIELD_MAPPING)

    # âœ… æŒ‰ç…§ä½ æƒ³è¦çš„é †åºä¿ç•™æ¬„ä½ï¼ˆå³é‚Šçš„æ¬„ä½åç¨±ï¼‰
    sync_columns_ordered = [FIELD_MAPPING[k] for k in FIELD_MAPPING if FIELD_MAPPING[k] in df.columns]
    df = df[sync_columns_ordered]

    
    # âœ… å„²å­˜åˆ° Unclustered è³‡æ–™å¤¾ä¸¦åŠ ä¸Š Unclustered å¾Œç¶´
    excel_filename = f"{uid}_Unclustered.xlsx"
    excel_path = os.path.join(basedir, 'excel_result_Unclustered', excel_filename)    
    df.to_excel(excel_path, index=False)

    # ç¢ºèª JSON æª”æ¡ˆæ˜¯å¦å¯«å…¥æˆåŠŸ
    if os.path.exists(json_path):
        print("âœ… JSON æª”æ¡ˆå·²æˆåŠŸå„²å­˜")
    else:
        print("âŒ JSON æª”æ¡ˆå„²å­˜å¤±æ•—ï¼")

    print(f"âœ… åˆ†æå ±è¡¨å·²å„²å­˜ï¼š{excel_path}")
    print("ğŸ“ JSON çµ•å°è·¯å¾‘ï¼š", os.path.abspath(json_path))
    print("ğŸ“ Excel çµ•å°è·¯å¾‘ï¼š", os.path.abspath(excel_path))
    timestamp = uid.replace("result_", "")
    original_excel_path = os.path.abspath(os.path.join(basedir, 'uploads', f"original_{timestamp}.xlsx"))

    # âœ… é™„åŠ åˆ°åŒæ­¥çš„ Excel æª”æ¡ˆï¼ˆä¸å½±éŸ¿åŸæœ‰å„²å­˜æµç¨‹ï¼‰
    try:
        sync_target = r"C:\Users\tachang\Microsoft\MSTC ITG - Timmy\IncidentAnalysisDB.xlsx"

        # âœ… è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨å°±å…ˆå»ºç«‹
        folder_path = os.path.dirname(sync_target)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
            print(f"ğŸ“ å·²è‡ªå‹•å»ºç«‹è³‡æ–™å¤¾ï¼š{folder_path}")

        if os.path.exists(sync_target):
            print("ğŸ“– è®€å–æ­·å² Excel ä¸­è³‡æ–™é€²è¡Œåˆä½µèˆ‡å»é‡")
            old_df = pd.read_excel(sync_target, sheet_name="Sheet1", engine="openpyxl")
            print(f"ğŸ“„ æ­·å²è³‡æ–™ç­†æ•¸ï¼š{len(old_df)}")
            combined_df = pd.concat([old_df, df], ignore_index=True)
            combined_df = deduplicate_by_id_and_time(combined_df)
        else:
            print("ğŸ†• ç„¡æ­·å²æª”æ¡ˆï¼Œç›´æ¥ä½¿ç”¨æœ¬æ¬¡è³‡æ–™")
            combined_df = df

        # âœ… æ•´é«”è¦†è“‹å¯«å…¥ï¼Œä¸å†ä½¿ç”¨ append
        with pd.ExcelWriter(sync_target, engine="openpyxl", mode="w") as writer:
            combined_df.to_excel(writer, sheet_name="Sheet1", index=False)

        # âœ… è³‡æ–™å¯«å…¥å®Œæˆå¾Œï¼Œå†é€²è¡Œæ ¼å¼å¥—ç”¨
        apply_excel_formatting(sync_target)
        print(f"ğŸ“Š æº–å‚™è¦†è“‹å¯«å…¥ {sync_target}ï¼Œå…± {len(combined_df)} ç­†è³‡æ–™")
        print(f"âœ… æˆåŠŸè¦†è“‹å¯«å…¥ Excelï¼ˆå…± {len(combined_df)} ç­†ï¼‰ï¼š{sync_target}")

    except PermissionError:
        print("âš ï¸ ç„¡æ³•å¯«å…¥ï¼Œè«‹ç¢ºèª Excel æª”æ¡ˆæ˜¯å¦å·²é—œé–‰ï¼")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•å¯«å…¥ Excelï¼š{e}")
        traceback.print_exc()

        
        
        
@app.route("/compare-file", methods=["POST"])
def compare_file():
    print("ğŸ“¥ æ”¶åˆ°æª”æ¡ˆæ¯”å°è«‹æ±‚") 
    uploaded_file = request.files.get("file")
    if not uploaded_file:
        print("âŒ æ²’æœ‰æ”¶åˆ°ä¸Šå‚³çš„æª”æ¡ˆ")
        return jsonify({"error": "No file uploaded"}), 400
 
    try:
        # âœ… å»ºç«‹æš«å­˜è³‡æ–™å¤¾
        temp_dir = "tmp_upload"
        os.makedirs(temp_dir, exist_ok=True)
        print(f"ğŸ“ ç¢ºä¿æš«å­˜è³‡æ–™å¤¾å­˜åœ¨ï¼š{temp_dir}")
 
        # âœ… å„²å­˜ä¸Šå‚³æª”æ¡ˆ
        temp_path = os.path.join(temp_dir, uploaded_file.filename)
        uploaded_file.save(temp_path)
        print(f"ğŸ“„ å·²å„²å­˜ä¸Šå‚³æª”æ¡ˆè‡³æš«å­˜ï¼š{temp_path}")
 
        # âœ… è®€å–ä¸Šå‚³çš„æª”æ¡ˆ
        df_new = pd.read_excel(temp_path)
        print("ğŸ“Š æˆåŠŸè®€å–ä¸Šå‚³æª”æ¡ˆç‚º DataFrame")
 
        # âœ… æ¯”å° uploads ä¸­çš„æª”æ¡ˆ
        uploads_dir = "uploads"
        print(f"ğŸ” é–‹å§‹æ¯”å° uploads è³‡æ–™å¤¾å…§æª”æ¡ˆï¼Œå…± {len(os.listdir(uploads_dir))} å€‹")
 
        for fname in os.listdir(uploads_dir):
            fpath = os.path.join(uploads_dir, fname)
            try:
                df_existing = pd.read_excel(fpath)
                print(f"ğŸ“ æ­£åœ¨æ¯”å°ï¼š{fname}")
                if df_new.equals(df_existing):
                    print(f"âœ… ç™¼ç¾é‡è¤‡æª”æ¡ˆï¼š{fname}")
                    os.remove(temp_path)
                    print(f"ğŸ§¹ å·²åˆªé™¤æš«å­˜æª”æ¡ˆï¼š{temp_path}")
                    return jsonify({"duplicate": True})
            except Exception as ex:
                print(f"âš ï¸ ç„¡æ³•è®€å–æª”æ¡ˆ {fname}ï¼š{ex}")
                continue
 
        os.remove(temp_path)
        print(f"ğŸ§¹ æ¯”å°å®Œæˆï¼Œæœªç™¼ç¾é‡è¤‡ã€‚å·²åˆªé™¤æš«å­˜æª”æ¡ˆï¼š{temp_path}")
        return jsonify({"duplicate": False})
 
    except Exception as e:
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.remove(temp_path)
            print(f"â—ç™¼ç”ŸéŒ¯èª¤ï¼Œåˆªé™¤æš«å­˜æª”æ¡ˆï¼š{temp_path}")
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return jsonify({"error": str(e)}), 500
 



@app.route('/kb-status')
def kb_status():
    lock_exists = os.path.exists("kb_building.lock")
    print(f"[DEBUG] lock file exists? {lock_exists}")
    return jsonify({"building": lock_exists})


from datetime import datetime, timedelta


@app.route('/get-results')
def get_results():
    folder = 'json_data'
    results = []
    first_weights = {}
        # âœ… æ–°å¢ï¼šå–å¾—å‰ç«¯å‚³ä¾†çš„åƒæ•¸ï¼ˆé è¨­ start=0, limit=20ï¼‰
    start = int(request.args.get('start', 0))
    limit = int(request.args.get('limit', 20))
    filter_days = request.args.get('days', 'all')

    if not os.path.exists(folder):
        return jsonify({'error': f'è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼š{folder}'}), 404

    # ğŸ”„ è®€å–æ‰€æœ‰æª”æ¡ˆï¼Œæ‰¾å‡ºæœ€æ–°çš„é‚£ä»½åˆ†ææª”
    sorted_files = sorted(
        [f for f in os.listdir(folder) if f.endswith('.json')],
        reverse=True  # æœ€å¾Œé¢æœ€æ–°
    )

    for filename in sorted_files:
        filepath = os.path.join(folder, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = json.load(f)
                if isinstance(content, dict) and 'data' in content:
                    results.extend(content['data'])
                    if not first_weights and 'weights' in content:
                        first_weights = content['weights']
                elif isinstance(content, list):
                    results.extend(content)
        except Exception as e:
            print(f"âŒ éŒ¯èª¤è®€å– {filename}ï¼š{e}")
            
            
            
    # âœ… æ–°å¢ï¼šä¾æ“šç¯©é¸å¤©æ•¸éæ¿¾è³‡æ–™
    if filter_days != 'all':
        try:
            days = int(filter_days)
            now = datetime.now()
            if days == 0:
                # åªé¡¯ç¤ºä»Šå¤©ï¼Œthreshold æ˜¯ä»Šå¤©çš„æ—¥æœŸ
                threshold_date = now.date()
            else:
                threshold_date = (now - timedelta(days=days)).date()
            results = [
                r for r in results
                if 'analysisTime' in r
                and isinstance(r['analysisTime'], str)
                and datetime.fromisoformat(r['analysisTime']).date() >= threshold_date
            ]
        except:
            pass


    # âœ… æ–°å¢ï¼šåˆ†æ‰¹æˆªå–è¦å‚³å›çš„è³‡æ–™ï¼ˆå¾ç¬¬ start ç­†å– limit ç­†ï¼‰
    sliced = results[start:start + limit]

    return jsonify({
        'data': sliced,  # âœ… å‚³å›ç•¶å‰æ‰¹æ¬¡çš„è³‡æ–™
        'total': len(results),     # âœ… å‚³å›ç¸½ç­†æ•¸çµ¦å‰ç«¯åˆ¤æ–·æ˜¯å¦è¼‰å®Œ
        'weights': first_weights  # âœ… ç¢ºä¿å‚³å‡ºé€™å€‹æ¬„ä½
    })



HISTORY_FOLDER = 'json_data'  # ä½ çš„ JSON æª”æ¡ˆå­˜æ”¾è³‡æ–™å¤¾


@app.route('/history-list', methods=['GET'])
def get_history_list():
    # å–å¾—åˆ†é åƒæ•¸ï¼Œé è¨­ page=1, pageSize=10
    page = int(request.args.get('page', 1))
    pageSize = int(request.args.get('pageSize', 10))
    records = []

    if not os.path.exists(HISTORY_FOLDER):
        # å¤šå›å‚³ total è®“å‰ç«¯çŸ¥é“æ˜¯ 0 ç­†
        return jsonify({"records": [], "total": 0})

    # å…ˆæŠŠæ‰€æœ‰æª”æ¡ˆä¾æ™‚é–“æ’åºï¼ˆæ–°åˆ°èˆŠï¼‰
    all_files = sorted(
        [f for f in os.listdir(HISTORY_FOLDER) if f.endswith('.json')],
        reverse=True
    )
    total = len(all_files)  # ç¸½å…±å¹¾ç­†æª”æ¡ˆ

    # ç®—å‡ºè¦æ‹¿ç¬¬å¹¾ç­†åˆ°ç¬¬å¹¾ç­†
    start_idx = (page - 1) * pageSize
    end_idx = start_idx + pageSize
    page_files = all_files[start_idx:end_idx]

    for fname in page_files:
        file_path = os.path.join(HISTORY_FOLDER, fname)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                record = json.load(f)
                if "data" in record and isinstance(record["data"], list):
                    risk_levels = [row.get("riskLevel", "æœªçŸ¥") for row in record["data"]]
                    level_count = Counter(risk_levels)
                    risk_str = "ï½œ".join([
                        f"{label}:{level_count[label]}"
                        for label in ["é«˜é¢¨éšª", "ä¸­é¢¨éšª", "ä½é¢¨éšª", "å¿½ç•¥"]
                        if level_count.get(label, 0) > 0
                    ])
                else:
                    risk_str = "â€”"
                base_summary = record.get("summary", "")
                final_summary = risk_str if not base_summary else risk_str + "ï½œ" + base_summary

                records.append({
                    "uid": record.get("uid", fname.replace('.json','')),
                    "file": record.get("file", fname),
                    "summary": final_summary,
                    "time": record.get("analysisTime", record.get("time", "æœªçŸ¥æ™‚é–“"))
                })
        except Exception as e:
            print(f"âŒ è§£ææ­·å²æª”éŒ¯èª¤ï¼š{fname}", e)

    # å›å‚³æœ¬é è³‡æ–™å’Œç¸½æ•¸
    return jsonify({
        "records": records,    # ç•¶å‰é€™ä¸€é çš„è³‡æ–™ï¼ˆlistï¼‰
        "total": total         # æ‰€æœ‰æª”æ¡ˆç¸½æ•¸
    })
    
    


@app.route('/clear-history', methods=['POST'])
def clear_history():
    if not os.path.exists(HISTORY_FOLDER):
        return jsonify({"success": True})

    deleted = 0
    for fname in os.listdir(HISTORY_FOLDER):
        if fname.endswith('.json'):
            try:
                os.remove(os.path.join(HISTORY_FOLDER, fname))
                deleted += 1
            except Exception as e:
                print(f"âŒ ç„¡æ³•åˆªé™¤ {fname}", e)
    return jsonify({"success": True, "deleted": deleted})










# âœ… JSON é è¦½è·¯ç”±ï¼šæä¾› `/get-json?file=xxxx.json`
@app.route('/get-json', methods=['GET'])
def get_json_file():
    filename = request.args.get('file')  # e.g., result_20250423_152301.json
    if not filename:
        return jsonify({'error': 'ç¼ºå°‘ file åƒæ•¸'}), 400

    json_path = os.path.join('json_data', filename)
    if os.path.exists(json_path):
        return send_file(json_path, as_attachment=False)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„ JSON æª”æ¡ˆ'}), 404


@app.route('/download-excel', methods=['GET'])
def download_excel_file():
    uid = request.args.get('uid')  # e.g., result_20250508_203611
    if not uid:
        return jsonify({'error': 'ç¼ºå°‘ uid åƒæ•¸'}), 400

    # å…ˆæª¢æŸ¥ Clustered
    clustered_path = os.path.join('excel_result_Clustered', f"{uid}_Clustered.xlsx")
    if os.path.exists(clustered_path):
        return send_file(clustered_path, as_attachment=True)

    # å†æª¢æŸ¥ Unclustered
    unclustered_path = os.path.join('excel_result_Unclustered', f"{uid}_Unclustered.xlsx")
    if os.path.exists(unclustered_path):
        return send_file(unclustered_path, as_attachment=True)

    return jsonify({'error': f'æ‰¾ä¸åˆ° {uid} å°æ‡‰çš„ Excel æª”æ¡ˆ'}), 404


@app.route('/download-original', methods=['GET'])
def download_original_excel():
    uid = request.args.get('uid')  # uid = result_20250423_152301
    if not uid:
        return jsonify({'error': 'ç¼ºå°‘ uid åƒæ•¸'}), 400

    # å–å‡ºå°æ‡‰çš„æ™‚é–“æˆ³
    timestamp = uid.replace('result_', '')
    original_filename = f'original_{timestamp}.xlsx'
    original_path = os.path.join('uploads', original_filename)

    if os.path.exists(original_path):
        return send_file(original_path, as_attachment=True)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„åŸå§‹æª”æ¡ˆ'}), 404


# ------------------------------------------------------------------------------

# å®šç¾©æª”æ¡ˆåˆ—è¡¨è·¯ç”±
@app.route('/files', methods=['GET'])
def get_file_list():
    files = os.listdir(UPLOAD_FOLDER)  # åˆ—å‡ºä¸Šå‚³è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ
    return jsonify({'files': files}), 200

# ------------------------------------------------------------------------------

# å®šç¾©åŸ·è¡Œå‹•ä½œçš„è·¯ç”±
@app.route('/perform-action', methods=['POST'])
def perform_action():
    data = request.json  # å–å¾— JSON è³‡æ–™
    action = data.get('action')  # å–å¾—å‹•ä½œåç¨±

    if action == 'start':  # å¦‚æœå‹•ä½œæ˜¯ 'start'
        result = "Server received 'start' action and performed the task."
        print(result)
        return jsonify({'status': 'success', 'message': result}), 200
    else:  # å¦‚æœæ˜¯æœªçŸ¥å‹•ä½œ
        result = f"Server received unknown action: {action}"
        print(result)
        return jsonify({'status': 'error', 'message': 'Unknown action!'}), 400

# ------------------------------------------------------------------------------

def is_flask_running():
    """æª¢æŸ¥ Flask (127.0.0.1:5000) æ˜¯å¦å·²å•Ÿå‹•"""
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(("127.0.0.1", 5000))
        s.shutdown(socket.SHUT_RDWR)
        return True
    except:
        return False
    finally:
        s.close()

if __name__ == "__main__":
    # åˆ¤æ–· Flask æ˜¯å¦å·²ç¶“æœ‰æœå‹™
    if not is_flask_running():
        print("ğŸŒ é–‹å•Ÿç€è¦½å™¨ http://127.0.0.1:5000")
        webbrowser.open("http://127.0.0.1:5000")
    else:
        print("âš ï¸ Flask å·²åœ¨é‹ä½œï¼Œä¸é‡è¤‡é–‹å•Ÿç€è¦½å™¨")
    app.run(debug=True, use_reloader=True)











# async def analyze_excel_async(filepath, weights=None):
#     start_time = time.time()
#     default_weights = {
#         'keyword': 5.0,
#         'multi_user': 3.0,
#         'escalation': 2.0,
#         'config_item': 5.0,
#         'role_component': 3.0,
#         'time_cluster': 2.0
#     }
#     weights = {**default_weights, **(weights or {})}

#     df = pd.read_excel(filepath)
#     component_counts = df['Role/Component'].value_counts()
#     configuration_item_counts = df['Configuration item'].value_counts()
#     configuration_item_max = configuration_item_counts.max()
#     df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')
#     analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#     # éåŒæ­¥è™•ç†æ¯ç­†è³‡æ–™
#     tasks = [
#         analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time)
#         for idx, row in df.iterrows()
#     ]
#     results_raw = await asyncio.gather(*tasks)
#     results = [r for r in results_raw if r]

#     # âœ… åˆ†ç¾¤é‚è¼¯ï¼ˆç…§åŸæœ¬é‚è¼¯å³å¯ï¼‰
#     all_scores = [r['impactScore'] for r in results]
#     score_range = max(all_scores) - min(all_scores)
#     score_std = np.std(all_scores)

#     print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

#     if (
#         len(all_scores) >= KMEANS_MIN_COUNT and
#         score_range >= KMEANS_MIN_RANGE and
#         score_std >= KMEANS_MIN_STDDEV
#     ):
#         kmeans = KMeans(n_clusters=4, random_state=42)
#         labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
#         centroids = kmeans.cluster_centers_.flatten()
#         set_kmeans_thresholds_from_centroids(centroids)
#         print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
#         label_map = {}
#         for i, idx in enumerate(np.argsort(centroids)[::-1]):
#             label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
#         for i, r in enumerate(results):
#             r['riskLevel'] = label_map[labels[i]]
#         print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
#     else:
#         print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
#         for r in results:
#             r['riskLevel'] = get_risk_level(r['impactScore'])

#     total_time = time.time() - start_time
#     avg_time = total_time / len(results) if results else 0

#     print(f"\nğŸ¯ æ‰€æœ‰åˆ†æç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’")
#     print(f"ğŸ“Š å–®ç­†å¹³å‡è€—æ™‚ï¼š{avg_time:.2f} ç§’")

#     print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
#     return {
#         'data': results,
#         'analysisTime': analysis_time
#     }







# async def analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time):
#     try:
#         description_text = row.get('Description', 'not filled')
#         short_description_text = row.get('Short description', 'not filled')
#         close_note_text = row.get('Close notes', 'not filled')

#         keyword_score = is_high_risk(short_description_text)
#         user_impact_score = is_multi_user(description_text)
#         escalation_score = is_escalated(close_note_text)

#         config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)
#         configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0

#         role_comp = row.get('Role/Component', 'not filled')
#         count = component_counts.get(role_comp, 0)
#         role_component_freq = 3 if count >= 5 else 2 if count >= 3 else 1 if count == 2 else 0

#         this_time = row.get('Opened', 'not filled')
#         if pd.isnull(this_time):
#             time_cluster_score = 1
#         else:
#             others = df[df['Role/Component'] == role_comp]
#             close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
#                                   (others['Opened'] <= this_time + pd.Timedelta(hours=24))]
#             count_cluster = len(close_events)
#             time_cluster_score = 3 if count_cluster >= 3 else 2 if count_cluster == 2 else 1

#         severity_score = round(
#             keyword_score * weights['keyword'] +
#             user_impact_score * weights['multi_user'] +
#             escalation_score * weights['escalation'], 2
#         )
#         frequency_score = round(
#             configuration_item_freq * weights['config_item'] +
#             role_component_freq * weights['role_component'] +
#             time_cluster_score * weights['time_cluster'], 2
#         )
#         impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
#         risk_level = get_risk_level(impact_score)

#         desc = str(description_text).strip()
#         short_desc = str(short_description_text).strip()
#         close_notes = str(close_note_text).strip()
#         resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

#         ai_suggestion, ai_summary = await asyncio.gather(
#             extract_resolution_suggestion(resolution_text),
#             extract_problem_with_custom_prompt(f"{short_desc}\n{desc}".strip())
#         )

#         recommended = recommend_solution(short_description_text)
#         keywords = extract_keywords(short_description_text)

#         return {
#             'id': safe_value(row.get('Incident') or row.get('Number')),
#             'configurationItem': safe_value(row.get('Configuration item')),
#             'roleComponent': safe_value(row.get('Role/Component')),
#             'subcategory': safe_value(row.get('Subcategory')),
#             'aiSummary': safe_value(ai_summary),
#             'originalShortDescription': safe_value(short_desc),
#             'originalDescription': safe_value(desc),
#             'severityScore': safe_value(severity_score),
#             'frequencyScore': safe_value(frequency_score),
#             'impactScore': safe_value(impact_score),
#             'severityScoreNorm': round(severity_score / 10, 2),
#             'frequencyScoreNorm': round(frequency_score / 20, 2),
#             'impactScoreNorm': round(impact_score / 30, 2),
#             'riskLevel': risk_level,
#             'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
#             'location': safe_value(row.get('Location')),
#             'analysisTime': analysis_time,
#             'weights': {k: round(v / 10, 2) for k, v in weights.items()},
#         }

#     except Exception as e:
#         print(f"âŒ åˆ†æç¬¬ {idx+1} ç­†å¤±æ•—ï¼š", e)
#         return None











# def analyze_excel(filepath, weights=None):
#         # é è¨­æ¬Šé‡è¨­å®šï¼ˆå¯è¢«è¦†è“‹ï¼‰
#     default_weights = {
#         'keyword': 5.0,
#         'multi_user': 3.0,
#         'escalation': 2.0,
#         'config_item': 5.0,
#         'role_component': 3.0,
#         'time_cluster': 2.0
#     }
#     weights = {**default_weights, **(weights or {})}  # åˆä½µé è¨­æ¬Šé‡èˆ‡ä½¿ç”¨è€…æä¾›çš„æ¬Šé‡è¨­å®š
#     print("ğŸ›ï¸ ä½¿ç”¨ä¸­çš„æ¬Šé‡è¨­å®šï¼š", weights)
#     print("ğŸ” é–‹å§‹åˆ†æ Excel æª”æ¡ˆ...")
#     print(f"\nğŸ“‚ è®€å– Excelï¼š{filepath}")
#     df = pd.read_excel(filepath)  # è®€å– Excel æª”æ¡ˆ
#     print(f"ğŸ“Š å…±è®€å– {len(df)} ç­†è³‡æ–™\n")
#     component_counts = df['Role/Component'].value_counts()  # è¨ˆç®—æ¯å€‹è§’è‰²/å…ƒä»¶çš„å‡ºç¾æ¬¡æ•¸
#     df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')  # å°‡ 'Opened' æ¬„ä½è½‰ç‚ºæ—¥æœŸæ ¼å¼
#     results = []  # å„²å­˜åˆ†æçµæœ
#     configuration_item_counts = df['Configuration item'].value_counts()  # è¨ˆç®—æ¯å€‹é…ç½®é …çš„å‡ºç¾æ¬¡æ•¸
#     configuration_item_max = configuration_item_counts.max()  # æ‰¾å‡ºé…ç½®é …çš„æœ€å¤§å‡ºç¾æ¬¡æ•¸
#     analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
#     print(f"ğŸ“… åˆ†ææ™‚é–“ï¼š{analysis_time}")

#     for idx, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ“Š åˆ†æé€²åº¦"):
#         print(f"\nğŸ” ç¬¬ {idx + 1} ç­†åˆ†æä¸­...")
#         description_text = row.get('Description', 'not filled')  # å–å¾—æè¿°æ–‡å­—
#         short_description_text = row.get('Short description', 'not filled') # å–å¾—ç°¡çŸ­æè¿°æ–‡å­—
#         close_note_text = row.get('Close notes', 'not filled')  # å–å¾—é—œé–‰è¨»è§£æ–‡å­—
#         print(f"ğŸ“„ æè¿°ï¼š{description_text}")
#         print(f"ğŸ”‘ ç°¡çŸ­æè¿°ï¼š{short_description_text}")
#         print(f"ğŸ”’ é—œé–‰è¨»è§£ï¼š{close_note_text}")
#         # é€™è£¡å¯ä»¥åŠ å…¥å°æè¿°æ–‡å­—çš„é è™•ç†ï¼Œä¾‹å¦‚å»é™¤å¤šé¤˜ç©ºæ ¼ã€è½‰ç‚ºå°å¯«ç­‰
#         # description_text = normalize_text(description_text)  # æ¨™æº–åŒ–æ–‡å­—    


#         #é€™è£¡è¦æ”¹æˆä½¿ç”¨èªæ„åˆ†ææ¨¡å‹

#         keyword_score = is_high_risk(short_description_text)  # è¨ˆç®—é—œéµå­—åˆ†æ•¸
#         print(f"âš ï¸ é«˜é¢¨éšªèªæ„åˆ†æ•¸ï¼ˆkeyword_scoreï¼‰ï¼š{keyword_score}")
#         user_impact_score = is_multi_user(description_text)  # è¨ˆç®—ä½¿ç”¨è€…å½±éŸ¿åˆ†æ•¸
#         print(f"ğŸ‘¥ å¤šäººå½±éŸ¿åˆ†æ•¸ï¼ˆuser_impact_scoreï¼‰ï¼š{user_impact_score}")
#         escalation_score = is_escalated(close_note_text)  # è¨ˆç®—å‡ç´šè™•ç†åˆ†æ•¸
#         print(f"ğŸ“ˆ å‡ç´šè™•ç†åˆ†æ•¸ï¼ˆescalation_scoreï¼‰ï¼š{escalation_score}")



#         config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)  # å–å¾—é…ç½®é …çš„å‡ºç¾æ¬¡æ•¸
#         configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0  # è¨ˆç®—é…ç½®é …é »ç‡

#         role_comp = row.get('Role/Component', 'not filled')  # å–å¾—è§’è‰²/å…ƒä»¶
#         count = component_counts.get(role_comp, 0)  # å–å¾—è§’è‰²/å…ƒä»¶çš„å‡ºç¾æ¬¡æ•¸
#         if count >= 5:
#             role_component_freq = 3
#         elif count >= 3:
#             role_component_freq = 2
#         elif count == 2:
#             role_component_freq = 1
#         else:
#             role_component_freq = 0

#         this_time = row.get('Opened', 'not filled')  # å–å¾—é–‹å•Ÿæ™‚é–“
#         if pd.isnull(this_time):  # å¦‚æœé–‹å•Ÿæ™‚é–“ç‚ºç©º
#             time_cluster_score = 1
#         else:
#             others = df[df['Role/Component'] == role_comp]  # ç¯©é¸ç›¸åŒè§’è‰²/å…ƒä»¶çš„è³‡æ–™
#             close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
#                                   (others['Opened'] <= this_time + pd.Timedelta(hours=24))]  # æ‰¾å‡º 24 å°æ™‚å…§çš„äº‹ä»¶
#             count_cluster = len(close_events)  # è¨ˆç®—äº‹ä»¶æ•¸é‡
#             if count_cluster >= 3:
#                 time_cluster_score = 3
#             elif count_cluster == 2:
#                 time_cluster_score = 2
#             else:
#                 time_cluster_score = 1

#         severity_score = round(
#             keyword_score * weights['keyword'] +
#             user_impact_score * weights['multi_user'] +
#             escalation_score * weights['escalation'], 2
#         )

#         frequency_score = round(
#             configuration_item_freq * weights['config_item'] +
#             role_component_freq * weights['role_component'] +
#             time_cluster_score * weights['time_cluster'], 2
#         )


        
#         print(f"ğŸ“Š åš´é‡æ€§åˆ†æ•¸ï¼š{severity_score}ï¼Œé »ç‡åˆ†æ•¸ï¼š{frequency_score}")
#         print("ğŸ§  é »ç‡åˆ†æ•¸ç´°é …ï¼š")
#         print(f"ğŸ”¸ é…ç½®é …ï¼ˆConfiguration Itemï¼‰å‡ºç¾æ¯”ä¾‹ï¼š{configuration_item_freq:.2f}ï¼Œä¹˜ä»¥æ¬Šé‡å¾Œå¾— {configuration_item_freq * weights['config_item']:.2f} åˆ†")
#         print(f"ğŸ”¸ å…ƒä»¶æˆ–è§’è‰²ï¼ˆRole/Componentï¼‰åœ¨æ•´é«”ä¸­å‡ºç¾ {count} æ¬¡ â†’ çµ¦ {role_component_freq * weights['role_component']:.2f} åˆ†")
#         print(f"ğŸ”¸ åœ¨ 24 å°æ™‚å…§æœ‰ {count_cluster} ç­†åŒå…ƒä»¶äº‹ä»¶ â†’ ç¾¤èšåŠ åˆ† {time_cluster_score * weights['time_cluster']:.2f} åˆ†")
#         print(f"ğŸ“Š é »ç‡ç¸½åˆ† = {frequency_score}\n")




#         # è¨ˆç®—å½±éŸ¿åˆ†æ•¸
#         impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
#         risk_level = get_risk_level(impact_score)
#         print(f"ğŸ“‰ åš´é‡æ€§ï¼š{severity_score}, é »ç‡ï¼š{frequency_score}, ç¸½åˆ†(After KMean process)ï¼š{impact_score} â†’ åˆ†ç´šï¼š{risk_level}")
#         desc = str(row.get('Description', "")).strip()
#         short_desc = str(row.get('Short Description', "")).strip()
#         close_notes = str(row.get('Close notes', "")).strip()
#         resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

#         print(f"ğŸ“¦ Resolution åŸå§‹æ–‡å­—ï¼š{resolution_text}")  # âœ… ç¢ºèªåŸå§‹æ¬„ä½å…§å®¹

#         ai_suggestion = extract_resolution_suggestion(resolution_text)
#         print(f"ğŸ¤– GPT å»ºè­°å¥å›å‚³ï¼š{ai_suggestion}")  # âœ… ç¢ºèª GPT æ˜¯å¦æˆåŠŸå›æ‡‰

#         # âœ… å®‰å…¨åœ°å»ºç«‹ AI æ‘˜è¦è¼¸å…¥ï¼ˆè‹¥å…¨ç©ºå‰‡é¡¯ç¤ºç„¡è³‡æ–™ï¼‰
#         summary_input_text = f"{short_desc}\n{desc}".strip()
#         if not summary_input_text:
#             summary_input_text = "ï¼ˆç„¡åŸå§‹æ‘˜è¦è¼¸å…¥ï¼‰"

#         # âœ… å‘¼å« GPT æ‘˜è¦å‡½å¼
#         ai_summary = extract_problem_with_custom_prompt(summary_input_text)

#         print(f"ğŸ“¦ Resolution åŸå§‹æ–‡å­—ï¼š{resolution_text}")
#         print(f"ğŸ“ AI æ‘˜è¦è¼¸å…¥ï¼š{summary_input_text}")
#         print(f"ğŸ¤– GPT æ‘˜è¦å›å‚³ï¼š{ai_summary}")

#         # å„²å­˜åˆ†æçµæœ
#         results.append({
#             'id': safe_value(row.get('Incident') or row.get('Number')),
#             'configurationItem': safe_value(row.get('Configuration item')),
#             'roleComponent': safe_value(row.get('Role/Component')),
#             'subcategory': safe_value(row.get('Subcategory')),
#             'aiSummary': safe_value(ai_summary),
#             'originalShortDescription': safe_value(short_desc),
#             'originalDescription': safe_value(desc),
#             'severityScore': safe_value(severity_score),
#             'frequencyScore': safe_value(frequency_score),
#             'impactScore': safe_value(impact_score),
#             'severityScoreNorm': round(severity_score / 10, 2),
#             'frequencyScoreNorm': round(frequency_score / 20, 2),
#             'impactScoreNorm': round(impact_score / 30, 2),
#             'riskLevel': safe_value(get_risk_level(impact_score)),
#             'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
#             'location': safe_value(row.get('Location')),
#             'analysisTime': analysis_time,
#             'weights': {k: round(v / 10, 2) for k, v in weights.items()},
#         })

#         # solution_text = row.get('Close notes') or 'ç„¡æä¾›è§£æ³•'
#         recommended = recommend_solution(short_description_text)
#         keywords = extract_keywords(short_description_text)

#         print(f"âœ… å·²å„²å­˜ solutionï¼š{results[-1]['solution']}")
#         print(f"ğŸ’¡ å»ºè­°è§£æ³•ï¼š{recommended}")
#         print(f"ğŸ”‘ æŠ½å–é—œéµå­—ï¼š{keywords}")
#         print("â€”" * 250)  # åˆ†éš”ç·š




#     # â¬‡â¬‡â¬‡ KMeans åˆ†ç¾¤é‚è¼¯ï¼ˆæ”¯æ´ä¸‰æ¢ä»¶ï¼‰ â¬‡â¬‡â¬‡
#     all_scores = [r['impactScore'] for r in results]
#     score_range = max(all_scores) - min(all_scores)
#     score_std = np.std(all_scores)

#     print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

#     if (
#         len(all_scores) >= KMEANS_MIN_COUNT and
#         score_range >= KMEANS_MIN_RANGE and
#         score_std >= KMEANS_MIN_STDDEV
#     ):
#         kmeans = KMeans(n_clusters=4, random_state=42)
#         labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
#         centroids = kmeans.cluster_centers_.flatten()
#         set_kmeans_thresholds_from_centroids(centroids)
#         print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
#         label_map = {}
#         for i, idx in enumerate(np.argsort(centroids)[::-1]):
#             label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
#         for i, r in enumerate(results):
#             r['riskLevel'] = label_map[labels[i]]
#         print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
#     else:
#         print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
#         for r in results:
#             r['riskLevel'] = get_risk_level(r['impactScore'])
#     # â¬†â¬†â¬† åˆ†ç¾¤é‚è¼¯çµæŸ â¬†â¬†â¬†
#     print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
#     return {
#         'data': results,
#         'analysisTime': analysis_time
#     }








