# åŒ¯å…¥ Flask æ¡†æ¶åŠç›¸é—œæ¨¡çµ„
from flask import Flask, request, jsonify, render_template, session, send_file
from gpt_utils import extract_resolution_suggestion
from gpt_utils import extract_problem_with_custom_prompt
from gptChatbackup import run_offline_gpt
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from collections import Counter
import hashlib
import subprocess
import sys
from pathlib import Path
import umap
import hdbscan
# åŒ¯å…¥æ•¸å­¸é‹ç®—æ¨¡çµ„
import math
import json
# åŒ¯å…¥ pandas ç”¨æ–¼è™•ç† Excel è³‡æ–™
import pandas as pd
# åŒ¯å…¥ os æ¨¡çµ„è™•ç†æª”æ¡ˆèˆ‡è·¯å¾‘
import os
import shutil
# åŒ¯å…¥æ­£å‰‡è¡¨é”å¼æ¨¡çµ„
import re
import glob
# åŒ¯å…¥ webbrowser ç”¨æ–¼é–‹å•Ÿç¶²é 
import webbrowser
import socket
# åŒ¯å…¥ traceback ç”¨æ–¼éŒ¯èª¤è¿½è¹¤
import traceback
# åŒ¯å…¥ Werkzeug çš„å·¥å…·å‡½æ•¸ç¢ºä¿æª”æ¡ˆåç¨±å®‰å…¨
from werkzeug.utils import secure_filename
# âœ… åŒ¯å…¥èªæ„åˆ†ææ¨¡çµ„
from SmartScoring1 import is_high_risk, is_escalated, is_multi_user, extract_keywords, recommend_solution, is_actionable_resolution, load_embeddings, load_examples_from_json
# âœ… é å…ˆ encode ä¸€ç­†è³‡æ–™ä»¥åŠ é€Ÿé¦–æ¬¡è«‹æ±‚
from SmartScoring1 import bert_model  # ç¢ºä¿ä½ æœ‰å¾ SmartScoring è¼‰å…¥æ¨¡å‹
from SmartScoring1 import extract_cluster_name  # åŒ¯å…¥è‡ªå®šçš„ cluster å‘½åå‡½å¼
from tqdm import tqdm
from sentence_transformers import util
# âœ… åŒ¯å…¥é—œéµå­—æŠ½å–æ¨¡çµ„
from datetime import datetime
from sklearn.cluster import KMeans
import numpy as np
from datetime import datetime
import time
# --- åˆ†ç¾¤å•Ÿç”¨æ¢ä»¶ï¼ˆå¯ä¾è³‡æ–™èª¿æ•´ï¼‰---
import asyncio
import math
import requests
import threading
import json
import tempfile
from jsonschema import validate, ValidationError
from datetime import datetime



KMEANS_MIN_COUNT = 4         # æœ€å°‘è³‡æ–™ç­†æ•¸
KMEANS_MIN_RANGE = 5.0       # åˆ†æ•¸æœ€å¤§æœ€å°å€¼å·®
KMEANS_MIN_STDDEV = 3.0      # æ¨™æº–å·®ä¸‹é™


start = time.time()
print("ğŸ”¥ é ç†±èªæ„æ¨¡å‹ä¸­...")
bert_model.encode("warmup")  # é ç†±ä¸€æ¬¡ï¼Œé¿å…ç¬¬ä¸€æ¬¡ä½¿ç”¨å¤ªæ…¢
print(f"âœ… æ¨¡å‹é ç†±å®Œæˆï¼Œç”¨æ™‚ï¼š{time.time() - start:.2f} ç§’")

# å»ºç«‹ Flask æ‡‰ç”¨
app = Flask(__name__)
# è¨­å®šæ‡‰ç”¨çš„å¯†é‘°ï¼Œç”¨æ–¼ session åŠ å¯†
app.secret_key = 'gwegweqgt22e'
# è¨­å®š session å„²å­˜æ–¹å¼ç‚ºæª”æ¡ˆç³»çµ±
app.config['SESSION_TYPE'] = 'filesystem'

# ------------------------------------------------------------------------------
# è¨­å®šä¸Šå‚³è³‡æ–™å¤¾èˆ‡å¤§å°é™åˆ¶ï¼ˆ10MBï¼‰
UPLOAD_FOLDER = 'uploads'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # é™åˆ¶æª”æ¡ˆå¤§å°ç‚º 10MB
ALLOWED_EXTENSIONS = {'xlsx'}  # åƒ…å…è¨±ä¸Šå‚³ xlsx æª”æ¡ˆ


basedir = os.path.abspath(os.path.dirname(__file__))  # å–å¾—ç•¶å‰ app.py çš„çµ•å°ç›®éŒ„
# ç¢ºä¿ä¸Šå‚³è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(os.path.join(basedir, 'json_data'), exist_ok=True)
os.makedirs(os.path.join(basedir, 'excel_result_Unclustered'), exist_ok=True)  # æ–°å¢æœªåˆ†ç¾¤è³‡æ–™å¤¾
os.makedirs(os.path.join(basedir, 'excel_result_Clustered'), exist_ok=True) # æ–°å¢åˆ†ç¾¤è³‡æ–™å¤¾

# ------------------------------------------------------------------------------

# åˆ¤æ–·æ˜¯å¦å…è¨±çš„æª”æ¡ˆæ ¼å¼
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# ------------------------------------------------------------------------------

# å®šç¾©å‡½æ•¸ï¼šè™•ç†ç‰¹æ®Šå€¼ï¼ˆå¦‚ NaNã€None ç­‰ï¼‰
def safe_value(val):
    if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
        return 0
    elif val is None:
        return None
    elif isinstance(val, str):
        return val
    else:
        return val

# ------------------------------------------------------------------------------




@app.route('/check-unclustered', methods=['GET'])
def check_unclustered_files():
    folder = 'excel_result_Unclustered'
    if not os.path.exists(folder):
        return jsonify({'exists': False}), 200
    files = [f for f in os.listdir(folder) if f.endswith('.xlsx')]
    return jsonify({'exists': len(files) > 0}), 200


@app.route('/clustered-files', methods=['GET'])
def list_clustered_files():
    clustered_folder = 'excel_result_Clustered'
    if not os.path.exists(clustered_folder):
        return jsonify({'files': []})

    pattern = re.compile(r"^Cluster-\[CI\].+_\[RC\].+_\[SC\].+\.xlsx$")
    files_info = []

    for f in os.listdir(clustered_folder):
        if not (f.endswith('.xlsx') and pattern.match(f)):
            continue

        filepath = os.path.join(clustered_folder, f)
        try:
            df = pd.read_excel(filepath)
            row_count = len(df)
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å– {f}ï¼š{e}")
            row_count = 0

        files_info.append({
            'name': f,
            'rows': row_count
        })

    # âœ… å¯é¸ï¼šä¾ç…§ row æ•¸é™å†ªæ’åºï¼ˆæœ€å¤šçš„æ’å‰é¢ï¼‰
    files_info.sort(key=lambda x: x['rows'], reverse=True)

    return jsonify({'files': files_info})


@app.route('/download-clustered', methods=['GET'])
def download_clustered_file():
    filename = request.args.get('file')
    path = os.path.join('excel_result_Clustered', filename)
    if os.path.exists(path):
        return send_file(path, as_attachment=True)
    return jsonify({'error': 'æ‰¾ä¸åˆ°æª”æ¡ˆ'}), 404




# æ ¹æ“šåˆ†æ•¸åˆ¤æ–·é¢¨éšªç­‰ç´šï¼ˆæ”¯æ´ KMeans åˆ†ç¾¤ï¼‰
kmeans_thresholds = None  # å…¨åŸŸè®Šæ•¸ï¼Œå­˜å„² KMeans åˆ†ç¾¤é–€æª»


def get_risk_level(score):
    global kmeans_thresholds
    level = ''

    if kmeans_thresholds and len(kmeans_thresholds) == 4:
        thresholds = sorted(kmeans_thresholds)
        if score >= thresholds[3]:
            level = 'é«˜é¢¨éšª'
        elif score >= thresholds[2]:
            level = 'ä¸­é¢¨éšª'
        elif score >= thresholds[1]:
            level = 'ä½é¢¨éšª'
        else:
            level = 'å¿½ç•¥'
        print(f"ğŸ“Š KMeansï¼šimpactScore: {score} â†’ åˆ†ç´šï¼š{level}ï¼ˆä½¿ç”¨å‹•æ…‹é–€æª»ï¼‰")
    else:
        if score >= 18:
            level = 'é«˜é¢¨éšª'
        elif score >= 12:
            level = 'ä¸­é¢¨éšª'
        elif score >= 6:
            level = 'ä½é¢¨éšª'
        else:
            level = 'å¿½ç•¥'
        print(f"ğŸ“Š å›ºå®šé–€æª»ï¼šimpactScore: {score} â†’ åˆ†ç´šï¼š{level}")

    return level

# åœ¨åˆ†æå®Œæˆå¾Œè‡ªå‹•è¨­å®š kmeans_thresholds
# ï¼ˆè«‹æ”¾åœ¨ KMeans åˆ†ç¾¤å®Œæˆå¾Œï¼‰
def set_kmeans_thresholds_from_centroids(centroids):
    global kmeans_thresholds
    kmeans_thresholds = sorted(centroids)
    print(f"âœ… å·²è¨­å®š KMeans åˆ†ç¾¤é–€æª»ï¼ˆsortedï¼‰ï¼š{kmeans_thresholds}")

# ------------------------------------------------------------------------------


# âœ… æ–°å¢è·¯ç”±ï¼šè™•ç†æ‰€æœ‰ Unclustered Excel æª”æ¡ˆçš„åˆ†ç¾¤èˆ‡æ¬ç§»
@app.route('/cluster-excel', methods=['POST'])
def cluster_excel():
    unclustered_dir = 'excel_result_Unclustered'
    clustered_dir = 'excel_result_Clustered'
    os.makedirs(clustered_dir, exist_ok=True)  # âœ… ç¢ºä¿ Clustered è³‡æ–™å¤¾å­˜åœ¨

    files = [f for f in os.listdir(unclustered_dir) if f.endswith('_Unclustered.xlsx')]
    print(f"ğŸ” åµæ¸¬åˆ° {len(files)} ç­†å¾…åˆ†ç¾¤æª”æ¡ˆ")

    for filename in files:
        uid = filename.replace('_Unclustered.xlsx', '')
        excel_path = os.path.join(unclustered_dir, filename)
        print(f"ğŸ“‚ è™•ç†æª”æ¡ˆï¼š{excel_path}")

        # è¼‰å…¥ Excel ä¸¦é€²è¡Œåˆ†ç¾¤åŒ¯å‡º
        df = pd.read_excel(excel_path)
        results = df.to_dict(orient='records')
        cluster_excel_export(results)  # âœ… å‘¼å«å·²å®šç¾©çš„å‡½å¼é€²è¡Œåˆ†ç¾¤åŒ¯å‡º

        # æ¬ç§»æª”æ¡ˆåˆ° Clustered ä¸¦æ”¹å
        clustered_path = os.path.join(clustered_dir, uid + '_Clustered.xlsx')
        shutil.move(excel_path, clustered_path)
        print(f"ğŸ“ å·²ç§»å‹•ä¸¦æ”¹åï¼š{clustered_path}")

    return jsonify({'message': f'å·²æˆåŠŸè™•ç† {len(files)} ç­† Excel æª”æ¡ˆä¸¦å®Œæˆåˆ†ç¾¤'}), 200

# ------------------------------------------------------------------------------

def cluster_excel_export(results, export_dir="excel_result_Clustered"):
    def clean(text):
        return re.sub(r'[^\w\-_.]', '_', str(text).strip())[:30] or "Unknown"

    cluster_data = defaultdict(list)
    for r in results:
        config_item = r.get("configurationItem", "Unknown")
        role_component = r.get("roleComponent", "Unknown")
        subcategory = r.get("subcategory", "Unknown")
        cluster_key = f"{config_item}_{role_component}_{subcategory}"
        r['cluster'] = cluster_key
        cluster_data[cluster_key].append(r)

    os.makedirs(export_dir, exist_ok=True)

    for key, group in cluster_data.items():
        cluster_df = pd.DataFrame(group)

        try:
            config_item, role_component, subcategory = key.split('_', 2)
        except ValueError:
            config_item, role_component, subcategory = key, "Unknown", "Unknown"

        filename = f"{export_dir}/Cluster-[CI]{clean(config_item)}_[RC]{clean(role_component)}_[SC]{clean(subcategory)}.xlsx"

        if os.path.exists(filename):
            old_df = pd.read_excel(filename)
            cluster_df = pd.concat([old_df, cluster_df], ignore_index=True)

        cluster_df = cluster_df.sort_values(by="analysisTime", ascending=False)
        cluster_df.to_excel(filename, index=False)
        print(f"ğŸ“ å·²è¼¸å‡ºï¼š{filename}ï¼ˆå…± {len(cluster_df)} ç­†ï¼‰")

        high_count = sum(1 for e in group if e.get('riskLevel') == 'é«˜é¢¨éšª')
        total = len(group)
        if total > 0 and (high_count / total) >= 0.5:
            print(f"ğŸš¨ é è­¦ï¼šCluster {key} æœ‰ {high_count}/{total} ç­†é«˜é¢¨éšªäº‹ä»¶")
    print("âœ… åˆ†ç¾¤ Excel æª”æ¡ˆå·²å„²å­˜ï¼")






# ç”¨æ–¼åŒæ­¥ Flask è·¯ç”±å‘¼å« async åˆ†æé‚è¼¯
def analyze_excel(filepath, weights=None, resolution_priority=None, summary_priority=None):
    return asyncio.run(analyze_excel_async(filepath, weights, resolution_priority, summary_priority))



# ç”¨æ–¼åŒæ­¥ Flask è·¯ç”±å‘¼å« async åˆ†æé‚è¼¯
async def analyze_excel_async(filepath, weights=None, resolution_priority=None, summary_priority=None):
    start_time = time.time()
    default_weights = {
        'keyword': 5.0,
        'multi_user': 3.0,
        'escalation': 2.0,
        'config_item': 5.0,
        'role_component': 3.0,
        'time_cluster': 2.0
    }
    weights = {**default_weights, **(weights or {})}
    print(f"ğŸŸ© æœ¬æ¬¡åˆ†æé–‹å§‹ï¼Œå°‡å³æ™‚è®€å–ä¸‰é¡èªå¥ json æª”æ¡ˆ...")
    # â­ è®€å–èªå¥å’Œ embedding
    high_risk_examples, high_risk_embeddings = load_embeddings("high_risk")
    escalation_examples, escalation_embeddings = load_embeddings("escalate")
    multi_user_examples, multi_user_embeddings = load_embeddings("multi_user")

    df = pd.read_excel(filepath)


    # âœ… æ¬„ä½é †ä½ fallback é è¨­
    resolution_priority = resolution_priority or ['Description', 'Short description', 'Close notes']
    summary_priority = summary_priority or ['Short description', 'Description']

    def combine_fields_with_priority(row, field_order, limit):
        parts = []
        for f in field_order:
            if f in row and pd.notna(row[f]):
                parts.append(str(row[f]).strip())

        combined = "\n".join(parts)
        while len(combined) > limit and len(parts) > 1:
            removed = parts.pop()
            print(f"ğŸ” ç§»é™¤æ¬„ä½ï¼š{removed[:20]}...")
            combined = "\n".join(parts)

        # é¡å¤–ï¼šå°å‡ºå¯¦éš›ä½¿ç”¨çš„æ¬„ä½åç¨±
        used_fields = field_order[:len(parts)]
        print(f"âœ… å¯¦éš›ä½¿ç”¨æ¬„ä½ï¼š{used_fields}ï¼Œåˆä½µé•·åº¦ï¼š{len(combined)}")
        return combined.strip()

    # âœ… ç”¢ç”Ÿ resolution_input / summary_input çµ¦ GPT ç”¨
    df['resolution_input'] = df.apply(lambda row: combine_fields_with_priority(row, resolution_priority, 10000), axis=1)
    df['summary_input'] = df.apply(lambda row: combine_fields_with_priority(row, summary_priority, 8000), axis=1)


    component_counts = df['Role/Component'].value_counts()
    configuration_item_counts = df['Configuration item'].value_counts()
    configuration_item_max = configuration_item_counts.max()
    df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')
    analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # éåŒæ­¥è™•ç†
    tasks = [
        analyze_row_async(
            row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time,
            high_risk_examples, high_risk_embeddings,
            escalation_examples, escalation_embeddings,
            multi_user_examples, multi_user_embeddings,
            row['resolution_input'], row['summary_input']  # âœ… æ–°å¢é€™å…©æ¬„
        )
        for idx, row in df.iterrows()
    ]
    results_raw = await asyncio.gather(*tasks, return_exceptions=True)
    results = [r for r in results_raw if r and not isinstance(r, Exception)]

    # âœ… é˜²å‘†ï¼šæ²’æœ‰ä»»ä½•æˆåŠŸçš„çµæœå°±ç›´æ¥å›å‚³é¿å…å´©æ½°
    if not results:
        print("âš ï¸ æ‰€æœ‰è³‡æ–™éƒ½ç„¡æ³•åˆ†æï¼Œè«‹æª¢æŸ¥æ¬„ä½æ˜¯å¦ç¼ºå¤±")
        return {
            'data': [],
            'analysisTime': analysis_time
        }

    all_scores = [r['impactScore'] for r in results]
    score_range = max(all_scores) - min(all_scores)
    score_std = np.std(all_scores)

    print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

    if (
        len(all_scores) >= KMEANS_MIN_COUNT and
        score_range >= KMEANS_MIN_RANGE and
        score_std >= KMEANS_MIN_STDDEV
    ):
        kmeans = KMeans(n_clusters=4, random_state=42)
        labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
        centroids = kmeans.cluster_centers_.flatten()
        set_kmeans_thresholds_from_centroids(centroids)
        print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
        label_map = {}
        for i, idx in enumerate(np.argsort(centroids)[::-1]):
            label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
        for i, r in enumerate(results):
            r['riskLevel'] = label_map[labels[i]]
        print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
    else:
        print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
        for r in results:
            r['riskLevel'] = get_risk_level(r['impactScore'])

    total_time = time.time() - start_time
    avg_time = total_time / len(results)

    print(f"\nğŸ¯ æ‰€æœ‰åˆ†æç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’")
    print(f"ğŸ“Š å–®ç­†å¹³å‡è€—æ™‚ï¼š{avg_time:.2f} ç§’")
    print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
    return {
        'data': results,
        'analysisTime': analysis_time
    }






async def analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time,     
    high_risk_examples, high_risk_embeddings,
    escalation_examples, escalation_embeddings,
    multi_user_examples, multi_user_embeddings,
    resolution_text, summary_input):
    print(f"[åˆ†æ Row#{idx+1}] æœ¬æ¬¡ç”¨çš„é«˜é¢¨éšªèªå¥æ•¸ï¼š{len(high_risk_examples)}ï¼Œå€’æ•¸å…©å¥ï¼š{high_risk_examples[-2:] if high_risk_examples else 'ç©º'}")
    print(f"[åˆ†æ Row#{idx+1}] æœ¬æ¬¡ç”¨çš„å‡ç´šèªå¥æ•¸ï¼š{len(escalation_examples)}ï¼Œå€’æ•¸å…©å¥ï¼š{escalation_examples[-2:] if escalation_examples else 'ç©º'}")
    print(f"[åˆ†æ Row#{idx+1}] æœ¬æ¬¡ç”¨çš„å½±éŸ¿å¤šä½¿ç”¨è€…èªå¥æ•¸ï¼š{len(multi_user_examples)}ï¼Œå€’æ•¸å…©å¥ï¼š{multi_user_examples[-2:] if multi_user_examples else 'ç©º'}")
    try:
        # åŸå§‹æ¬„ä½ä¿ç•™
        description_text = row.get('Description', 'not filled')
        short_description_text = row.get('Short description', 'not filled')
        close_note_text = row.get('Close notes', 'not filled')

        # å­—ä¸²æ¸…ç†ï¼ˆä¿ç•™è®Šæ•¸å‘½åï¼‰
        desc = str(description_text).strip()
        short_desc = str(short_description_text).strip()
        close_notes = str(close_note_text).strip()

        # è‹¥å…¨éƒ¨å…§å®¹çš†ç‚ºç©ºï¼Œç›´æ¥è·³éæ­¤ç­†
        if not (desc or short_desc or close_notes):
            print(f"âš ï¸ ç¬¬ {idx+1} ç­†å…§å®¹å…¨ç‚ºç©ºç™½ï¼Œç•¥éåˆ†æ")
            return None

        # âœ… æ”¹ç”¨åˆä½µå¾Œæ¬„ä½ï¼ˆå·²ç”±å‰æ®µ fallback è™•ç†ï¼‰
        print(f"ğŸ§  [Row#{idx+1}] ä½¿ç”¨ resolution_textï¼ˆé•·åº¦ï¼š{len(resolution_text)}ï¼‰")
        print(f"ğŸ“Œ Resolution æ¬„ä½åŸå§‹åˆä½µå…§å®¹ï¼š\n{resolution_text[:1000]}")
        print(f"ğŸ§  [Row#{idx+1}] ä½¿ç”¨ summary_inputï¼ˆé•·åº¦ï¼š{len(summary_input)}ï¼‰")
        print(f"ğŸ“Œ Summary æ¬„ä½åŸå§‹åˆä½µå…§å®¹ï¼š\n{summary_input[:1000]}")



        # resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

        # if len(resolution_text) > 10000:
        #     print(f"ğŸŸ¡ [Row#{idx+1}] resolution_text > 3000ï¼Œä½¿ç”¨ short_desc + close_notes")
        #     resolution_text = f"{short_desc}\n{close_notes}".strip()
        #     if len(resolution_text) > 10000:
        #         print(f"ğŸ”´ [Row#{idx+1}] short_desc + close_notes > 3000ï¼Œåªç”¨ close_notes")
        #         resolution_text = close_notes.strip()
        # else:
        #     print(f"ğŸŸ¢ [Row#{idx+1}] resolution_text ä½¿ç”¨ desc + short_desc + close_notes")


        keyword_score = is_high_risk(short_desc, high_risk_examples, high_risk_embeddings)
        user_impact_score = is_multi_user(desc, multi_user_examples, multi_user_embeddings)
        escalation_score = is_escalated(close_notes, escalation_examples, escalation_embeddings)

        config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)
        configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0

        role_comp = row.get('Role/Component', 'not filled')
        count = component_counts.get(role_comp, 0)
        role_component_freq = 3 if count >= 5 else 2 if count >= 3 else 1 if count == 2 else 0

        this_time = row.get('Opened', 'not filled')
        if pd.isnull(this_time):
            time_cluster_score = 1
        else:
            others = df[df['Role/Component'] == role_comp]
            close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
                                  (others['Opened'] <= this_time + pd.Timedelta(hours=24))]
            count_cluster = len(close_events)
            time_cluster_score = 3 if count_cluster >= 3 else 2 if count_cluster == 2 else 1

        severity_score = round(
            keyword_score * weights['keyword'] +
            user_impact_score * weights['multi_user'] +
            escalation_score * weights['escalation'], 2
        )
        frequency_score = round(
            configuration_item_freq * weights['config_item'] +
            role_component_freq * weights['role_component'] +
            time_cluster_score * weights['time_cluster'], 2
        )
        impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
        risk_level = get_risk_level(impact_score)

        # ==== åˆ¤æ–· summary è¼¸å…¥é•·åº¦ ====
        # summary_input = f"{short_desc}\n{desc}".strip()
        # if len(summary_input) > 8000:
        #     print(f"ğŸŸ¡ [Row#{idx+1}] summary_input > 2000ï¼Œä½¿ç”¨ short_desc + close_notes")
        #     summary_input = f"{short_desc}\n{close_notes}".strip()
        #     if len(summary_input) > 8000:
        #         print(f"ğŸ”´ [Row#{idx+1}] short_desc + close_notes > 2000ï¼Œåªç”¨ short_desc")
        #         summary_input = short_desc.strip()
        # else:
        #     print(f"ğŸŸ¢ [Row#{idx+1}] summary_input ä½¿ç”¨ short_desc + desc")



        # GPT è™•ç†å…è¨±å¤±æ•—
        try:
            ai_suggestion, ai_summary = await asyncio.gather(
                extract_resolution_suggestion(resolution_text, source_id=f"Row#{idx+1}"),
                extract_problem_with_custom_prompt(summary_input, source_id=f"Row#{idx+1}")
            )

        except Exception as e:
            print(f"âš ï¸ GPT æ“·å–å¤±æ•—ï¼š{e}")
            ai_suggestion = "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"
            ai_summary = "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"

        recommended = recommend_solution(short_desc)
        keywords = extract_keywords(short_desc)

        return {
            'id': safe_value(row.get('Incident') or row.get('Number')),
            'configurationItem': safe_value(row.get('Configuration item')),
            'roleComponent': safe_value(row.get('Role/Component')),
            'subcategory': safe_value(row.get('Subcategory')),
            'aiSummary': safe_value(ai_summary),
            'originalShortDescription': safe_value(short_desc),
            'originalDescription': safe_value(desc),
            'severityScore': safe_value(severity_score),
            'frequencyScore': safe_value(frequency_score),
            'impactScore': safe_value(impact_score),
            'severityScoreNorm': round(severity_score / 10, 2),
            'frequencyScoreNorm': round(frequency_score / 20, 2),
            'impactScoreNorm': round(impact_score / 30, 2),
            'riskLevel': risk_level,
            'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
            'location': safe_value(row.get('Location')),
            'opened': row.get('Opened'),  # âœ… æ–°å¢é€™è¡Œ
            'analysisTime': analysis_time,
            'weights': {k: round(v / 10, 2) for k, v in weights.items()},
            'usedResolutionInput': safe_value(resolution_text),
            'usedSummaryInput': safe_value(summary_input),

        }

    except Exception as e:
        print(f"âŒ åˆ†æç¬¬ {idx + 1} ç­†å¤±æ•—ï¼š", e)
        return None










SENTENCE_DIR = os.path.join("data", "sentences")
os.makedirs(SENTENCE_DIR, exist_ok=True)

def get_file_path(tag):
    return os.path.join(SENTENCE_DIR, f"{tag}.json")

@app.route("/get-sentence-db")
def get_sentence_db():
    result = []
    for tag in ["high_risk", "escalate", "multi_user"]:
        path = get_file_path(tag)
        if os.path.exists(path):
            with open(path, encoding='utf-8') as f:
                sentences = json.load(f)
                for entry in sentences:
                    result.append({"text": entry["text"], "tag": tag})
    return jsonify(result)

@app.route("/save-sentence-db", methods=["POST"])
def save_sentence():
    new_entry = request.get_json()
    tag = new_entry.get("tag")
    if tag not in ["high_risk", "escalate", "multi_user"]:
        return jsonify({"message": "invalid tag"}), 400

    path = get_file_path(tag)
    if os.path.exists(path):
        with open(path, encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = []

    if any(d['text'] == new_entry['text'] for d in data):
        return jsonify({"message": "duplicate"}), 409

    data.append({"text": new_entry['text']})
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return get_sentence_db()

@app.route("/delete-sentence", methods=["POST"])
def delete_sentence():
    req = request.get_json()
    tag = req.get("tag")
    text = req.get("text")
    if tag not in ["high_risk", "escalate", "multi_user"] or not text:
        return jsonify({"message": "invalid input"}), 400

    path = get_file_path(tag)
    if not os.path.exists(path):
        return jsonify({"message": "not found"}), 404

    with open(path, encoding='utf-8') as f:
        data = json.load(f)

    new_data = [d for d in data if d['text'] != text]
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(new_data, f, ensure_ascii=False, indent=2)

    return get_sentence_db()

@app.route("/edit-sentence", methods=["POST"])
def edit_sentence():
    req = request.get_json()
    tag = req.get("tag")
    old_text = req.get("oldText")
    new_text = req.get("newText")

    if tag not in ["high_risk", "escalate", "multi_user"] or not old_text or not new_text:
        return jsonify({"message": "invalid input"}), 400

    path = get_file_path(tag)
    if not os.path.exists(path):
        return jsonify({"message": "not found"}), 404

    with open(path, encoding='utf-8') as f:
        data = json.load(f)

    updated = False
    for d in data:
        if d['text'] == old_text:
            d['text'] = new_text
            updated = True
            break

    if not updated:
        return jsonify({"message": "not found"}), 404

    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return get_sentence_db()








GPT_DATA_DIR = "gpt_data"
PROMPT_FILE = os.path.join(GPT_DATA_DIR, "gpt_prompts.json")
MAP_FILE = os.path.join(GPT_DATA_DIR, "gpt_prompt_map.json")

os.makedirs(GPT_DATA_DIR, exist_ok=True)

def read_json(path, default=None):
    if os.path.exists(path):
        with open(path, encoding='utf-8') as f:
            return json.load(f)
    return default if default is not None else {}

def write_json(path, obj):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

@app.route('/get-gpt-prompts')
def get_gpt_prompts():
    all_data = read_json(PROMPT_FILE, {})
    # åŒ…æˆ {ç”¨é€”: {"prompts": [...]} }
    wrapped = {k: {"prompts": v if isinstance(v, list) else [v]} for k, v in all_data.items()}
    return jsonify(wrapped)

@app.route('/save-gpt-prompt', methods=['POST'])
def save_gpt_prompt():
    """
    æ–°å¢ä¸€ç­† prompt åˆ°æŸå€‹åˆ†é¡ã€‚body: { "task": "solution", "prompt": "xxx" }
    """
    data = request.get_json()
    task = data.get('task')
    prompt = data.get('prompt', '').strip()

    if not task or not prompt:
        return jsonify(success=False, message='âŒ ç¼ºå°‘ task æˆ– prompt'), 400

    all_prompts = read_json(PROMPT_FILE, {})
    prompt_list = all_prompts.get(task, [])
    if not isinstance(prompt_list, list):
        prompt_list = [prompt_list]

    if prompt not in prompt_list:
        prompt_list.append(prompt)
    else:
        return jsonify(success=False, message='âš ï¸ è©² prompt å·²å­˜åœ¨'), 409

    all_prompts[task] = prompt_list
    write_json(PROMPT_FILE, all_prompts)

    return jsonify(success=True, allPrompts=all_prompts)

@app.route('/delete-gpt-prompt', methods=['POST'])
def delete_gpt_prompt():
    """
    æ”¯æ´åˆªé™¤åˆ†é¡æˆ–åˆ†é¡ä¸‹çš„å–®ä¸€å¥å­ã€‚
    body: { "task": "solution", "prompt": "xxx" } æˆ–åªçµ¦ task ä»£è¡¨æ•´é¡åˆªé™¤ã€‚
    """
    data = request.get_json()
    task = data.get('task')
    prompt = data.get('prompt', '').strip()

    all_prompts = read_json(PROMPT_FILE, {})

    if task not in all_prompts:
        return jsonify(success=False, message=f'æ‰¾ä¸åˆ°ç”¨é€” {task}'), 404

    if prompt:
        prompt_list = all_prompts[task]
        if prompt in prompt_list:
            prompt_list.remove(prompt)
            if prompt_list:
                all_prompts[task] = prompt_list
            else:
                del all_prompts[task]
        else:
            return jsonify(success=False, message='æ‰¾ä¸åˆ°è©² prompt'), 404
    else:
        del all_prompts[task]  # åˆªæ•´é¡

    write_json(PROMPT_FILE, all_prompts)

    # åˆªæ‰ mapping ä¸­çš„å°æ‡‰
    mapping = read_json(MAP_FILE, {})
    if task in mapping:
        del mapping[task]
        write_json(MAP_FILE, mapping)

    return jsonify(success=True, allPrompts=all_prompts)

@app.route('/get-gpt-prompt-map')
def get_gpt_prompt_map():
    return jsonify(read_json(MAP_FILE, {}))

@app.route('/save-gpt-prompt-map', methods=['POST'])
def save_gpt_prompt_map():
    data = request.get_json()

    solution_prompt = data.get("solution")
    summary_prompt = data.get("ai_summary")
    models = data.get("models", {})

    new_mapping = {
        "solution": {
            "prompt": solution_prompt,
            "model": models.get("solution", "")
        },
        "ai_summary": {
            "prompt": summary_prompt,
            "model": models.get("ai_summary", "")
        }
    }

    write_json(MAP_FILE, new_mapping)
    return jsonify(success=True, mapping=new_mapping)

def get_prompt_for_use(use_type):
    mapping = read_json(MAP_FILE, {})
    all_prompts = read_json(PROMPT_FILE, {})
    mapped_key = mapping.get(use_type, use_type)
    prompt_list = all_prompts.get(mapped_key, [])
    if isinstance(prompt_list, list):
        return {'prompt': prompt_list[0] if prompt_list else '', 'model': mapping.get(mapped_key, {}).get("model", "")}
    return {'prompt': prompt_list, 'model': mapping.get(mapped_key, {}).get("model", "")}

# å®šç¾©é¦–é è·¯ç”±
@app.route('/')
def index():
    return render_template('FrontEnd.html')  # æ¸²æŸ“é¦–é æ¨¡æ¿

# å®šç¾©çµæœé é¢è·¯ç”±
@app.route('/result')
def result_page():
    data = session.get('analysis_data', [])  # å¾ session å–å¾—åˆ†æçµæœ
    return render_template('result.html', data=data)  # æ¸²æŸ“çµæœé é¢

# å®šç¾©æ­·å²ç´€éŒ„é é¢è·¯ç”±
@app.route('/history')
def history_page():
    return render_template('history.html')  # æ¸²æŸ“æ­·å²ç´€éŒ„é é¢


@app.route('/generate_cluster')
def generate_cluster_page():
    return render_template('generate_cluster.html')  # æ¸²æŸ“ç”Ÿæˆåˆ†ç¾¤é é¢

@app.route("/manual_input")
def manual_input_page():
    return render_template("manual_input.html")

@app.route("/gpt_prompt")
def gpt_prompt_page():
    return render_template("gpt_prompt.html")

@app.route("/chat_ui")
def helpdesk_ui():
    return render_template("chat.html")



# ------------------------------------------------------------------------------

@app.route("/chat", methods=["POST"])
def chat_with_model():
    data = request.get_json()
    message = data.get("message", "")
    model = data.get("model", "mistral") # model é è¨­æ˜¯ mistral
    history = data.get("history", [])
    chat_id = data.get("chatId", "")  # âœ… å‰ç«¯å‚³å…¥çš„å”¯ä¸€ ID

    if not chat_id:
        return jsonify({"error": "Missing chatId"}), 400

    # å»ºç«‹è³‡æ–™å¤¾èˆ‡æª”æ¡ˆè·¯å¾‘
    os.makedirs("chat_history", exist_ok=True)
    file_path = os.path.join("chat_history", f"{chat_id}.json")

    try:
        # âœ… å‘¼å« GPT æ¨¡å‹è™•ç†ï¼ˆä½ çš„æ ¸å¿ƒé‚è¼¯ï¼‰
        reply = asyncio.run(run_offline_gpt(message, model=model, history=history, chat_id=chat_id))
        
        print("ğŸ§¾ reply é¡å‹ï¼š", type(reply))
        print("ğŸ§¾ reply å…§å®¹é è¦½ï¼š", str(reply)[:1000])

        # âœ… å®‰å…¨è™•ç†ï¼šä¿è­‰ reply ä¸€å®šæ˜¯ strï¼Œé¿å… list/dict éŒ¯èª¤
        if not isinstance(reply, str):
            print("âš ï¸ reply ä¸æ˜¯å­—ä¸²ï¼Œè‡ªå‹•è½‰æ›ç‚ºå®‰å…¨æ ¼å¼")
            try:
                reply = json.dumps(reply, ensure_ascii=False)
            except Exception as e:
                reply = f"âš ï¸ å›å‚³æ ¼å¼éŒ¯èª¤ï¼š{e}"

        # âœ… åˆ¤æ–·æ˜¯æ–°è©±é¡Œé‚„æ˜¯ç¹¼çºŒèŠ
        if not os.path.exists(file_path):
            print("ğŸ†• æ–°çš„å°è©±ç´€éŒ„ï¼Œå»ºç«‹æ–°æª”æ¡ˆ")
            # ğŸ†• é¦–æ¬¡å»ºç«‹æ–°æª”æ¡ˆ
            chat_record = {
                "id": chat_id,
                "title": chat_id,     # å›ºå®šç‚ºæª”å
                "edit_title": "",     # é è¨­ç©º
                "model": model,
                "timestamp": datetime.now().isoformat(),
                "history": history + [
                    {"role": "user", "content": message},
                    {"role": "assistant", "content": reply}
                ]
            }
        else:
            # ğŸ” è¼‰å…¥åŸæª”æ¡ˆä¸¦è¿½åŠ 
            with open(file_path, "r", encoding="utf-8") as f: # æ‰“é–‹æ—¢æœ‰çš„å°è©±ç´€éŒ„æª”æ¡ˆï¼ˆJSON æ ¼å¼ï¼‰
                chat_record = json.load(f) 

            chat_record["history"].append({"role": "user", "content": message}) # è¿½åŠ ä½¿ç”¨è€…çš„è¨Šæ¯
            chat_record["history"].append({"role": "assistant", "content": reply}) # è¿½åŠ åŠ©æ‰‹çš„å›è¦†
            print("ğŸ“ å·²è¼‰å…¥æ—¢æœ‰å°è©±ç´€éŒ„ï¼Œä¸¦è¿½åŠ æ–°çš„è¨Šæ¯ã€‚")

        # âœ… å¯«å›æª”æ¡ˆ
        with open(file_path, "w", encoding="utf-8") as f: # æ‰“é–‹æª”æ¡ˆæº–å‚™å¯«å…¥,å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œæœƒè¦†è“‹åŸå…§å®¹ã€‚
            # å°‡å°è©±ç´€éŒ„å¯«å…¥ JSON æª”æ¡ˆ
            json.dump(chat_record, f, ensure_ascii=False, indent=2)

        print(f"âœ… å°è©±ç´€éŒ„å·²å„²å­˜åˆ° {file_path}")
        return jsonify({"reply": reply}) # å›å‚³åŠ©æ‰‹çš„å›è¦†ç”¨jsonå½¢å¼

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤test: {str(e)}")  # âœ… åŠ ä¸ŠéŒ¯èª¤å…§å®¹
        return jsonify({"error": str(e)}), 500 # å¦‚æœç™¼ç”ŸéŒ¯èª¤ï¼Œå›å‚³éŒ¯èª¤è¨Šæ¯



@app.route("/rename-chat", methods=["POST"])
def rename_chat():
    data = request.get_json()
    chat_id = data.get("chatId")
    new_title = data.get("newTitle")

    if not chat_id or new_title is None:
        return jsonify({"error": "ç¼ºå°‘åƒæ•¸"}), 400

    file_path = Path(f"chat_history/{chat_id}.json")
    if not file_path.exists():
        return jsonify({"error": "æ‰¾ä¸åˆ°æª”æ¡ˆ"}), 404

    try:
        with open(file_path, encoding="utf-8") as f:
            record = json.load(f)
        record["edit_title"] = new_title  # âœ… å¯«å…¥æ–°æ¨™é¡Œ
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    

@app.route("/delete-chat/<chat_id>", methods=["DELETE"])
def delete_chat(chat_id):
    file_path = Path(f"chat_history/{chat_id}.json")
    if file_path.exists():
        try:
            file_path.unlink()  # åˆªé™¤æª”æ¡ˆ
            return jsonify({"success": True})
        except Exception as e:
            return jsonify({"error": f"ç„¡æ³•åˆªé™¤ï¼š{e}"}), 500
    return jsonify({"error": "æª”æ¡ˆä¸å­˜åœ¨"}), 404



@app.route("/chat-history-list")
def get_chat_history_list():
    records = []
    path = Path("chat_history")
    if not path.exists():
        return jsonify([])

    for file in path.glob("*.json"):
        with open(file, encoding="utf-8") as f:
            try:
                obj = json.load(f)
                records.append({
                    "id": obj.get("id"),
                    "title": obj.get("edit_title") or obj.get("title"),
                    "timestamp": obj.get("timestamp"),
                    "model": obj.get("model")
                })
            except:
                continue
    return jsonify(sorted(records, key=lambda x: x["timestamp"], reverse=True))

@app.route("/chat-history/<id>")
def get_chat_history_by_id(id):
    file_path = Path(f"chat_history/{id}.json")
    if not file_path.exists():
        return jsonify({"error": "not found"}), 404
    with open(file_path, encoding="utf-8") as f:
        return jsonify(json.load(f))




@app.route('/preview-excel', methods=['POST'])
def preview_excel():
    file = request.files.get('file')
    if not file:
        return jsonify({'error': 'æœªæä¾›æª”æ¡ˆ'})

    try:
        df = pd.read_excel(file)
        columns = df.columns.tolist()
        rows = df.head(50).fillna('').astype(str).to_dict(orient='records')  # é è¦½å‰ 50 ç­†è³‡æ–™
        return jsonify({'columns': columns, 'rows': rows})
    except Exception as e:
        return jsonify({'error': str(e)})





@app.route('/ping')
def ping():
    return "pong", 200





@app.route('/upload', methods=['POST'])
def upload_file():
    print("ğŸ“¥ æ”¶åˆ°ä¸Šå‚³è«‹æ±‚")

    if 'file' not in request.files:
        print("âŒ æ²’æœ‰ file æ¬„ä½")
        return jsonify({'error': 'æ²’æœ‰æ‰¾åˆ°æª”æ¡ˆæ¬„ä½'}), 400

    file = request.files['file']
    if file.filename == '':
        print("âš ï¸ æª”æ¡ˆåç¨±ç‚ºç©º")
        return jsonify({'error': 'æœªé¸æ“‡æª”æ¡ˆ'}), 400

    if not allowed_file(file.filename):
        print("âš ï¸ æª”æ¡ˆé¡å‹ä¸ç¬¦")
        return jsonify({'error': 'è«‹ä¸Šå‚³ .xlsx æª”æ¡ˆ'}), 400

    # æ¥æ”¶æ¬Šé‡è¨­å®š
    weights = None
    weights_raw = request.form.get('weights')
    if weights_raw:
        try:
            weights = json.loads(weights_raw)
            print("ğŸ“¥ æ”¶åˆ°æ¬Šé‡è¨­å®šï¼š", weights)
        except Exception as e:
            print(f"âš ï¸ æ¬Šé‡è§£æå¤±æ•—ï¼š{e}")
            return jsonify({'error': 'æ¬Šé‡è§£æå¤±æ•—'}), 400
    else:
        print("â„¹ï¸ æœªæä¾›è‡ªè¨‚æ¬Šé‡ï¼Œä½¿ç”¨é è¨­å€¼åˆ†æ")

    # è§£æ resolution/summary æ¬„ä½é †ä½
    try:
        resolution_priority = json.loads(request.form.get('resolution_priority', '[]'))
        summary_priority = json.loads(request.form.get('summary_priority', '[]'))
        print("ğŸ“Œ Resolution é †ä½æ¬„ä½ï¼š", resolution_priority)
        print("ğŸ“Œ Summary é †ä½æ¬„ä½ï¼š", summary_priority)
    except Exception as e:
        print(f"âš ï¸ æ¬„ä½é †ä½è§£æå¤±æ•—ï¼š{e}")
        return jsonify({'error': 'æ¬„ä½é †ä½è§£æå¤±æ•—'}), 400

    # ç”¢ç”Ÿæ™‚é–“æˆ³è¨˜èˆ‡æª”å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    uid = f"result_{timestamp}"
    original_filename = f"original_{timestamp}.xlsx"
    original_path = os.path.join('uploads', original_filename)

    try:
        file.save(original_path)
        print(f"ğŸ“ åŸå§‹æª”å·²å„²å­˜ï¼š{original_path}")
    except Exception as e:
        return jsonify({'error': f'å„²å­˜åŸå§‹æª”å¤±æ•—ï¼š{str(e)}'}), 500

    try:
        # å‘¼å«åˆ†æä¸»é‚è¼¯
        analysis_result = analyze_excel(
            original_path,
            weights=weights,
            resolution_priority=resolution_priority,
            summary_priority=summary_priority
        )
        results = analysis_result['data']
        save_analysis_files(analysis_result, uid)
        print(f"âœ… åˆ†æå®Œæˆï¼Œå…± {len(results)} ç­†")




        # è‡ªå‹•è§¸ç™¼å»ºåº«è…³æœ¬
        print("ğŸš€ è‡ªå‹•åŸ·è¡Œ build_kb.py å»ºç«‹çŸ¥è­˜åº«")
        # å‘¼å«æœ¬åœ°çš„ Python åŸ·è¡Œ build_kb.pyï¼ˆä¿è­‰å’Œ Flask ç”¨åŒä¸€å€‹è§£è­¯å™¨ï¼‰
        script_path = os.path.join(os.path.dirname(__file__), "build_kb.py")
        print("ğŸš€ å˜—è©¦ç”¨ sys.executable åŸ·è¡Œï¼š", script_path)
        subprocess.Popen([sys.executable, script_path])



        

        session['analysis_data'] = results
        return jsonify({'data': results, 'uid': uid, 'weights': weights}), 200

    except Exception as e:
        print(f"âŒ åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500
    


def make_json_serializable(obj):
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    if isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [make_json_serializable(v) for v in obj]
    return obj


    
    
def save_analysis_files(result, uid):
    os.makedirs('json_data', exist_ok=True)
    os.makedirs('excel_result_Unclustered', exist_ok=True)  # âœ… ä½¿ç”¨æ–°çš„è³‡æ–™å¤¾

    # å„²å­˜ JSON
    json_path = os.path.join(basedir, 'json_data', f"{uid}.json")
    print(f"ğŸ“ é è¨ˆå„²å­˜ JSONï¼š{json_path}")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(make_json_serializable(result), f, ensure_ascii=False, indent=2)
    print("âœ… JSON æª”æ¡ˆå·²å¯«å…¥æˆåŠŸ")

    # å„²å­˜åˆ†æå ±è¡¨ Excelï¼ˆåªå„²å­˜ result['data']ï¼‰
    df = pd.DataFrame(result['data'])
    # âœ… å„²å­˜åˆ° Unclustered è³‡æ–™å¤¾ä¸¦åŠ ä¸Š Unclustered å¾Œç¶´
    excel_filename = f"{uid}_Unclustered.xlsx"
    excel_path = os.path.join(basedir, 'excel_result_Unclustered', excel_filename)    
    df.to_excel(excel_path, index=False)

    # ç¢ºèª JSON æª”æ¡ˆæ˜¯å¦å¯«å…¥æˆåŠŸ
    if os.path.exists(json_path):
        print("âœ… JSON æª”æ¡ˆå·²æˆåŠŸå„²å­˜")
    else:
        print("âŒ JSON æª”æ¡ˆå„²å­˜å¤±æ•—ï¼")

    print(f"âœ… åˆ†æå ±è¡¨å·²å„²å­˜ï¼š{excel_path}")
    print("ğŸ“ JSON çµ•å°è·¯å¾‘ï¼š", os.path.abspath(json_path))
    print("ğŸ“ Excel çµ•å°è·¯å¾‘ï¼š", os.path.abspath(excel_path))
    timestamp = uid.replace("result_", "")
    original_excel_path = os.path.abspath(os.path.join(basedir, 'uploads', f"original_{timestamp}.xlsx"))

        # âœ… è‡ªå‹•é€å‡ºåˆ° Power Automate
    try:
        send_to_power_automate_from_file(json_path)
        print("âœ… å·²è‡ªå‹•é€å‡ºåˆ° Power Automate")
    except Exception as e:
        print(f"âš ï¸ ç™¼é€åˆ° Power Automate å¤±æ•—ï¼š{e}")


    if os.path.exists(original_excel_path):
        print("ğŸ“ åŸå§‹æª”çµ•å°è·¯å¾‘ï¼š", original_excel_path)
    else:
        print("âš ï¸ æ‰¾ä¸åˆ°åŸå§‹ Excel è·¯å¾‘ï¼")



# âœ… Power Automate çš„ URLï¼ˆè«‹æ›æˆä½ è‡ªå·±çš„ï¼‰
FLOW_URL = "https://prod-32.southeastasia.logic.azure.com:443/workflows/a016bdb3910146859b049fb7f0b6793b/triggers/manual/paths/invoke?api-version=2016-06-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=VefuSepIkpp5OhHGX7l6cgSs-rg7NykrpPhmXfKjnNk"

# âœ… æ¬„ä½åç¨±å°ç…§ï¼šåŸå§‹åç¨± â†’ è¦é€å‡ºçš„åç¨±
FIELD_MAPPING = {
    "id": "id",
    "configurationItem": "configurationItem",
    "roleComponent": "roleComponent",
    "subcategory": "subcategory",
    "aiSummary": "problem",  # â† æ”¹é€™è¡Œ
    "solution": "solution",
    "severityScore": "severityScore",
    "frequencyScore": "frequencyScore",
    "impactScore": "impactScore",
    "severityScoreNorm": "severityScore",
    "frequencyScoreNorm": "frequencyScore",
    "impactScoreNorm": "impactScore",
    "riskLevel": "riskLevel",
    "location": "location",
    "opened": "opened"
}

def default_value_for(field):
    default_values = {
        "id": "N/A",
        "configurationItem": "Unknown",
        "roleComponent": "Unknown",
        "subcategory": "Unknown",
        "problem": "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰",
        "solution": "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰",
        "severityScore": 0.0,
        "frequencyScore": 0.0,
        "impactScore": 0.0,
        "riskLevel": "æœªçŸ¥",
        "location": "æœªå¡«",
        "opened": "1970-01-01T00:00:00"
    }
    return default_values.get(field, None)


def enforce_schema_types(filtered_item):
    for field in ["severityScore", "frequencyScore", "impactScore"]:
        try:
            filtered_item[field] = float(filtered_item.get(field, 0.0))
        except:
            filtered_item[field] = 0.0

    for field in ["id", "configurationItem", "roleComponent", "subcategory",
                  "problem", "solution", "riskLevel", "location", "opened"]:
        val = filtered_item.get(field)
        if val is not None:
            filtered_item[field] = str(val)
        else:
            filtered_item[field] = default_value_for(field)





# ğŸ”’ åŠ å…¥ä½ åŸå§‹çš„ schemaï¼ˆæ”¾åœ¨ç¨‹å¼é–‹é ­æˆ–ä¸€å€‹è®Šæ•¸ä¸­ï¼‰
SCHEMA = {
    "type": "object",
    "properties": {
        "data": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "id": {"type": "string"},
                    "configurationItem": {"type": "string"},
                    "roleComponent": {"type": "string"},
                    "subcategory": {"type": "string"},
                    "problem": {"type": "string"},
                    "solution": {"type": "string"},
                    "severityScore": {"type": "number"},
                    "frequencyScore": {"type": "number"},
                    "impactScore": {"type": "number"},
                    "riskLevel": {"type": "string"},
                    "location": {"type": "string"},
                    "opened": {"type": "string"}
                },
                "required": [
                    "id", "configurationItem", "roleComponent", "subcategory",
                    "problem", "solution", "severityScore", "frequencyScore",
                    "impactScore", "riskLevel", "location", "opened"
                ]
            }
        },
        "analysisTime": {"type": "string"}
    },
    "required": ["data", "analysisTime"]
}


def send_to_power_automate_from_file(json_path):
    if not os.path.exists(json_path):
        print(f"âŒ æ‰¾ä¸åˆ°åˆ†æçµæœæª”æ¡ˆï¼š{json_path}")
        return

    def _post():
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                raw_data = json.load(f)

            filtered_data = []
            for i, item in enumerate(raw_data.get("data", [])):
                filtered_item = {}
                for old_k, new_k in FIELD_MAPPING.items():
                    if old_k in item:
                        filtered_item[new_k] = item[old_k]
                    else:
                        default_val = default_value_for(new_k)
                        print(f"âš ï¸ ç¬¬ {i+1} ç­†è³‡æ–™æ¬„ä½ç¼ºå¤±ï¼š{old_k}ï¼ˆå°æ‡‰ {new_k}ï¼‰ï¼Œå·²ä½¿ç”¨é è¨­å€¼ï¼š{default_val}")
                        filtered_item[new_k] = default_val

                enforce_schema_types(filtered_item)  # âœ… å‹åˆ¥èˆ‡é è¨­å€¼ä¿è­·
                filtered_data.append(filtered_item)

            payload = {
                "data": filtered_data,
                "analysisTime": raw_data.get("analysisTime")
            }
            print("ğŸ“¤ æ­£åœ¨é€å‡ºä»¥ä¸‹ payload çµ¦ Power Automateï¼š")
            print(json.dumps(payload, indent=2, ensure_ascii=False))





            # æª¢æŸ¥ schema æ˜¯å¦ç¬¦åˆ
            try:
                validate(instance=payload, schema=SCHEMA)
                print("âœ… JSON payload ç¬¦åˆæŒ‡å®š schemaï¼Œå¯ä»¥é€å‡ºã€‚")
            except ValidationError as ve:
                print("âŒ JSON payload ä¸ç¬¦åˆ schemaï¼")
                print("ğŸ“ éŒ¯èª¤ä½ç½®ï¼š", ve.json_path)
                print("ğŸ“‹ è©³ç´°éŒ¯èª¤ï¼š", ve.message)
                print("ğŸ“Œ ç™¼ç”Ÿæ–¼ payload ç¬¬", i+1, "ç­†ï¼ˆå¯èƒ½ï¼‰è³‡æ–™")
                return


            headers = {"Content-Type": "application/json"}
            response = requests.post(FLOW_URL, headers=headers, json=payload, timeout=120)

            if response.status_code == 200:
                print("âœ… æˆåŠŸé€å‡ºè³‡æ–™çµ¦ Power Automate")
            else:
                print(f"âš ï¸ å·²é€å‡ºï¼Œä½† HTTP ç‹€æ…‹ï¼š{response.status_code}")
        except Exception as e:
            print(f"âš ï¸ ç™¼é€ Power Automate æ™‚éŒ¯èª¤ï¼ˆå¿½ç•¥å›æ‡‰ï¼‰ï¼š{e}")

    threading.Thread(target=_post).start()
    
    
    
    
@app.route("/compare-file", methods=["POST"])
def compare_file():
    print("ğŸ“¥ æ”¶åˆ°æª”æ¡ˆæ¯”å°è«‹æ±‚") 
    uploaded_file = request.files.get("file")
    if not uploaded_file:
        print("âŒ æ²’æœ‰æ”¶åˆ°ä¸Šå‚³çš„æª”æ¡ˆ")
        return jsonify({"error": "No file uploaded"}), 400
 
    try:
        # âœ… å»ºç«‹æš«å­˜è³‡æ–™å¤¾
        temp_dir = "tmp_upload"
        os.makedirs(temp_dir, exist_ok=True)
        print(f"ğŸ“ ç¢ºä¿æš«å­˜è³‡æ–™å¤¾å­˜åœ¨ï¼š{temp_dir}")
 
        # âœ… å„²å­˜ä¸Šå‚³æª”æ¡ˆ
        temp_path = os.path.join(temp_dir, uploaded_file.filename)
        uploaded_file.save(temp_path)
        print(f"ğŸ“„ å·²å„²å­˜ä¸Šå‚³æª”æ¡ˆè‡³æš«å­˜ï¼š{temp_path}")
 
        # âœ… è®€å–ä¸Šå‚³çš„æª”æ¡ˆ
        df_new = pd.read_excel(temp_path)
        print("ğŸ“Š æˆåŠŸè®€å–ä¸Šå‚³æª”æ¡ˆç‚º DataFrame")
 
        # âœ… æ¯”å° uploads ä¸­çš„æª”æ¡ˆ
        uploads_dir = "uploads"
        print(f"ğŸ” é–‹å§‹æ¯”å° uploads è³‡æ–™å¤¾å…§æª”æ¡ˆï¼Œå…± {len(os.listdir(uploads_dir))} å€‹")
 
        for fname in os.listdir(uploads_dir):
            fpath = os.path.join(uploads_dir, fname)
            try:
                df_existing = pd.read_excel(fpath)
                print(f"ğŸ“ æ­£åœ¨æ¯”å°ï¼š{fname}")
                if df_new.equals(df_existing):
                    print(f"âœ… ç™¼ç¾é‡è¤‡æª”æ¡ˆï¼š{fname}")
                    os.remove(temp_path)
                    print(f"ğŸ§¹ å·²åˆªé™¤æš«å­˜æª”æ¡ˆï¼š{temp_path}")
                    return jsonify({"duplicate": True})
            except Exception as ex:
                print(f"âš ï¸ ç„¡æ³•è®€å–æª”æ¡ˆ {fname}ï¼š{ex}")
                continue
 
        os.remove(temp_path)
        print(f"ğŸ§¹ æ¯”å°å®Œæˆï¼Œæœªç™¼ç¾é‡è¤‡ã€‚å·²åˆªé™¤æš«å­˜æª”æ¡ˆï¼š{temp_path}")
        return jsonify({"duplicate": False})
 
    except Exception as e:
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.remove(temp_path)
            print(f"â—ç™¼ç”ŸéŒ¯èª¤ï¼Œåˆªé™¤æš«å­˜æª”æ¡ˆï¼š{temp_path}")
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return jsonify({"error": str(e)}), 500
 



@app.route('/kb-status')
def kb_status():
    lock_exists = os.path.exists("kb_building.lock")
    print(f"[DEBUG] lock file exists? {lock_exists}")
    return jsonify({"building": lock_exists})




@app.route('/get-results')
def get_results():
    folder = 'json_data'
    results = []
    first_weights = {}

    if not os.path.exists(folder):
        return jsonify({'error': f'è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼š{folder}'}), 404

    # ğŸ”„ è®€å–æ‰€æœ‰æª”æ¡ˆï¼Œæ‰¾å‡ºæœ€æ–°çš„é‚£ä»½åˆ†ææª”
    sorted_files = sorted(
        [f for f in os.listdir(folder) if f.endswith('.json')],
        reverse=True  # æœ€å¾Œé¢æœ€æ–°
    )

    for filename in sorted_files:
        filepath = os.path.join(folder, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = json.load(f)
                if isinstance(content, dict) and 'data' in content:
                    results.extend(content['data'])
                    if not first_weights and 'weights' in content:
                        first_weights = content['weights']
                elif isinstance(content, list):
                    results.extend(content)
        except Exception as e:
            print(f"âŒ éŒ¯èª¤è®€å– {filename}ï¼š{e}")

    return jsonify({
        'data': results,
        'weights': first_weights  # âœ… ç¢ºä¿å‚³å‡ºé€™å€‹æ¬„ä½
    })










# âœ… JSON é è¦½è·¯ç”±ï¼šæä¾› `/get-json?file=xxxx.json`
@app.route('/get-json', methods=['GET'])
def get_json_file():
    filename = request.args.get('file')  # e.g., result_20250423_152301.json
    if not filename:
        return jsonify({'error': 'ç¼ºå°‘ file åƒæ•¸'}), 400

    json_path = os.path.join('json_data', filename)
    if os.path.exists(json_path):
        return send_file(json_path, as_attachment=False)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„ JSON æª”æ¡ˆ'}), 404


@app.route('/download-excel', methods=['GET'])
def download_excel_file():
    uid = request.args.get('uid')  # e.g., result_20250508_203611
    if not uid:
        return jsonify({'error': 'ç¼ºå°‘ uid åƒæ•¸'}), 400

    # å…ˆæª¢æŸ¥ Clustered
    clustered_path = os.path.join('excel_result_Clustered', f"{uid}_Clustered.xlsx")
    if os.path.exists(clustered_path):
        return send_file(clustered_path, as_attachment=True)

    # å†æª¢æŸ¥ Unclustered
    unclustered_path = os.path.join('excel_result_Unclustered', f"{uid}_Unclustered.xlsx")
    if os.path.exists(unclustered_path):
        return send_file(unclustered_path, as_attachment=True)

    return jsonify({'error': f'æ‰¾ä¸åˆ° {uid} å°æ‡‰çš„ Excel æª”æ¡ˆ'}), 404


@app.route('/download-original', methods=['GET'])
def download_original_excel():
    uid = request.args.get('uid')  # uid = result_20250423_152301
    if not uid:
        return jsonify({'error': 'ç¼ºå°‘ uid åƒæ•¸'}), 400

    # å–å‡ºå°æ‡‰çš„æ™‚é–“æˆ³
    timestamp = uid.replace('result_', '')
    original_filename = f'original_{timestamp}.xlsx'
    original_path = os.path.join('uploads', original_filename)

    if os.path.exists(original_path):
        return send_file(original_path, as_attachment=True)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„åŸå§‹æª”æ¡ˆ'}), 404


# ------------------------------------------------------------------------------

# å®šç¾©æª”æ¡ˆåˆ—è¡¨è·¯ç”±
@app.route('/files', methods=['GET'])
def get_file_list():
    files = os.listdir(UPLOAD_FOLDER)  # åˆ—å‡ºä¸Šå‚³è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ
    return jsonify({'files': files}), 200

# ------------------------------------------------------------------------------

# å®šç¾©åŸ·è¡Œå‹•ä½œçš„è·¯ç”±
@app.route('/perform-action', methods=['POST'])
def perform_action():
    data = request.json  # å–å¾— JSON è³‡æ–™
    action = data.get('action')  # å–å¾—å‹•ä½œåç¨±

    if action == 'start':  # å¦‚æœå‹•ä½œæ˜¯ 'start'
        result = "Server received 'start' action and performed the task."
        print(result)
        return jsonify({'status': 'success', 'message': result}), 200
    else:  # å¦‚æœæ˜¯æœªçŸ¥å‹•ä½œ
        result = f"Server received unknown action: {action}"
        print(result)
        return jsonify({'status': 'error', 'message': 'Unknown action!'}), 400

# ------------------------------------------------------------------------------

def is_flask_running():
    """æª¢æŸ¥ Flask (127.0.0.1:5000) æ˜¯å¦å·²å•Ÿå‹•"""
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(("127.0.0.1", 5000))
        s.shutdown(socket.SHUT_RDWR)
        return True
    except:
        return False
    finally:
        s.close()

if __name__ == "__main__":
    # åˆ¤æ–· Flask æ˜¯å¦å·²ç¶“æœ‰æœå‹™
    if not is_flask_running():
        print("ğŸŒ é–‹å•Ÿç€è¦½å™¨ http://127.0.0.1:5000")
        webbrowser.open("http://127.0.0.1:5000")
    else:
        print("âš ï¸ Flask å·²åœ¨é‹ä½œï¼Œä¸é‡è¤‡é–‹å•Ÿç€è¦½å™¨")
    app.run(debug=True, use_reloader=True)











# async def analyze_excel_async(filepath, weights=None):
#     start_time = time.time()
#     default_weights = {
#         'keyword': 5.0,
#         'multi_user': 3.0,
#         'escalation': 2.0,
#         'config_item': 5.0,
#         'role_component': 3.0,
#         'time_cluster': 2.0
#     }
#     weights = {**default_weights, **(weights or {})}

#     df = pd.read_excel(filepath)
#     component_counts = df['Role/Component'].value_counts()
#     configuration_item_counts = df['Configuration item'].value_counts()
#     configuration_item_max = configuration_item_counts.max()
#     df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')
#     analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#     # éåŒæ­¥è™•ç†æ¯ç­†è³‡æ–™
#     tasks = [
#         analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time)
#         for idx, row in df.iterrows()
#     ]
#     results_raw = await asyncio.gather(*tasks)
#     results = [r for r in results_raw if r]

#     # âœ… åˆ†ç¾¤é‚è¼¯ï¼ˆç…§åŸæœ¬é‚è¼¯å³å¯ï¼‰
#     all_scores = [r['impactScore'] for r in results]
#     score_range = max(all_scores) - min(all_scores)
#     score_std = np.std(all_scores)

#     print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

#     if (
#         len(all_scores) >= KMEANS_MIN_COUNT and
#         score_range >= KMEANS_MIN_RANGE and
#         score_std >= KMEANS_MIN_STDDEV
#     ):
#         kmeans = KMeans(n_clusters=4, random_state=42)
#         labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
#         centroids = kmeans.cluster_centers_.flatten()
#         set_kmeans_thresholds_from_centroids(centroids)
#         print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
#         label_map = {}
#         for i, idx in enumerate(np.argsort(centroids)[::-1]):
#             label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
#         for i, r in enumerate(results):
#             r['riskLevel'] = label_map[labels[i]]
#         print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
#     else:
#         print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
#         for r in results:
#             r['riskLevel'] = get_risk_level(r['impactScore'])

#     total_time = time.time() - start_time
#     avg_time = total_time / len(results) if results else 0

#     print(f"\nğŸ¯ æ‰€æœ‰åˆ†æç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’")
#     print(f"ğŸ“Š å–®ç­†å¹³å‡è€—æ™‚ï¼š{avg_time:.2f} ç§’")

#     print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
#     return {
#         'data': results,
#         'analysisTime': analysis_time
#     }







# async def analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time):
#     try:
#         description_text = row.get('Description', 'not filled')
#         short_description_text = row.get('Short description', 'not filled')
#         close_note_text = row.get('Close notes', 'not filled')

#         keyword_score = is_high_risk(short_description_text)
#         user_impact_score = is_multi_user(description_text)
#         escalation_score = is_escalated(close_note_text)

#         config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)
#         configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0

#         role_comp = row.get('Role/Component', 'not filled')
#         count = component_counts.get(role_comp, 0)
#         role_component_freq = 3 if count >= 5 else 2 if count >= 3 else 1 if count == 2 else 0

#         this_time = row.get('Opened', 'not filled')
#         if pd.isnull(this_time):
#             time_cluster_score = 1
#         else:
#             others = df[df['Role/Component'] == role_comp]
#             close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
#                                   (others['Opened'] <= this_time + pd.Timedelta(hours=24))]
#             count_cluster = len(close_events)
#             time_cluster_score = 3 if count_cluster >= 3 else 2 if count_cluster == 2 else 1

#         severity_score = round(
#             keyword_score * weights['keyword'] +
#             user_impact_score * weights['multi_user'] +
#             escalation_score * weights['escalation'], 2
#         )
#         frequency_score = round(
#             configuration_item_freq * weights['config_item'] +
#             role_component_freq * weights['role_component'] +
#             time_cluster_score * weights['time_cluster'], 2
#         )
#         impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
#         risk_level = get_risk_level(impact_score)

#         desc = str(description_text).strip()
#         short_desc = str(short_description_text).strip()
#         close_notes = str(close_note_text).strip()
#         resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

#         ai_suggestion, ai_summary = await asyncio.gather(
#             extract_resolution_suggestion(resolution_text),
#             extract_problem_with_custom_prompt(f"{short_desc}\n{desc}".strip())
#         )

#         recommended = recommend_solution(short_description_text)
#         keywords = extract_keywords(short_description_text)

#         return {
#             'id': safe_value(row.get('Incident') or row.get('Number')),
#             'configurationItem': safe_value(row.get('Configuration item')),
#             'roleComponent': safe_value(row.get('Role/Component')),
#             'subcategory': safe_value(row.get('Subcategory')),
#             'aiSummary': safe_value(ai_summary),
#             'originalShortDescription': safe_value(short_desc),
#             'originalDescription': safe_value(desc),
#             'severityScore': safe_value(severity_score),
#             'frequencyScore': safe_value(frequency_score),
#             'impactScore': safe_value(impact_score),
#             'severityScoreNorm': round(severity_score / 10, 2),
#             'frequencyScoreNorm': round(frequency_score / 20, 2),
#             'impactScoreNorm': round(impact_score / 30, 2),
#             'riskLevel': risk_level,
#             'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
#             'location': safe_value(row.get('Location')),
#             'analysisTime': analysis_time,
#             'weights': {k: round(v / 10, 2) for k, v in weights.items()},
#         }

#     except Exception as e:
#         print(f"âŒ åˆ†æç¬¬ {idx+1} ç­†å¤±æ•—ï¼š", e)
#         return None











# def analyze_excel(filepath, weights=None):
#         # é è¨­æ¬Šé‡è¨­å®šï¼ˆå¯è¢«è¦†è“‹ï¼‰
#     default_weights = {
#         'keyword': 5.0,
#         'multi_user': 3.0,
#         'escalation': 2.0,
#         'config_item': 5.0,
#         'role_component': 3.0,
#         'time_cluster': 2.0
#     }
#     weights = {**default_weights, **(weights or {})}  # åˆä½µé è¨­æ¬Šé‡èˆ‡ä½¿ç”¨è€…æä¾›çš„æ¬Šé‡è¨­å®š
#     print("ğŸ›ï¸ ä½¿ç”¨ä¸­çš„æ¬Šé‡è¨­å®šï¼š", weights)
#     print("ğŸ” é–‹å§‹åˆ†æ Excel æª”æ¡ˆ...")
#     print(f"\nğŸ“‚ è®€å– Excelï¼š{filepath}")
#     df = pd.read_excel(filepath)  # è®€å– Excel æª”æ¡ˆ
#     print(f"ğŸ“Š å…±è®€å– {len(df)} ç­†è³‡æ–™\n")
#     component_counts = df['Role/Component'].value_counts()  # è¨ˆç®—æ¯å€‹è§’è‰²/å…ƒä»¶çš„å‡ºç¾æ¬¡æ•¸
#     df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')  # å°‡ 'Opened' æ¬„ä½è½‰ç‚ºæ—¥æœŸæ ¼å¼
#     results = []  # å„²å­˜åˆ†æçµæœ
#     configuration_item_counts = df['Configuration item'].value_counts()  # è¨ˆç®—æ¯å€‹é…ç½®é …çš„å‡ºç¾æ¬¡æ•¸
#     configuration_item_max = configuration_item_counts.max()  # æ‰¾å‡ºé…ç½®é …çš„æœ€å¤§å‡ºç¾æ¬¡æ•¸
#     analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
#     print(f"ğŸ“… åˆ†ææ™‚é–“ï¼š{analysis_time}")

#     for idx, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ“Š åˆ†æé€²åº¦"):
#         print(f"\nğŸ” ç¬¬ {idx + 1} ç­†åˆ†æä¸­...")
#         description_text = row.get('Description', 'not filled')  # å–å¾—æè¿°æ–‡å­—
#         short_description_text = row.get('Short description', 'not filled') # å–å¾—ç°¡çŸ­æè¿°æ–‡å­—
#         close_note_text = row.get('Close notes', 'not filled')  # å–å¾—é—œé–‰è¨»è§£æ–‡å­—
#         print(f"ğŸ“„ æè¿°ï¼š{description_text}")
#         print(f"ğŸ”‘ ç°¡çŸ­æè¿°ï¼š{short_description_text}")
#         print(f"ğŸ”’ é—œé–‰è¨»è§£ï¼š{close_note_text}")
#         # é€™è£¡å¯ä»¥åŠ å…¥å°æè¿°æ–‡å­—çš„é è™•ç†ï¼Œä¾‹å¦‚å»é™¤å¤šé¤˜ç©ºæ ¼ã€è½‰ç‚ºå°å¯«ç­‰
#         # description_text = normalize_text(description_text)  # æ¨™æº–åŒ–æ–‡å­—    


#         #é€™è£¡è¦æ”¹æˆä½¿ç”¨èªæ„åˆ†ææ¨¡å‹

#         keyword_score = is_high_risk(short_description_text)  # è¨ˆç®—é—œéµå­—åˆ†æ•¸
#         print(f"âš ï¸ é«˜é¢¨éšªèªæ„åˆ†æ•¸ï¼ˆkeyword_scoreï¼‰ï¼š{keyword_score}")
#         user_impact_score = is_multi_user(description_text)  # è¨ˆç®—ä½¿ç”¨è€…å½±éŸ¿åˆ†æ•¸
#         print(f"ğŸ‘¥ å¤šäººå½±éŸ¿åˆ†æ•¸ï¼ˆuser_impact_scoreï¼‰ï¼š{user_impact_score}")
#         escalation_score = is_escalated(close_note_text)  # è¨ˆç®—å‡ç´šè™•ç†åˆ†æ•¸
#         print(f"ğŸ“ˆ å‡ç´šè™•ç†åˆ†æ•¸ï¼ˆescalation_scoreï¼‰ï¼š{escalation_score}")



#         config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)  # å–å¾—é…ç½®é …çš„å‡ºç¾æ¬¡æ•¸
#         configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0  # è¨ˆç®—é…ç½®é …é »ç‡

#         role_comp = row.get('Role/Component', 'not filled')  # å–å¾—è§’è‰²/å…ƒä»¶
#         count = component_counts.get(role_comp, 0)  # å–å¾—è§’è‰²/å…ƒä»¶çš„å‡ºç¾æ¬¡æ•¸
#         if count >= 5:
#             role_component_freq = 3
#         elif count >= 3:
#             role_component_freq = 2
#         elif count == 2:
#             role_component_freq = 1
#         else:
#             role_component_freq = 0

#         this_time = row.get('Opened', 'not filled')  # å–å¾—é–‹å•Ÿæ™‚é–“
#         if pd.isnull(this_time):  # å¦‚æœé–‹å•Ÿæ™‚é–“ç‚ºç©º
#             time_cluster_score = 1
#         else:
#             others = df[df['Role/Component'] == role_comp]  # ç¯©é¸ç›¸åŒè§’è‰²/å…ƒä»¶çš„è³‡æ–™
#             close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
#                                   (others['Opened'] <= this_time + pd.Timedelta(hours=24))]  # æ‰¾å‡º 24 å°æ™‚å…§çš„äº‹ä»¶
#             count_cluster = len(close_events)  # è¨ˆç®—äº‹ä»¶æ•¸é‡
#             if count_cluster >= 3:
#                 time_cluster_score = 3
#             elif count_cluster == 2:
#                 time_cluster_score = 2
#             else:
#                 time_cluster_score = 1

#         severity_score = round(
#             keyword_score * weights['keyword'] +
#             user_impact_score * weights['multi_user'] +
#             escalation_score * weights['escalation'], 2
#         )

#         frequency_score = round(
#             configuration_item_freq * weights['config_item'] +
#             role_component_freq * weights['role_component'] +
#             time_cluster_score * weights['time_cluster'], 2
#         )


        
#         print(f"ğŸ“Š åš´é‡æ€§åˆ†æ•¸ï¼š{severity_score}ï¼Œé »ç‡åˆ†æ•¸ï¼š{frequency_score}")
#         print("ğŸ§  é »ç‡åˆ†æ•¸ç´°é …ï¼š")
#         print(f"ğŸ”¸ é…ç½®é …ï¼ˆConfiguration Itemï¼‰å‡ºç¾æ¯”ä¾‹ï¼š{configuration_item_freq:.2f}ï¼Œä¹˜ä»¥æ¬Šé‡å¾Œå¾— {configuration_item_freq * weights['config_item']:.2f} åˆ†")
#         print(f"ğŸ”¸ å…ƒä»¶æˆ–è§’è‰²ï¼ˆRole/Componentï¼‰åœ¨æ•´é«”ä¸­å‡ºç¾ {count} æ¬¡ â†’ çµ¦ {role_component_freq * weights['role_component']:.2f} åˆ†")
#         print(f"ğŸ”¸ åœ¨ 24 å°æ™‚å…§æœ‰ {count_cluster} ç­†åŒå…ƒä»¶äº‹ä»¶ â†’ ç¾¤èšåŠ åˆ† {time_cluster_score * weights['time_cluster']:.2f} åˆ†")
#         print(f"ğŸ“Š é »ç‡ç¸½åˆ† = {frequency_score}\n")




#         # è¨ˆç®—å½±éŸ¿åˆ†æ•¸
#         impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
#         risk_level = get_risk_level(impact_score)
#         print(f"ğŸ“‰ åš´é‡æ€§ï¼š{severity_score}, é »ç‡ï¼š{frequency_score}, ç¸½åˆ†(After KMean process)ï¼š{impact_score} â†’ åˆ†ç´šï¼š{risk_level}")
#         desc = str(row.get('Description', "")).strip()
#         short_desc = str(row.get('Short Description', "")).strip()
#         close_notes = str(row.get('Close notes', "")).strip()
#         resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

#         print(f"ğŸ“¦ Resolution åŸå§‹æ–‡å­—ï¼š{resolution_text}")  # âœ… ç¢ºèªåŸå§‹æ¬„ä½å…§å®¹

#         ai_suggestion = extract_resolution_suggestion(resolution_text)
#         print(f"ğŸ¤– GPT å»ºè­°å¥å›å‚³ï¼š{ai_suggestion}")  # âœ… ç¢ºèª GPT æ˜¯å¦æˆåŠŸå›æ‡‰

#         # âœ… å®‰å…¨åœ°å»ºç«‹ AI æ‘˜è¦è¼¸å…¥ï¼ˆè‹¥å…¨ç©ºå‰‡é¡¯ç¤ºç„¡è³‡æ–™ï¼‰
#         summary_input_text = f"{short_desc}\n{desc}".strip()
#         if not summary_input_text:
#             summary_input_text = "ï¼ˆç„¡åŸå§‹æ‘˜è¦è¼¸å…¥ï¼‰"

#         # âœ… å‘¼å« GPT æ‘˜è¦å‡½å¼
#         ai_summary = extract_problem_with_custom_prompt(summary_input_text)

#         print(f"ğŸ“¦ Resolution åŸå§‹æ–‡å­—ï¼š{resolution_text}")
#         print(f"ğŸ“ AI æ‘˜è¦è¼¸å…¥ï¼š{summary_input_text}")
#         print(f"ğŸ¤– GPT æ‘˜è¦å›å‚³ï¼š{ai_summary}")

#         # å„²å­˜åˆ†æçµæœ
#         results.append({
#             'id': safe_value(row.get('Incident') or row.get('Number')),
#             'configurationItem': safe_value(row.get('Configuration item')),
#             'roleComponent': safe_value(row.get('Role/Component')),
#             'subcategory': safe_value(row.get('Subcategory')),
#             'aiSummary': safe_value(ai_summary),
#             'originalShortDescription': safe_value(short_desc),
#             'originalDescription': safe_value(desc),
#             'severityScore': safe_value(severity_score),
#             'frequencyScore': safe_value(frequency_score),
#             'impactScore': safe_value(impact_score),
#             'severityScoreNorm': round(severity_score / 10, 2),
#             'frequencyScoreNorm': round(frequency_score / 20, 2),
#             'impactScoreNorm': round(impact_score / 30, 2),
#             'riskLevel': safe_value(get_risk_level(impact_score)),
#             'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
#             'location': safe_value(row.get('Location')),
#             'analysisTime': analysis_time,
#             'weights': {k: round(v / 10, 2) for k, v in weights.items()},
#         })

#         # solution_text = row.get('Close notes') or 'ç„¡æä¾›è§£æ³•'
#         recommended = recommend_solution(short_description_text)
#         keywords = extract_keywords(short_description_text)

#         print(f"âœ… å·²å„²å­˜ solutionï¼š{results[-1]['solution']}")
#         print(f"ğŸ’¡ å»ºè­°è§£æ³•ï¼š{recommended}")
#         print(f"ğŸ”‘ æŠ½å–é—œéµå­—ï¼š{keywords}")
#         print("â€”" * 250)  # åˆ†éš”ç·š




#     # â¬‡â¬‡â¬‡ KMeans åˆ†ç¾¤é‚è¼¯ï¼ˆæ”¯æ´ä¸‰æ¢ä»¶ï¼‰ â¬‡â¬‡â¬‡
#     all_scores = [r['impactScore'] for r in results]
#     score_range = max(all_scores) - min(all_scores)
#     score_std = np.std(all_scores)

#     print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

#     if (
#         len(all_scores) >= KMEANS_MIN_COUNT and
#         score_range >= KMEANS_MIN_RANGE and
#         score_std >= KMEANS_MIN_STDDEV
#     ):
#         kmeans = KMeans(n_clusters=4, random_state=42)
#         labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
#         centroids = kmeans.cluster_centers_.flatten()
#         set_kmeans_thresholds_from_centroids(centroids)
#         print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
#         label_map = {}
#         for i, idx in enumerate(np.argsort(centroids)[::-1]):
#             label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
#         for i, r in enumerate(results):
#             r['riskLevel'] = label_map[labels[i]]
#         print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
#     else:
#         print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
#         for r in results:
#             r['riskLevel'] = get_risk_level(r['impactScore'])
#     # â¬†â¬†â¬† åˆ†ç¾¤é‚è¼¯çµæŸ â¬†â¬†â¬†
#     print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
#     return {
#         'data': results,
#         'analysisTime': analysis_time
#     }








