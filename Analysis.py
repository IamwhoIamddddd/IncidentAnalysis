# åŒ¯å…¥ Flask æ¡†æ¶åŠç›¸é—œæ¨¡çµ„
from flask import Flask, request, jsonify, render_template, session, send_file
from gpt_utils import extract_resolution_suggestion
from gpt_utils import extract_problem_with_custom_prompt
from concurrent.futures import ThreadPoolExecutor, as_completed

from collections import defaultdict
from collections import Counter
import umap
import hdbscan
# åŒ¯å…¥æ•¸å­¸é‹ç®—æ¨¡çµ„
import math
import json
# åŒ¯å…¥ pandas ç”¨æ–¼è™•ç† Excel è³‡æ–™
import pandas as pd
# åŒ¯å…¥ os æ¨¡çµ„è™•ç†æª”æ¡ˆèˆ‡è·¯å¾‘
import os
import shutil
# åŒ¯å…¥æ­£å‰‡è¡¨é”å¼æ¨¡çµ„
import re
import glob
# åŒ¯å…¥ webbrowser ç”¨æ–¼é–‹å•Ÿç¶²é 
import webbrowser
# åŒ¯å…¥ traceback ç”¨æ–¼éŒ¯èª¤è¿½è¹¤
import traceback
# åŒ¯å…¥ Werkzeug çš„å·¥å…·å‡½æ•¸ç¢ºä¿æª”æ¡ˆåç¨±å®‰å…¨
from werkzeug.utils import secure_filename
# âœ… åŒ¯å…¥èªæ„åˆ†ææ¨¡çµ„
from SmartScoring import is_high_risk, is_escalated, is_multi_user, extract_keywords, recommend_solution
# âœ… é å…ˆ encode ä¸€ç­†è³‡æ–™ä»¥åŠ é€Ÿé¦–æ¬¡è«‹æ±‚
from SmartScoring import bert_model  # ç¢ºä¿ä½ æœ‰å¾ SmartScoring è¼‰å…¥æ¨¡å‹
from SmartScoring import extract_cluster_name  # åŒ¯å…¥è‡ªå®šçš„ cluster å‘½åå‡½å¼
from tqdm import tqdm
from sentence_transformers import util
# âœ… åŒ¯å…¥é—œéµå­—æŠ½å–æ¨¡çµ„
from datetime import datetime
from sklearn.cluster import KMeans
import numpy as np
from datetime import datetime
import time
# --- åˆ†ç¾¤å•Ÿç”¨æ¢ä»¶ï¼ˆå¯ä¾è³‡æ–™èª¿æ•´ï¼‰---
import asyncio
import math
KMEANS_MIN_COUNT = 4         # æœ€å°‘è³‡æ–™ç­†æ•¸
KMEANS_MIN_RANGE = 5.0       # åˆ†æ•¸æœ€å¤§æœ€å°å€¼å·®
KMEANS_MIN_STDDEV = 3.0      # æ¨™æº–å·®ä¸‹é™



print("ğŸ”¥ é ç†±èªæ„æ¨¡å‹ä¸­...")
bert_model.encode("warmup")  # é ç†±ä¸€æ¬¡ï¼Œé¿å…ç¬¬ä¸€æ¬¡ä½¿ç”¨å¤ªæ…¢
print("âœ… æ¨¡å‹å·²é ç†±å®Œæˆ")

# å»ºç«‹ Flask æ‡‰ç”¨
app = Flask(__name__)
# è¨­å®šæ‡‰ç”¨çš„å¯†é‘°ï¼Œç”¨æ–¼ session åŠ å¯†
app.secret_key = 'gwegweqgt22e'
# è¨­å®š session å„²å­˜æ–¹å¼ç‚ºæª”æ¡ˆç³»çµ±
app.config['SESSION_TYPE'] = 'filesystem'

# ------------------------------------------------------------------------------
# è¨­å®šä¸Šå‚³è³‡æ–™å¤¾èˆ‡å¤§å°é™åˆ¶ï¼ˆ10MBï¼‰
UPLOAD_FOLDER = 'uploads'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # é™åˆ¶æª”æ¡ˆå¤§å°ç‚º 10MB
ALLOWED_EXTENSIONS = {'xlsx'}  # åƒ…å…è¨±ä¸Šå‚³ xlsx æª”æ¡ˆ


basedir = os.path.abspath(os.path.dirname(__file__))  # å–å¾—ç•¶å‰ app.py çš„çµ•å°ç›®éŒ„


# ç¢ºä¿ä¸Šå‚³è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(os.path.join(basedir, 'json_data'), exist_ok=True)
os.makedirs(os.path.join(basedir, 'excel_result_Unclustered'), exist_ok=True)  # æ–°å¢æœªåˆ†ç¾¤è³‡æ–™å¤¾
os.makedirs(os.path.join(basedir, 'excel_result_Clustered'), exist_ok=True) # æ–°å¢åˆ†ç¾¤è³‡æ–™å¤¾



# åˆ¤æ–·æ˜¯å¦å…è¨±çš„æª”æ¡ˆæ ¼å¼
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# ------------------------------------------------------------------------------

# å®šç¾©å‡½æ•¸ï¼šè™•ç†ç‰¹æ®Šå€¼ï¼ˆå¦‚ NaNã€None ç­‰ï¼‰
def safe_value(val):
    if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
        return 0
    elif val is None:
        return None
    elif isinstance(val, str):
        return val
    else:
        return val

# ------------------------------------------------------------------------------


@app.route('/check-unclustered', methods=['GET'])
def check_unclustered_files():
    folder = 'excel_result_Unclustered'
    if not os.path.exists(folder):
        return jsonify({'exists': False}), 200
    files = [f for f in os.listdir(folder) if f.endswith('.xlsx')]
    return jsonify({'exists': len(files) > 0}), 200


@app.route('/clustered-files', methods=['GET'])
def list_clustered_files():
    clustered_folder = 'excel_result_Clustered'
    if not os.path.exists(clustered_folder):
        return jsonify({'files': []})

    pattern = re.compile(r"^Cluster-\[CI\].+_\[RC\].+_\[SC\].+\.xlsx$")
    files_info = []

    for f in os.listdir(clustered_folder):
        if not (f.endswith('.xlsx') and pattern.match(f)):
            continue

        filepath = os.path.join(clustered_folder, f)
        try:
            df = pd.read_excel(filepath)
            row_count = len(df)
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å– {f}ï¼š{e}")
            row_count = 0

        files_info.append({
            'name': f,
            'rows': row_count
        })

    # âœ… å¯é¸ï¼šä¾ç…§ row æ•¸é™å†ªæ’åºï¼ˆæœ€å¤šçš„æ’å‰é¢ï¼‰
    files_info.sort(key=lambda x: x['rows'], reverse=True)

    return jsonify({'files': files_info})


@app.route('/download-clustered', methods=['GET'])
def download_clustered_file():
    filename = request.args.get('file')
    path = os.path.join('excel_result_Clustered', filename)
    if os.path.exists(path):
        return send_file(path, as_attachment=True)
    return jsonify({'error': 'æ‰¾ä¸åˆ°æª”æ¡ˆ'}), 404




# æ ¹æ“šåˆ†æ•¸åˆ¤æ–·é¢¨éšªç­‰ç´šï¼ˆæ”¯æ´ KMeans åˆ†ç¾¤ï¼‰
kmeans_thresholds = None  # å…¨åŸŸè®Šæ•¸ï¼Œå­˜å„² KMeans åˆ†ç¾¤é–€æª»


def get_risk_level(score):
    global kmeans_thresholds
    level = ''

    if kmeans_thresholds and len(kmeans_thresholds) == 4:
        thresholds = sorted(kmeans_thresholds)
        if score >= thresholds[3]:
            level = 'é«˜é¢¨éšª'
        elif score >= thresholds[2]:
            level = 'ä¸­é¢¨éšª'
        elif score >= thresholds[1]:
            level = 'ä½é¢¨éšª'
        else:
            level = 'å¿½ç•¥'
        print(f"ğŸ“Š KMeansï¼šimpactScore: {score} â†’ åˆ†ç´šï¼š{level}ï¼ˆä½¿ç”¨å‹•æ…‹é–€æª»ï¼‰")
    else:
        if score >= 18:
            level = 'é«˜é¢¨éšª'
        elif score >= 12:
            level = 'ä¸­é¢¨éšª'
        elif score >= 6:
            level = 'ä½é¢¨éšª'
        else:
            level = 'å¿½ç•¥'
        print(f"ğŸ“Š å›ºå®šé–€æª»ï¼šimpactScore: {score} â†’ åˆ†ç´šï¼š{level}")

    return level

# åœ¨åˆ†æå®Œæˆå¾Œè‡ªå‹•è¨­å®š kmeans_thresholds
# ï¼ˆè«‹æ”¾åœ¨ KMeans åˆ†ç¾¤å®Œæˆå¾Œï¼‰
def set_kmeans_thresholds_from_centroids(centroids):
    global kmeans_thresholds
    kmeans_thresholds = sorted(centroids)
    print(f"âœ… å·²è¨­å®š KMeans åˆ†ç¾¤é–€æª»ï¼ˆsortedï¼‰ï¼š{kmeans_thresholds}")

# ------------------------------------------------------------------------------


# âœ… æ–°å¢è·¯ç”±ï¼šè™•ç†æ‰€æœ‰ Unclustered Excel æª”æ¡ˆçš„åˆ†ç¾¤èˆ‡æ¬ç§»
@app.route('/cluster-excel', methods=['POST'])
def cluster_excel():
    unclustered_dir = 'excel_result_Unclustered'
    clustered_dir = 'excel_result_Clustered'
    os.makedirs(clustered_dir, exist_ok=True)  # âœ… ç¢ºä¿ Clustered è³‡æ–™å¤¾å­˜åœ¨

    files = [f for f in os.listdir(unclustered_dir) if f.endswith('_Unclustered.xlsx')]
    print(f"ğŸ” åµæ¸¬åˆ° {len(files)} ç­†å¾…åˆ†ç¾¤æª”æ¡ˆ")

    for filename in files:
        uid = filename.replace('_Unclustered.xlsx', '')
        excel_path = os.path.join(unclustered_dir, filename)
        print(f"ğŸ“‚ è™•ç†æª”æ¡ˆï¼š{excel_path}")

        # è¼‰å…¥ Excel ä¸¦é€²è¡Œåˆ†ç¾¤åŒ¯å‡º
        df = pd.read_excel(excel_path)
        results = df.to_dict(orient='records')
        cluster_excel_export(results)  # âœ… å‘¼å«å·²å®šç¾©çš„å‡½å¼é€²è¡Œåˆ†ç¾¤åŒ¯å‡º

        # æ¬ç§»æª”æ¡ˆåˆ° Clustered ä¸¦æ”¹å
        clustered_path = os.path.join(clustered_dir, uid + '_Clustered.xlsx')
        shutil.move(excel_path, clustered_path)
        print(f"ğŸ“ å·²ç§»å‹•ä¸¦æ”¹åï¼š{clustered_path}")

    return jsonify({'message': f'å·²æˆåŠŸè™•ç† {len(files)} ç­† Excel æª”æ¡ˆä¸¦å®Œæˆåˆ†ç¾¤'}), 200

# ------------------------------------------------------------------------------

def cluster_excel_export(results, export_dir="excel_result_Clustered"):
    def clean(text):
        return re.sub(r'[^\w\-_.]', '_', str(text).strip())[:30] or "Unknown"

    cluster_data = defaultdict(list)
    for r in results:
        config_item = r.get("configurationItem", "Unknown")
        role_component = r.get("roleComponent", "Unknown")
        subcategory = r.get("subcategory", "Unknown")
        cluster_key = f"{config_item}_{role_component}_{subcategory}"
        r['cluster'] = cluster_key
        cluster_data[cluster_key].append(r)

    os.makedirs(export_dir, exist_ok=True)

    for key, group in cluster_data.items():
        cluster_df = pd.DataFrame(group)

        try:
            config_item, role_component, subcategory = key.split('_', 2)
        except ValueError:
            config_item, role_component, subcategory = key, "Unknown", "Unknown"

        filename = f"{export_dir}/Cluster-[CI]{clean(config_item)}_[RC]{clean(role_component)}_[SC]{clean(subcategory)}.xlsx"

        if os.path.exists(filename):
            old_df = pd.read_excel(filename)
            cluster_df = pd.concat([old_df, cluster_df], ignore_index=True)

        cluster_df = cluster_df.sort_values(by="analysisTime", ascending=False)
        cluster_df.to_excel(filename, index=False)
        print(f"ğŸ“ å·²è¼¸å‡ºï¼š{filename}ï¼ˆå…± {len(cluster_df)} ç­†ï¼‰")

        high_count = sum(1 for e in group if e.get('riskLevel') == 'é«˜é¢¨éšª')
        total = len(group)
        if total > 0 and (high_count / total) >= 0.5:
            print(f"ğŸš¨ é è­¦ï¼šCluster {key} æœ‰ {high_count}/{total} ç­†é«˜é¢¨éšªäº‹ä»¶")
    print("âœ… åˆ†ç¾¤ Excel æª”æ¡ˆå·²å„²å­˜ï¼")





# ç”¨æ–¼åŒæ­¥ Flask è·¯ç”±å‘¼å« async åˆ†æé‚è¼¯
def analyze_excel(filepath, weights=None):
    return asyncio.run(analyze_excel_async(filepath, weights))




async def analyze_excel_async(filepath, weights=None):
    start_time = time.time()
    default_weights = {
        'keyword': 5.0,
        'multi_user': 3.0,
        'escalation': 2.0,
        'config_item': 5.0,
        'role_component': 3.0,
        'time_cluster': 2.0
    }
    weights = {**default_weights, **(weights or {})}

    df = pd.read_excel(filepath)
    component_counts = df['Role/Component'].value_counts()
    configuration_item_counts = df['Configuration item'].value_counts()
    configuration_item_max = configuration_item_counts.max()
    df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')
    analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # éåŒæ­¥è™•ç†æ¯ç­†è³‡æ–™
    tasks = [
        analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time)
        for idx, row in df.iterrows()
    ]
    results_raw = await asyncio.gather(*tasks, return_exceptions=True)
    results = [r for r in results_raw if r and not isinstance(r, Exception)]


    # âœ… åˆ†ç¾¤é‚è¼¯ï¼ˆç…§åŸæœ¬é‚è¼¯å³å¯ï¼‰
    all_scores = [r['impactScore'] for r in results]
    score_range = max(all_scores) - min(all_scores)
    score_std = np.std(all_scores)

    print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

    if (
        len(all_scores) >= KMEANS_MIN_COUNT and
        score_range >= KMEANS_MIN_RANGE and
        score_std >= KMEANS_MIN_STDDEV
    ):
        kmeans = KMeans(n_clusters=4, random_state=42)
        labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
        centroids = kmeans.cluster_centers_.flatten()
        set_kmeans_thresholds_from_centroids(centroids)
        print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
        label_map = {}
        for i, idx in enumerate(np.argsort(centroids)[::-1]):
            label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
        for i, r in enumerate(results):
            r['riskLevel'] = label_map[labels[i]]
        print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
    else:
        print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
        for r in results:
            r['riskLevel'] = get_risk_level(r['impactScore'])

    total_time = time.time() - start_time
    avg_time = total_time / len(results) if results else 0

    print(f"\nğŸ¯ æ‰€æœ‰åˆ†æç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’")
    print(f"ğŸ“Š å–®ç­†å¹³å‡è€—æ™‚ï¼š{avg_time:.2f} ç§’")

    print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
    return {
        'data': results,
        'analysisTime': analysis_time
    }







async def analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time):
    try:
        description_text = row.get('Description', 'not filled')
        short_description_text = row.get('Short description', 'not filled')
        close_note_text = row.get('Close notes', 'not filled')

        keyword_score = is_high_risk(short_description_text)
        user_impact_score = is_multi_user(description_text)
        escalation_score = is_escalated(close_note_text)

        config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)
        configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0

        role_comp = row.get('Role/Component', 'not filled')
        count = component_counts.get(role_comp, 0)
        role_component_freq = 3 if count >= 5 else 2 if count >= 3 else 1 if count == 2 else 0

        this_time = row.get('Opened', 'not filled')
        if pd.isnull(this_time):
            time_cluster_score = 1
        else:
            others = df[df['Role/Component'] == role_comp]
            close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
                                  (others['Opened'] <= this_time + pd.Timedelta(hours=24))]
            count_cluster = len(close_events)
            time_cluster_score = 3 if count_cluster >= 3 else 2 if count_cluster == 2 else 1

        severity_score = round(
            keyword_score * weights['keyword'] +
            user_impact_score * weights['multi_user'] +
            escalation_score * weights['escalation'], 2
        )
        frequency_score = round(
            configuration_item_freq * weights['config_item'] +
            role_component_freq * weights['role_component'] +
            time_cluster_score * weights['time_cluster'], 2
        )
        impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
        risk_level = get_risk_level(impact_score)

        desc = str(description_text).strip()
        short_desc = str(short_description_text).strip()
        close_notes = str(close_note_text).strip()
        resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

        ai_suggestion, ai_summary = await asyncio.gather(
            extract_resolution_suggestion(resolution_text),
            extract_problem_with_custom_prompt(f"{short_desc}\n{desc}".strip())
        )

        recommended = recommend_solution(short_description_text)
        keywords = extract_keywords(short_description_text)

        return {
            'id': safe_value(row.get('Incident') or row.get('Number')),
            'configurationItem': safe_value(row.get('Configuration item')),
            'roleComponent': safe_value(row.get('Role/Component')),
            'subcategory': safe_value(row.get('Subcategory')),
            'aiSummary': safe_value(ai_summary),
            'originalShortDescription': safe_value(short_desc),
            'originalDescription': safe_value(desc),
            'severityScore': safe_value(severity_score),
            'frequencyScore': safe_value(frequency_score),
            'impactScore': safe_value(impact_score),
            'severityScoreNorm': round(severity_score / 10, 2),
            'frequencyScoreNorm': round(frequency_score / 20, 2),
            'impactScoreNorm': round(impact_score / 30, 2),
            'riskLevel': risk_level,
            'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
            'location': safe_value(row.get('Location')),
            'analysisTime': analysis_time,
            'weights': {k: round(v / 10, 2) for k, v in weights.items()},
        }

    except Exception as e:
        print(f"âŒ åˆ†æç¬¬ {idx+1} ç­†å¤±æ•—ï¼š", e)
        return None





















# ------------------------------------------------------------------------------


# # åˆ†æ Excel è³‡æ–™çš„ä¸»é‚è¼¯ 
# def analyze_excel(filepath, weights=None):

#     start_time = time.time()
#     default_weights = {
#         'keyword': 5.0,
#         'multi_user': 3.0,
#         'escalation': 2.0,
#         'config_item': 5.0,
#         'role_component': 3.0,
#         'time_cluster': 2.0
#     }
#     weights = {**default_weights, **(weights or {})}
#     print("ğŸ›ï¸ ä½¿ç”¨ä¸­çš„æ¬Šé‡è¨­å®šï¼š", weights)

#     df = pd.read_excel(filepath)
#     print(f"ğŸ“Š å…±è®€å– {len(df)} ç­†è³‡æ–™\n")

#     component_counts = df['Role/Component'].value_counts()
#     df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')
#     configuration_item_counts = df['Configuration item'].value_counts()
#     configuration_item_max = configuration_item_counts.max()
#     analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#     def analyze_row(row, idx):
#         try:
#             description_text = row.get('Description', 'not filled')
#             short_description_text = row.get('Short description', 'not filled')
#             close_note_text = row.get('Close notes', 'not filled')

#             keyword_score = is_high_risk(short_description_text)
#             user_impact_score = is_multi_user(description_text)
#             escalation_score = is_escalated(close_note_text)

#             config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)
#             configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0

#             role_comp = row.get('Role/Component', 'not filled')
#             count = component_counts.get(role_comp, 0)
#             role_component_freq = 3 if count >= 5 else 2 if count >= 3 else 1 if count == 2 else 0

#             this_time = row.get('Opened', 'not filled')
#             if pd.isnull(this_time):
#                 time_cluster_score = 1
#             else:
#                 others = df[df['Role/Component'] == role_comp]
#                 close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
#                                       (others['Opened'] <= this_time + pd.Timedelta(hours=24))]
#                 count_cluster = len(close_events)
#                 time_cluster_score = 3 if count_cluster >= 3 else 2 if count_cluster == 2 else 1

#             severity_score = round(
#                 keyword_score * weights['keyword'] +
#                 user_impact_score * weights['multi_user'] +
#                 escalation_score * weights['escalation'], 2
#             )

#             frequency_score = round(
#                 configuration_item_freq * weights['config_item'] +
#                 role_component_freq * weights['role_component'] +
#                 time_cluster_score * weights['time_cluster'], 2
#             )

#             impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
#             risk_level = get_risk_level(impact_score)

#             desc = str(row.get('Description', "")).strip()
#             short_desc = str(row.get('Short description', "")).strip()
#             close_notes = str(row.get('Close notes', "")).strip()
#             resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()
#             ai_suggestion = extract_resolution_suggestion(resolution_text)
#             ai_summary = extract_problem_with_custom_prompt(f"{short_desc}\n{desc}".strip())
#             recommended = recommend_solution(short_description_text)
#             keywords = extract_keywords(short_description_text)

#             return {
#                 'id': safe_value(row.get('Incident') or row.get('Number')),
#                 'configurationItem': safe_value(row.get('Configuration item')),
#                 'roleComponent': safe_value(row.get('Role/Component')),
#                 'subcategory': safe_value(row.get('Subcategory')),
#                 'aiSummary': safe_value(ai_summary),
#                 'originalShortDescription': safe_value(short_desc),
#                 'originalDescription': safe_value(desc),
#                 'severityScore': safe_value(severity_score),
#                 'frequencyScore': safe_value(frequency_score),
#                 'impactScore': safe_value(impact_score),
#                 'severityScoreNorm': round(severity_score / 10, 2),
#                 'frequencyScoreNorm': round(frequency_score / 20, 2),
#                 'impactScoreNorm': round(impact_score / 30, 2),
#                 'riskLevel': risk_level,
#                 'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
#                 'location': safe_value(row.get('Location')),
#                 'analysisTime': analysis_time,
#                 'weights': {k: round(v / 10, 2) for k, v in weights.items()},
#             }

#         except Exception as e:
#             print(f"âŒ åˆ†æç¬¬ {idx+1} ç­†å¤±æ•—ï¼š", e)
#             return None

#     # âœ… éåŒæ­¥è™•ç†æ‰€æœ‰ row
#     results = []
#     per_row_times = []
#     with ThreadPoolExecutor(max_workers=8) as executor:
#         futures = {}
#         for idx, row in df.iterrows():
#             futures[executor.submit(analyze_row, row, idx)] = idx

#         for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ“Š éåŒæ­¥åˆ†æä¸­"):
#             idx = futures[future]
#             t0 = time.time()
#             res = future.result()
#             t1 = time.time()
#             elapsed = t1 - t0
#             per_row_times.append(elapsed)

#             if res:
#                 results.append(res)
#             print(f"â±ï¸ ç¬¬ {idx + 1} ç­†ï¼š{elapsed:.2f} ç§’å®Œæˆ")


#     # âœ… KMeans åˆ†ç¾¤ï¼ˆç•¥ï¼‰
#     # å¯ä¾ç…§ä½ åŸæœ¬çš„é‚è¼¯å¥—ç”¨ KMeansï¼Œå¦‚ï¼š
#     # â¬‡â¬‡â¬‡ KMeans åˆ†ç¾¤é‚è¼¯ï¼ˆæ”¯æ´ä¸‰æ¢ä»¶ï¼‰ â¬‡â¬‡â¬‡
#     all_scores = [r['impactScore'] for r in results]
#     score_range = max(all_scores) - min(all_scores)
#     score_std = np.std(all_scores)

#     print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

#     if (
#         len(all_scores) >= KMEANS_MIN_COUNT and
#         score_range >= KMEANS_MIN_RANGE and
#         score_std >= KMEANS_MIN_STDDEV
#     ):
#         kmeans = KMeans(n_clusters=4, random_state=42)
#         labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
#         centroids = kmeans.cluster_centers_.flatten()
#         set_kmeans_thresholds_from_centroids(centroids)
#         print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
#         label_map = {}
#         for i, idx in enumerate(np.argsort(centroids)[::-1]):
#             label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
#         for i, r in enumerate(results):
#             r['riskLevel'] = label_map[labels[i]]
#         print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
#     else:
#         print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
#         for r in results:
#             r['riskLevel'] = get_risk_level(r['impactScore'])
#     # â¬†â¬†â¬† åˆ†ç¾¤é‚è¼¯çµæŸ â¬†â¬†â¬†


#     total_time = time.time() - start_time
#     avg_time = sum(per_row_times) / len(per_row_times) if per_row_times else 0
#     print(f"\nğŸ¯ æ‰€æœ‰åˆ†æç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’")
#     print(f"ğŸ“Š å–®ç­†å¹³å‡è€—æ™‚ï¼š{avg_time:.2f} ç§’")

#     print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
#     return {
#         'data': results,
#         'analysisTime': analysis_time
#     }


# ------------------------------------------------------------------------------

# å®šç¾©é¦–é è·¯ç”±
@app.route('/')
def index():
    return render_template('FrontEnd.html')  # æ¸²æŸ“é¦–é æ¨¡æ¿

# å®šç¾©çµæœé é¢è·¯ç”±
@app.route('/result')
def result_page():
    data = session.get('analysis_data', [])  # å¾ session å–å¾—åˆ†æçµæœ
    return render_template('result.html', data=data)  # æ¸²æŸ“çµæœé é¢

# å®šç¾©æ­·å²ç´€éŒ„é é¢è·¯ç”±
@app.route('/history')
def history_page():
    return render_template('history.html')  # æ¸²æŸ“æ­·å²ç´€éŒ„é é¢


@app.route('/generate_cluster')
def generate_cluster_page():
    return render_template('generate_cluster.html')  # æ¸²æŸ“ç”Ÿæˆåˆ†ç¾¤é é¢

# ------------------------------------------------------------------------------




@app.route('/ping')
def ping():
    return "pong", 200


# å®šç¾©æª”æ¡ˆä¸Šå‚³è·¯ç”±
@app.route('/upload', methods=['POST'])
def upload_file():
    print("ğŸ“¥ æ”¶åˆ°ä¸Šå‚³è«‹æ±‚")  # ç´€éŒ„è«‹æ±‚



    if 'file' not in request.files:  # æª¢æŸ¥æ˜¯å¦æœ‰æª”æ¡ˆæ¬„ä½
        print("âŒ æ²’æœ‰ file æ¬„ä½")
        return jsonify({'error': 'æ²’æœ‰æ‰¾åˆ°æª”æ¡ˆæ¬„ä½'}), 400

    file = request.files['file']  # å–å¾—æª”æ¡ˆ
    if file.filename == '':  # æª¢æŸ¥æª”æ¡ˆåç¨±æ˜¯å¦ç‚ºç©º
        print("âš ï¸ æª”æ¡ˆåç¨±ç‚ºç©º")
        return jsonify({'error': 'æœªé¸æ“‡æª”æ¡ˆ'}), 400

    if not allowed_file(file.filename):  # æª¢æŸ¥æª”æ¡ˆæ ¼å¼æ˜¯å¦å…è¨±
        print("âš ï¸ æª”æ¡ˆé¡å‹ä¸ç¬¦")
        return jsonify({'error': 'è«‹ä¸Šå‚³ .xlsx æª”æ¡ˆ'}), 400
        
    # æ¥æ”¶è‡ªè¨‚æ¬Šé‡
    weights_raw = request.form.get('weights')
    if not weights_raw:
        print("â„¹ï¸ æœªæä¾›è‡ªè¨‚æ¬Šé‡ï¼Œä½¿ç”¨é è¨­å€¼åˆ†æ")

    weights = None
    if weights_raw:
        try:
            weights = json.loads(weights_raw)
            print("ğŸ“¥ æ”¶åˆ°æ¬Šé‡è¨­å®šï¼š", weights)
        except Exception as e:
            print(f"âš ï¸ æ¬Šé‡è§£æå¤±æ•—ï¼š{e}")
            return jsonify({'error': 'æ¬Šé‡è§£æå¤±æ•—'}), 400
        
    # ç”¢ç”Ÿæ™‚é–“æˆ³è¨˜èˆ‡æª”å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    uid = f"result_{timestamp}" # ä¾‹å¦‚ result_20250423_152301 åˆ†æçµæœæª”åç¨±
    original_filename = f"original_{timestamp}.xlsx" # ä¾‹å¦‚ original_20250423_152301.xlsx åŸå§‹é»¨åç¨±
    original_path = os.path.join('uploads', original_filename)



    try:
        file.save(original_path)  # å„²å­˜åŸå§‹æª”æ¡ˆ
        print(f" åŸå§‹æª”å·²å„²å­˜ï¼š{original_path}")
    except Exception as e:
        return jsonify({'error': f'å„²å­˜åŸå§‹æª”å¤±æ•—ï¼š{str(e)}'}), 500

    try:
        analysis_result = analyze_excel(original_path, weights=weights)
        results = analysis_result['data']  # å–å¾—åˆ†æçµæœ


        save_analysis_files(analysis_result, uid)  # å„²å­˜åˆ†æçµæœæª”æ¡ˆ

        print(f"âœ… åˆ†æå®Œæˆï¼Œå…± {len(results)} ç­†")
        session['analysis_data'] = results  # å„²å­˜åˆ†æçµæœåˆ° session
        return jsonify({'data': results, 'uid': uid, 'weights': weights}), 200


    


    except Exception as e:
        print(f"âŒ åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        traceback.print_exc()  # å°å‡ºå®Œæ•´éŒ¯èª¤å †ç–Š
        return jsonify({'error': str(e)}), 500
    
def save_analysis_files(result, uid):
    os.makedirs('json_data', exist_ok=True)
    os.makedirs('excel_result_Unclustered', exist_ok=True)  # âœ… ä½¿ç”¨æ–°çš„è³‡æ–™å¤¾

    # å„²å­˜ JSON
    json_path = os.path.join(basedir, 'json_data', f"{uid}.json")
    print(f"ğŸ“ é è¨ˆå„²å­˜ JSONï¼š{json_path}")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print("âœ… JSON æª”æ¡ˆå·²å¯«å…¥æˆåŠŸ")

    # å„²å­˜åˆ†æå ±è¡¨ Excelï¼ˆåªå„²å­˜ result['data']ï¼‰
    df = pd.DataFrame(result['data'])
    # âœ… å„²å­˜åˆ° Unclustered è³‡æ–™å¤¾ä¸¦åŠ ä¸Š Unclustered å¾Œç¶´
    excel_filename = f"{uid}_Unclustered.xlsx"
    excel_path = os.path.join(basedir, 'excel_result_Unclustered', excel_filename)    
    df.to_excel(excel_path, index=False)

    # ç¢ºèª JSON æª”æ¡ˆæ˜¯å¦å¯«å…¥æˆåŠŸ
    if os.path.exists(json_path):
        print("âœ… JSON æª”æ¡ˆå·²æˆåŠŸå„²å­˜")
    else:
        print("âŒ JSON æª”æ¡ˆå„²å­˜å¤±æ•—ï¼")

    print(f"âœ… åˆ†æå ±è¡¨å·²å„²å­˜ï¼š{excel_path}")
    print("ğŸ“ JSON çµ•å°è·¯å¾‘ï¼š", os.path.abspath(json_path))
    print("ğŸ“ Excel çµ•å°è·¯å¾‘ï¼š", os.path.abspath(excel_path))
    timestamp = uid.replace("result_", "")
    original_excel_path = os.path.abspath(os.path.join(basedir, 'uploads', f"original_{timestamp}.xlsx"))

    if os.path.exists(original_excel_path):
        print("ğŸ“ åŸå§‹æª”çµ•å°è·¯å¾‘ï¼š", original_excel_path)
    else:
        print("âš ï¸ æ‰¾ä¸åˆ°åŸå§‹ Excel è·¯å¾‘ï¼")


@app.route('/get-results')
def get_results():
    folder = 'json_data'
    results = []
    first_weights = {}

    if not os.path.exists(folder):
        return jsonify({'error': f'è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼š{folder}'}), 404

    # ğŸ”„ è®€å–æ‰€æœ‰æª”æ¡ˆï¼Œæ‰¾å‡ºæœ€æ–°çš„é‚£ä»½åˆ†ææª”
    sorted_files = sorted(
        [f for f in os.listdir(folder) if f.endswith('.json')],
        reverse=True  # æœ€å¾Œé¢æœ€æ–°
    )

    for filename in sorted_files:
        filepath = os.path.join(folder, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = json.load(f)
                if isinstance(content, dict) and 'data' in content:
                    results.extend(content['data'])
                    if not first_weights and 'weights' in content:
                        first_weights = content['weights']
                elif isinstance(content, list):
                    results.extend(content)
        except Exception as e:
            print(f"âŒ éŒ¯èª¤è®€å– {filename}ï¼š{e}")

    return jsonify({
        'data': results,
        'weights': first_weights  # âœ… ç¢ºä¿å‚³å‡ºé€™å€‹æ¬„ä½
    })










# âœ… JSON é è¦½è·¯ç”±ï¼šæä¾› `/get-json?file=xxxx.json`
@app.route('/get-json', methods=['GET'])
def get_json_file():
    filename = request.args.get('file')  # e.g., result_20250423_152301.json
    if not filename:
        return jsonify({'error': 'ç¼ºå°‘ file åƒæ•¸'}), 400

    json_path = os.path.join('json_data', filename)
    if os.path.exists(json_path):
        return send_file(json_path, as_attachment=False)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„ JSON æª”æ¡ˆ'}), 404


@app.route('/download-excel', methods=['GET'])
def download_excel_file():
    uid = request.args.get('uid')  # e.g., result_20250508_203611
    if not uid:
        return jsonify({'error': 'ç¼ºå°‘ uid åƒæ•¸'}), 400

    # å…ˆæª¢æŸ¥ Clustered
    clustered_path = os.path.join('excel_result_Clustered', f"{uid}_Clustered.xlsx")
    if os.path.exists(clustered_path):
        return send_file(clustered_path, as_attachment=True)

    # å†æª¢æŸ¥ Unclustered
    unclustered_path = os.path.join('excel_result_Unclustered', f"{uid}_Unclustered.xlsx")
    if os.path.exists(unclustered_path):
        return send_file(unclustered_path, as_attachment=True)

    return jsonify({'error': f'æ‰¾ä¸åˆ° {uid} å°æ‡‰çš„ Excel æª”æ¡ˆ'}), 404


@app.route('/download-original', methods=['GET'])
def download_original_excel():
    uid = request.args.get('uid')  # uid = result_20250423_152301
    if not uid:
        return jsonify({'error': 'ç¼ºå°‘ uid åƒæ•¸'}), 400

    # å–å‡ºå°æ‡‰çš„æ™‚é–“æˆ³
    timestamp = uid.replace('result_', '')
    original_filename = f'original_{timestamp}.xlsx'
    original_path = os.path.join('uploads', original_filename)

    if os.path.exists(original_path):
        return send_file(original_path, as_attachment=True)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„åŸå§‹æª”æ¡ˆ'}), 404


# ------------------------------------------------------------------------------

# å®šç¾©æª”æ¡ˆåˆ—è¡¨è·¯ç”±
@app.route('/files', methods=['GET'])
def get_file_list():
    files = os.listdir(UPLOAD_FOLDER)  # åˆ—å‡ºä¸Šå‚³è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ
    return jsonify({'files': files}), 200

# ------------------------------------------------------------------------------

# å®šç¾©åŸ·è¡Œå‹•ä½œçš„è·¯ç”±
@app.route('/perform-action', methods=['POST'])
def perform_action():
    data = request.json  # å–å¾— JSON è³‡æ–™
    action = data.get('action')  # å–å¾—å‹•ä½œåç¨±

    if action == 'start':  # å¦‚æœå‹•ä½œæ˜¯ 'start'
        result = "Server received 'start' action and performed the task."
        print(result)
        return jsonify({'status': 'success', 'message': result}), 200
    else:  # å¦‚æœæ˜¯æœªçŸ¥å‹•ä½œ
        result = f"Server received unknown action: {action}"
        print(result)
        return jsonify({'status': 'error', 'message': 'Unknown action!'}), 400

# ------------------------------------------------------------------------------

# å•Ÿå‹• Flask æ‡‰ç”¨
if __name__ == '__main__':
    app.run(debug=True, use_reloader=True)











# async def analyze_excel_async(filepath, weights=None):
#     start_time = time.time()
#     default_weights = {
#         'keyword': 5.0,
#         'multi_user': 3.0,
#         'escalation': 2.0,
#         'config_item': 5.0,
#         'role_component': 3.0,
#         'time_cluster': 2.0
#     }
#     weights = {**default_weights, **(weights or {})}

#     df = pd.read_excel(filepath)
#     component_counts = df['Role/Component'].value_counts()
#     configuration_item_counts = df['Configuration item'].value_counts()
#     configuration_item_max = configuration_item_counts.max()
#     df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')
#     analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#     # éåŒæ­¥è™•ç†æ¯ç­†è³‡æ–™
#     tasks = [
#         analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time)
#         for idx, row in df.iterrows()
#     ]
#     results_raw = await asyncio.gather(*tasks)
#     results = [r for r in results_raw if r]

#     # âœ… åˆ†ç¾¤é‚è¼¯ï¼ˆç…§åŸæœ¬é‚è¼¯å³å¯ï¼‰
#     all_scores = [r['impactScore'] for r in results]
#     score_range = max(all_scores) - min(all_scores)
#     score_std = np.std(all_scores)

#     print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

#     if (
#         len(all_scores) >= KMEANS_MIN_COUNT and
#         score_range >= KMEANS_MIN_RANGE and
#         score_std >= KMEANS_MIN_STDDEV
#     ):
#         kmeans = KMeans(n_clusters=4, random_state=42)
#         labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
#         centroids = kmeans.cluster_centers_.flatten()
#         set_kmeans_thresholds_from_centroids(centroids)
#         print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
#         label_map = {}
#         for i, idx in enumerate(np.argsort(centroids)[::-1]):
#             label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
#         for i, r in enumerate(results):
#             r['riskLevel'] = label_map[labels[i]]
#         print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
#     else:
#         print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
#         for r in results:
#             r['riskLevel'] = get_risk_level(r['impactScore'])

#     total_time = time.time() - start_time
#     avg_time = total_time / len(results) if results else 0

#     print(f"\nğŸ¯ æ‰€æœ‰åˆ†æç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’")
#     print(f"ğŸ“Š å–®ç­†å¹³å‡è€—æ™‚ï¼š{avg_time:.2f} ç§’")

#     print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
#     return {
#         'data': results,
#         'analysisTime': analysis_time
#     }







# async def analyze_row_async(row, idx, df, weights, component_counts, configuration_item_counts, configuration_item_max, analysis_time):
#     try:
#         description_text = row.get('Description', 'not filled')
#         short_description_text = row.get('Short description', 'not filled')
#         close_note_text = row.get('Close notes', 'not filled')

#         keyword_score = is_high_risk(short_description_text)
#         user_impact_score = is_multi_user(description_text)
#         escalation_score = is_escalated(close_note_text)

#         config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)
#         configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0

#         role_comp = row.get('Role/Component', 'not filled')
#         count = component_counts.get(role_comp, 0)
#         role_component_freq = 3 if count >= 5 else 2 if count >= 3 else 1 if count == 2 else 0

#         this_time = row.get('Opened', 'not filled')
#         if pd.isnull(this_time):
#             time_cluster_score = 1
#         else:
#             others = df[df['Role/Component'] == role_comp]
#             close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
#                                   (others['Opened'] <= this_time + pd.Timedelta(hours=24))]
#             count_cluster = len(close_events)
#             time_cluster_score = 3 if count_cluster >= 3 else 2 if count_cluster == 2 else 1

#         severity_score = round(
#             keyword_score * weights['keyword'] +
#             user_impact_score * weights['multi_user'] +
#             escalation_score * weights['escalation'], 2
#         )
#         frequency_score = round(
#             configuration_item_freq * weights['config_item'] +
#             role_component_freq * weights['role_component'] +
#             time_cluster_score * weights['time_cluster'], 2
#         )
#         impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
#         risk_level = get_risk_level(impact_score)

#         desc = str(description_text).strip()
#         short_desc = str(short_description_text).strip()
#         close_notes = str(close_note_text).strip()
#         resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

#         ai_suggestion, ai_summary = await asyncio.gather(
#             extract_resolution_suggestion(resolution_text),
#             extract_problem_with_custom_prompt(f"{short_desc}\n{desc}".strip())
#         )

#         recommended = recommend_solution(short_description_text)
#         keywords = extract_keywords(short_description_text)

#         return {
#             'id': safe_value(row.get('Incident') or row.get('Number')),
#             'configurationItem': safe_value(row.get('Configuration item')),
#             'roleComponent': safe_value(row.get('Role/Component')),
#             'subcategory': safe_value(row.get('Subcategory')),
#             'aiSummary': safe_value(ai_summary),
#             'originalShortDescription': safe_value(short_desc),
#             'originalDescription': safe_value(desc),
#             'severityScore': safe_value(severity_score),
#             'frequencyScore': safe_value(frequency_score),
#             'impactScore': safe_value(impact_score),
#             'severityScoreNorm': round(severity_score / 10, 2),
#             'frequencyScoreNorm': round(frequency_score / 20, 2),
#             'impactScoreNorm': round(impact_score / 30, 2),
#             'riskLevel': risk_level,
#             'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
#             'location': safe_value(row.get('Location')),
#             'analysisTime': analysis_time,
#             'weights': {k: round(v / 10, 2) for k, v in weights.items()},
#         }

#     except Exception as e:
#         print(f"âŒ åˆ†æç¬¬ {idx+1} ç­†å¤±æ•—ï¼š", e)
#         return None











# def analyze_excel(filepath, weights=None):
#         # é è¨­æ¬Šé‡è¨­å®šï¼ˆå¯è¢«è¦†è“‹ï¼‰
#     default_weights = {
#         'keyword': 5.0,
#         'multi_user': 3.0,
#         'escalation': 2.0,
#         'config_item': 5.0,
#         'role_component': 3.0,
#         'time_cluster': 2.0
#     }
#     weights = {**default_weights, **(weights or {})}  # åˆä½µé è¨­æ¬Šé‡èˆ‡ä½¿ç”¨è€…æä¾›çš„æ¬Šé‡è¨­å®š
#     print("ğŸ›ï¸ ä½¿ç”¨ä¸­çš„æ¬Šé‡è¨­å®šï¼š", weights)
#     print("ğŸ” é–‹å§‹åˆ†æ Excel æª”æ¡ˆ...")
#     print(f"\nğŸ“‚ è®€å– Excelï¼š{filepath}")
#     df = pd.read_excel(filepath)  # è®€å– Excel æª”æ¡ˆ
#     print(f"ğŸ“Š å…±è®€å– {len(df)} ç­†è³‡æ–™\n")
#     component_counts = df['Role/Component'].value_counts()  # è¨ˆç®—æ¯å€‹è§’è‰²/å…ƒä»¶çš„å‡ºç¾æ¬¡æ•¸
#     df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')  # å°‡ 'Opened' æ¬„ä½è½‰ç‚ºæ—¥æœŸæ ¼å¼
#     results = []  # å„²å­˜åˆ†æçµæœ
#     configuration_item_counts = df['Configuration item'].value_counts()  # è¨ˆç®—æ¯å€‹é…ç½®é …çš„å‡ºç¾æ¬¡æ•¸
#     configuration_item_max = configuration_item_counts.max()  # æ‰¾å‡ºé…ç½®é …çš„æœ€å¤§å‡ºç¾æ¬¡æ•¸
#     analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
#     print(f"ğŸ“… åˆ†ææ™‚é–“ï¼š{analysis_time}")

#     for idx, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ“Š åˆ†æé€²åº¦"):
#         print(f"\nğŸ” ç¬¬ {idx + 1} ç­†åˆ†æä¸­...")
#         description_text = row.get('Description', 'not filled')  # å–å¾—æè¿°æ–‡å­—
#         short_description_text = row.get('Short description', 'not filled') # å–å¾—ç°¡çŸ­æè¿°æ–‡å­—
#         close_note_text = row.get('Close notes', 'not filled')  # å–å¾—é—œé–‰è¨»è§£æ–‡å­—
#         print(f"ğŸ“„ æè¿°ï¼š{description_text}")
#         print(f"ğŸ”‘ ç°¡çŸ­æè¿°ï¼š{short_description_text}")
#         print(f"ğŸ”’ é—œé–‰è¨»è§£ï¼š{close_note_text}")
#         # é€™è£¡å¯ä»¥åŠ å…¥å°æè¿°æ–‡å­—çš„é è™•ç†ï¼Œä¾‹å¦‚å»é™¤å¤šé¤˜ç©ºæ ¼ã€è½‰ç‚ºå°å¯«ç­‰
#         # description_text = normalize_text(description_text)  # æ¨™æº–åŒ–æ–‡å­—    


#         #é€™è£¡è¦æ”¹æˆä½¿ç”¨èªæ„åˆ†ææ¨¡å‹

#         keyword_score = is_high_risk(short_description_text)  # è¨ˆç®—é—œéµå­—åˆ†æ•¸
#         print(f"âš ï¸ é«˜é¢¨éšªèªæ„åˆ†æ•¸ï¼ˆkeyword_scoreï¼‰ï¼š{keyword_score}")
#         user_impact_score = is_multi_user(description_text)  # è¨ˆç®—ä½¿ç”¨è€…å½±éŸ¿åˆ†æ•¸
#         print(f"ğŸ‘¥ å¤šäººå½±éŸ¿åˆ†æ•¸ï¼ˆuser_impact_scoreï¼‰ï¼š{user_impact_score}")
#         escalation_score = is_escalated(close_note_text)  # è¨ˆç®—å‡ç´šè™•ç†åˆ†æ•¸
#         print(f"ğŸ“ˆ å‡ç´šè™•ç†åˆ†æ•¸ï¼ˆescalation_scoreï¼‰ï¼š{escalation_score}")



#         config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)  # å–å¾—é…ç½®é …çš„å‡ºç¾æ¬¡æ•¸
#         configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0  # è¨ˆç®—é…ç½®é …é »ç‡

#         role_comp = row.get('Role/Component', 'not filled')  # å–å¾—è§’è‰²/å…ƒä»¶
#         count = component_counts.get(role_comp, 0)  # å–å¾—è§’è‰²/å…ƒä»¶çš„å‡ºç¾æ¬¡æ•¸
#         if count >= 5:
#             role_component_freq = 3
#         elif count >= 3:
#             role_component_freq = 2
#         elif count == 2:
#             role_component_freq = 1
#         else:
#             role_component_freq = 0

#         this_time = row.get('Opened', 'not filled')  # å–å¾—é–‹å•Ÿæ™‚é–“
#         if pd.isnull(this_time):  # å¦‚æœé–‹å•Ÿæ™‚é–“ç‚ºç©º
#             time_cluster_score = 1
#         else:
#             others = df[df['Role/Component'] == role_comp]  # ç¯©é¸ç›¸åŒè§’è‰²/å…ƒä»¶çš„è³‡æ–™
#             close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
#                                   (others['Opened'] <= this_time + pd.Timedelta(hours=24))]  # æ‰¾å‡º 24 å°æ™‚å…§çš„äº‹ä»¶
#             count_cluster = len(close_events)  # è¨ˆç®—äº‹ä»¶æ•¸é‡
#             if count_cluster >= 3:
#                 time_cluster_score = 3
#             elif count_cluster == 2:
#                 time_cluster_score = 2
#             else:
#                 time_cluster_score = 1

#         severity_score = round(
#             keyword_score * weights['keyword'] +
#             user_impact_score * weights['multi_user'] +
#             escalation_score * weights['escalation'], 2
#         )

#         frequency_score = round(
#             configuration_item_freq * weights['config_item'] +
#             role_component_freq * weights['role_component'] +
#             time_cluster_score * weights['time_cluster'], 2
#         )


        
#         print(f"ğŸ“Š åš´é‡æ€§åˆ†æ•¸ï¼š{severity_score}ï¼Œé »ç‡åˆ†æ•¸ï¼š{frequency_score}")
#         print("ğŸ§  é »ç‡åˆ†æ•¸ç´°é …ï¼š")
#         print(f"ğŸ”¸ é…ç½®é …ï¼ˆConfiguration Itemï¼‰å‡ºç¾æ¯”ä¾‹ï¼š{configuration_item_freq:.2f}ï¼Œä¹˜ä»¥æ¬Šé‡å¾Œå¾— {configuration_item_freq * weights['config_item']:.2f} åˆ†")
#         print(f"ğŸ”¸ å…ƒä»¶æˆ–è§’è‰²ï¼ˆRole/Componentï¼‰åœ¨æ•´é«”ä¸­å‡ºç¾ {count} æ¬¡ â†’ çµ¦ {role_component_freq * weights['role_component']:.2f} åˆ†")
#         print(f"ğŸ”¸ åœ¨ 24 å°æ™‚å…§æœ‰ {count_cluster} ç­†åŒå…ƒä»¶äº‹ä»¶ â†’ ç¾¤èšåŠ åˆ† {time_cluster_score * weights['time_cluster']:.2f} åˆ†")
#         print(f"ğŸ“Š é »ç‡ç¸½åˆ† = {frequency_score}\n")




#         # è¨ˆç®—å½±éŸ¿åˆ†æ•¸
#         impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
#         risk_level = get_risk_level(impact_score)
#         print(f"ğŸ“‰ åš´é‡æ€§ï¼š{severity_score}, é »ç‡ï¼š{frequency_score}, ç¸½åˆ†(After KMean process)ï¼š{impact_score} â†’ åˆ†ç´šï¼š{risk_level}")
#         desc = str(row.get('Description', "")).strip()
#         short_desc = str(row.get('Short Description', "")).strip()
#         close_notes = str(row.get('Close notes', "")).strip()
#         resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

#         print(f"ğŸ“¦ Resolution åŸå§‹æ–‡å­—ï¼š{resolution_text}")  # âœ… ç¢ºèªåŸå§‹æ¬„ä½å…§å®¹

#         ai_suggestion = extract_resolution_suggestion(resolution_text)
#         print(f"ğŸ¤– GPT å»ºè­°å¥å›å‚³ï¼š{ai_suggestion}")  # âœ… ç¢ºèª GPT æ˜¯å¦æˆåŠŸå›æ‡‰

#         # âœ… å®‰å…¨åœ°å»ºç«‹ AI æ‘˜è¦è¼¸å…¥ï¼ˆè‹¥å…¨ç©ºå‰‡é¡¯ç¤ºç„¡è³‡æ–™ï¼‰
#         summary_input_text = f"{short_desc}\n{desc}".strip()
#         if not summary_input_text:
#             summary_input_text = "ï¼ˆç„¡åŸå§‹æ‘˜è¦è¼¸å…¥ï¼‰"

#         # âœ… å‘¼å« GPT æ‘˜è¦å‡½å¼
#         ai_summary = extract_problem_with_custom_prompt(summary_input_text)

#         print(f"ğŸ“¦ Resolution åŸå§‹æ–‡å­—ï¼š{resolution_text}")
#         print(f"ğŸ“ AI æ‘˜è¦è¼¸å…¥ï¼š{summary_input_text}")
#         print(f"ğŸ¤– GPT æ‘˜è¦å›å‚³ï¼š{ai_summary}")

#         # å„²å­˜åˆ†æçµæœ
#         results.append({
#             'id': safe_value(row.get('Incident') or row.get('Number')),
#             'configurationItem': safe_value(row.get('Configuration item')),
#             'roleComponent': safe_value(row.get('Role/Component')),
#             'subcategory': safe_value(row.get('Subcategory')),
#             'aiSummary': safe_value(ai_summary),
#             'originalShortDescription': safe_value(short_desc),
#             'originalDescription': safe_value(desc),
#             'severityScore': safe_value(severity_score),
#             'frequencyScore': safe_value(frequency_score),
#             'impactScore': safe_value(impact_score),
#             'severityScoreNorm': round(severity_score / 10, 2),
#             'frequencyScoreNorm': round(frequency_score / 20, 2),
#             'impactScoreNorm': round(impact_score / 30, 2),
#             'riskLevel': safe_value(get_risk_level(impact_score)),
#             'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
#             'location': safe_value(row.get('Location')),
#             'analysisTime': analysis_time,
#             'weights': {k: round(v / 10, 2) for k, v in weights.items()},
#         })

#         # solution_text = row.get('Close notes') or 'ç„¡æä¾›è§£æ³•'
#         recommended = recommend_solution(short_description_text)
#         keywords = extract_keywords(short_description_text)

#         print(f"âœ… å·²å„²å­˜ solutionï¼š{results[-1]['solution']}")
#         print(f"ğŸ’¡ å»ºè­°è§£æ³•ï¼š{recommended}")
#         print(f"ğŸ”‘ æŠ½å–é—œéµå­—ï¼š{keywords}")
#         print("â€”" * 250)  # åˆ†éš”ç·š




#     # â¬‡â¬‡â¬‡ KMeans åˆ†ç¾¤é‚è¼¯ï¼ˆæ”¯æ´ä¸‰æ¢ä»¶ï¼‰ â¬‡â¬‡â¬‡
#     all_scores = [r['impactScore'] for r in results]
#     score_range = max(all_scores) - min(all_scores)
#     score_std = np.std(all_scores)

#     print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

#     if (
#         len(all_scores) >= KMEANS_MIN_COUNT and
#         score_range >= KMEANS_MIN_RANGE and
#         score_std >= KMEANS_MIN_STDDEV
#     ):
#         kmeans = KMeans(n_clusters=4, random_state=42)
#         labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
#         centroids = kmeans.cluster_centers_.flatten()
#         set_kmeans_thresholds_from_centroids(centroids)
#         print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
#         label_map = {}
#         for i, idx in enumerate(np.argsort(centroids)[::-1]):
#             label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
#         for i, r in enumerate(results):
#             r['riskLevel'] = label_map[labels[i]]
#         print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
#     else:
#         print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
#         for r in results:
#             r['riskLevel'] = get_risk_level(r['impactScore'])
#     # â¬†â¬†â¬† åˆ†ç¾¤é‚è¼¯çµæŸ â¬†â¬†â¬†
#     print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")
#     return {
#         'data': results,
#         'analysisTime': analysis_time
#     }








