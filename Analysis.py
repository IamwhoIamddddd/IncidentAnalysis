# åŒ¯å…¥ Flask æ¡†æ¶åŠç›¸é—œæ¨¡çµ„
from flask import Flask, request, jsonify, render_template, session, send_file
from gpt_utils import extract_resolution_suggestion
from collections import defaultdict
from collections import Counter
import umap
import hdbscan
# åŒ¯å…¥æ•¸å­¸é‹ç®—æ¨¡çµ„
import math
import json
# åŒ¯å…¥ pandas ç”¨æ–¼è™•ç† Excel è³‡æ–™
import pandas as pd
# åŒ¯å…¥ os æ¨¡çµ„è™•ç†æª”æ¡ˆèˆ‡è·¯å¾‘
import os
# åŒ¯å…¥æ­£å‰‡è¡¨é”å¼æ¨¡çµ„
import re
# åŒ¯å…¥ webbrowser ç”¨æ–¼é–‹å•Ÿç¶²é 
import webbrowser
# åŒ¯å…¥ traceback ç”¨æ–¼éŒ¯èª¤è¿½è¹¤
import traceback
# åŒ¯å…¥ Werkzeug çš„å·¥å…·å‡½æ•¸ç¢ºä¿æª”æ¡ˆåç¨±å®‰å…¨
from werkzeug.utils import secure_filename
# âœ… åŒ¯å…¥èªæ„åˆ†ææ¨¡çµ„
from SmartScoring import is_high_risk, is_escalated, is_multi_user, extract_keywords, recommend_solution
# âœ… é å…ˆ encode ä¸€ç­†è³‡æ–™ä»¥åŠ é€Ÿé¦–æ¬¡è«‹æ±‚
from SmartScoring import bert_model  # ç¢ºä¿ä½ æœ‰å¾ SmartScoring è¼‰å…¥æ¨¡å‹
from SmartScoring import extract_cluster_name  # åŒ¯å…¥è‡ªå®šçš„ cluster å‘½åå‡½å¼
from tqdm import tqdm
from sentence_transformers import util
# âœ… åŒ¯å…¥é—œéµå­—æŠ½å–æ¨¡çµ„
from datetime import datetime
from sklearn.cluster import KMeans
import numpy as np

# --- åˆ†ç¾¤å•Ÿç”¨æ¢ä»¶ï¼ˆå¯ä¾è³‡æ–™èª¿æ•´ï¼‰---
KMEANS_MIN_COUNT = 4         # æœ€å°‘è³‡æ–™ç­†æ•¸
KMEANS_MIN_RANGE = 5.0       # åˆ†æ•¸æœ€å¤§æœ€å°å€¼å·®
KMEANS_MIN_STDDEV = 3.0      # æ¨™æº–å·®ä¸‹é™



print("ğŸ”¥ é ç†±èªæ„æ¨¡å‹ä¸­...")
bert_model.encode("warmup")  # é ç†±ä¸€æ¬¡ï¼Œé¿å…ç¬¬ä¸€æ¬¡ä½¿ç”¨å¤ªæ…¢
print("âœ… æ¨¡å‹å·²é ç†±å®Œæˆ")

# å»ºç«‹ Flask æ‡‰ç”¨
app = Flask(__name__)
# è¨­å®šæ‡‰ç”¨çš„å¯†é‘°ï¼Œç”¨æ–¼ session åŠ å¯†
app.secret_key = 'gwegweqgt22e'
# è¨­å®š session å„²å­˜æ–¹å¼ç‚ºæª”æ¡ˆç³»çµ±
app.config['SESSION_TYPE'] = 'filesystem'

# ------------------------------------------------------------------------------
# è¨­å®šä¸Šå‚³è³‡æ–™å¤¾èˆ‡å¤§å°é™åˆ¶ï¼ˆ10MBï¼‰
UPLOAD_FOLDER = 'uploads'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # é™åˆ¶æª”æ¡ˆå¤§å°ç‚º 10MB
ALLOWED_EXTENSIONS = {'xlsx'}  # åƒ…å…è¨±ä¸Šå‚³ xlsx æª”æ¡ˆ


basedir = os.path.abspath(os.path.dirname(__file__))  # å–å¾—ç•¶å‰ app.py çš„çµ•å°ç›®éŒ„


# ç¢ºä¿ä¸Šå‚³è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(os.path.join(basedir, 'json_data'), exist_ok=True)
os.makedirs(os.path.join(basedir, 'excel_result'), exist_ok=True)


# åˆ¤æ–·æ˜¯å¦å…è¨±çš„æª”æ¡ˆæ ¼å¼
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# ------------------------------------------------------------------------------

# å®šç¾©å‡½æ•¸ï¼šè™•ç†ç‰¹æ®Šå€¼ï¼ˆå¦‚ NaNã€None ç­‰ï¼‰
def safe_value(val):
    if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
        return 0
    elif val is None:
        return None
    elif isinstance(val, str):
        return val
    else:
        return val

# ------------------------------------------------------------------------------

# åˆ†æ Excel è³‡æ–™çš„ä¸»é‚è¼¯
def analyze_excel(filepath, weights=None):
        # é è¨­æ¬Šé‡è¨­å®šï¼ˆå¯è¢«è¦†è“‹ï¼‰
    default_weights = {
        'keyword': 5.0,
        'multi_user': 3.0,
        'escalation': 2.0,
        'config_item': 5.0,
        'role_component': 3.0,
        'time_cluster': 2.0
    }
    weights = {**default_weights, **(weights or {})}  # åˆä½µé è¨­æ¬Šé‡èˆ‡ä½¿ç”¨è€…æä¾›çš„æ¬Šé‡è¨­å®š
    print("ğŸ›ï¸ ä½¿ç”¨ä¸­çš„æ¬Šé‡è¨­å®šï¼š", weights)
    print("ğŸ” é–‹å§‹åˆ†æ Excel æª”æ¡ˆ...")
    print(f"\nğŸ“‚ è®€å– Excelï¼š{filepath}")
    df = pd.read_excel(filepath)  # è®€å– Excel æª”æ¡ˆ
    print(f"ğŸ“Š å…±è®€å– {len(df)} ç­†è³‡æ–™\n")
    component_counts = df['Role/Component'].value_counts()  # è¨ˆç®—æ¯å€‹è§’è‰²/å…ƒä»¶çš„å‡ºç¾æ¬¡æ•¸
    df['Opened'] = pd.to_datetime(df['Opened'], errors='coerce')  # å°‡ 'Opened' æ¬„ä½è½‰ç‚ºæ—¥æœŸæ ¼å¼
    results = []  # å„²å­˜åˆ†æçµæœ
    configuration_item_counts = df['Configuration item'].value_counts()  # è¨ˆç®—æ¯å€‹é…ç½®é …çš„å‡ºç¾æ¬¡æ•¸
    configuration_item_max = configuration_item_counts.max()  # æ‰¾å‡ºé…ç½®é …çš„æœ€å¤§å‡ºç¾æ¬¡æ•¸
    analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"ğŸ“… åˆ†ææ™‚é–“ï¼š{analysis_time}")

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ“Š åˆ†æé€²åº¦"):
        print(f"\nğŸ” ç¬¬ {idx + 1} ç­†åˆ†æä¸­...")
        description_text = row.get('Description', 'not filled')  # å–å¾—æè¿°æ–‡å­—
        short_description_text = row.get('Short description', 'not filled') # å–å¾—ç°¡çŸ­æè¿°æ–‡å­—
        close_note_text = row.get('Close notes', 'not filled')  # å–å¾—é—œé–‰è¨»è§£æ–‡å­—
        print(f"ğŸ“„ æè¿°ï¼š{description_text}")
        print(f"ğŸ”‘ ç°¡çŸ­æè¿°ï¼š{short_description_text}")
        print(f"ğŸ”’ é—œé–‰è¨»è§£ï¼š{close_note_text}")
        # é€™è£¡å¯ä»¥åŠ å…¥å°æè¿°æ–‡å­—çš„é è™•ç†ï¼Œä¾‹å¦‚å»é™¤å¤šé¤˜ç©ºæ ¼ã€è½‰ç‚ºå°å¯«ç­‰
        # description_text = normalize_text(description_text)  # æ¨™æº–åŒ–æ–‡å­—    


        #é€™è£¡è¦æ”¹æˆä½¿ç”¨èªæ„åˆ†ææ¨¡å‹

        keyword_score = is_high_risk(short_description_text)  # è¨ˆç®—é—œéµå­—åˆ†æ•¸
        print(f"âš ï¸ é«˜é¢¨éšªèªæ„åˆ†æ•¸ï¼ˆkeyword_scoreï¼‰ï¼š{keyword_score}")
        user_impact_score = is_multi_user(description_text)  # è¨ˆç®—ä½¿ç”¨è€…å½±éŸ¿åˆ†æ•¸
        print(f"ğŸ‘¥ å¤šäººå½±éŸ¿åˆ†æ•¸ï¼ˆuser_impact_scoreï¼‰ï¼š{user_impact_score}")
        escalation_score = is_escalated(close_note_text)  # è¨ˆç®—å‡ç´šè™•ç†åˆ†æ•¸
        print(f"ğŸ“ˆ å‡ç´šè™•ç†åˆ†æ•¸ï¼ˆescalation_scoreï¼‰ï¼š{escalation_score}")



        config_raw = configuration_item_counts.get(row.get('Configuration item'), 0)  # å–å¾—é…ç½®é …çš„å‡ºç¾æ¬¡æ•¸
        configuration_item_freq = config_raw / configuration_item_max if configuration_item_max > 0 else 0  # è¨ˆç®—é…ç½®é …é »ç‡

        role_comp = row.get('Role/Component', 'not filled')  # å–å¾—è§’è‰²/å…ƒä»¶
        count = component_counts.get(role_comp, 0)  # å–å¾—è§’è‰²/å…ƒä»¶çš„å‡ºç¾æ¬¡æ•¸
        if count >= 5:
            role_component_freq = 3
        elif count >= 3:
            role_component_freq = 2
        elif count == 2:
            role_component_freq = 1
        else:
            role_component_freq = 0

        this_time = row.get('Opened', 'not filled')  # å–å¾—é–‹å•Ÿæ™‚é–“
        if pd.isnull(this_time):  # å¦‚æœé–‹å•Ÿæ™‚é–“ç‚ºç©º
            time_cluster_score = 1
        else:
            others = df[df['Role/Component'] == role_comp]  # ç¯©é¸ç›¸åŒè§’è‰²/å…ƒä»¶çš„è³‡æ–™
            close_events = others[(others['Opened'] >= this_time - pd.Timedelta(hours=24)) &
                                  (others['Opened'] <= this_time + pd.Timedelta(hours=24))]  # æ‰¾å‡º 24 å°æ™‚å…§çš„äº‹ä»¶
            count_cluster = len(close_events)  # è¨ˆç®—äº‹ä»¶æ•¸é‡
            if count_cluster >= 3:
                time_cluster_score = 3
            elif count_cluster == 2:
                time_cluster_score = 2
            else:
                time_cluster_score = 1

        severity_score = round(
            keyword_score * weights['keyword'] +
            user_impact_score * weights['multi_user'] +
            escalation_score * weights['escalation'], 2
        )

        frequency_score = round(
            configuration_item_freq * weights['config_item'] +
            role_component_freq * weights['role_component'] +
            time_cluster_score * weights['time_cluster'], 2
        )


        
        print(f"ğŸ“Š åš´é‡æ€§åˆ†æ•¸ï¼š{severity_score}ï¼Œé »ç‡åˆ†æ•¸ï¼š{frequency_score}")
        print("ğŸ§  é »ç‡åˆ†æ•¸ç´°é …ï¼š")
        print(f"ğŸ”¸ é…ç½®é …ï¼ˆConfiguration Itemï¼‰å‡ºç¾æ¯”ä¾‹ï¼š{configuration_item_freq:.2f}ï¼Œä¹˜ä»¥æ¬Šé‡å¾Œå¾— {configuration_item_freq * weights['config_item']:.2f} åˆ†")
        print(f"ğŸ”¸ å…ƒä»¶æˆ–è§’è‰²ï¼ˆRole/Componentï¼‰åœ¨æ•´é«”ä¸­å‡ºç¾ {count} æ¬¡ â†’ çµ¦ {role_component_freq * weights['role_component']:.2f} åˆ†")
        print(f"ğŸ”¸ åœ¨ 24 å°æ™‚å…§æœ‰ {count_cluster} ç­†åŒå…ƒä»¶äº‹ä»¶ â†’ ç¾¤èšåŠ åˆ† {time_cluster_score * weights['time_cluster']:.2f} åˆ†")
        print(f"ğŸ“Š é »ç‡ç¸½åˆ† = {frequency_score}\n")




        # è¨ˆç®—å½±éŸ¿åˆ†æ•¸
        impact_score = round(math.sqrt(severity_score**2 + frequency_score**2), 2)
        risk_level = get_risk_level(impact_score)
        print(f"ğŸ“‰ åš´é‡æ€§ï¼š{severity_score}, é »ç‡ï¼š{frequency_score}, ç¸½åˆ†(After KMean process)ï¼š{impact_score} â†’ åˆ†ç´šï¼š{risk_level}")
        desc = str(row.get('Description', "")).strip()
        short_desc = str(row.get('Short Description', "")).strip()
        close_notes = str(row.get('Close notes', "")).strip()
        resolution_text = f"{desc}\n{short_desc}\n{close_notes}".strip()

        print(f"ğŸ“¦ Resolution åŸå§‹æ–‡å­—ï¼š{resolution_text}")  # âœ… ç¢ºèªåŸå§‹æ¬„ä½å…§å®¹

        ai_suggestion = extract_resolution_suggestion(resolution_text)
        print(f"ğŸ¤– GPT å»ºè­°å¥å›å‚³ï¼š{ai_suggestion}")  # âœ… ç¢ºèª GPT æ˜¯å¦æˆåŠŸå›æ‡‰


        # å„²å­˜åˆ†æçµæœ
        results.append({
            'id': safe_value(row.get('Incident') or row.get('Number')),
            'configurationItem': safe_value(row.get('Configuration item')),
            'severityScore': safe_value(severity_score),
            'frequencyScore': safe_value(frequency_score),
            'impactScore': safe_value(impact_score),
            'severityScoreNorm': round(severity_score / 10, 2),
            'frequencyScoreNorm': round(frequency_score / 20, 2),
            'impactScoreNorm': round(impact_score / 30, 2),
            'riskLevel': safe_value(get_risk_level(impact_score)),
            'solution': safe_value(ai_suggestion or 'ç„¡æä¾›è§£æ³•'),
            'location': safe_value(row.get('Location')),
            'analysisTime': analysis_time,
            'weights': {k: round(v / 10, 2) for k, v in weights.items()},
        })

        # solution_text = row.get('Close notes') or 'ç„¡æä¾›è§£æ³•'
        recommended = recommend_solution(short_description_text)
        keywords = extract_keywords(short_description_text)

        print(f"âœ… å·²å„²å­˜ solutionï¼š{results[-1]['solution']}")
        print(f"ğŸ’¡ å»ºè­°è§£æ³•ï¼š{recommended}")
        print(f"ğŸ”‘ æŠ½å–é—œéµå­—ï¼š{keywords}")
        print("â€”" * 250)  # åˆ†éš”ç·š

    # â¬‡â¬‡â¬‡ KMeans åˆ†ç¾¤é‚è¼¯ï¼ˆæ”¯æ´ä¸‰æ¢ä»¶ï¼‰ â¬‡â¬‡â¬‡
    all_scores = [r['impactScore'] for r in results]
    score_range = max(all_scores) - min(all_scores)
    score_std = np.std(all_scores)

    print(f"ğŸ“ˆ åˆ†ç¾¤åˆ¤æ–·æŒ‡æ¨™ï¼šcount={len(all_scores)}, range={score_range:.2f}, stddev={score_std:.2f}")

    if (
        len(all_scores) >= KMEANS_MIN_COUNT and
        score_range >= KMEANS_MIN_RANGE and
        score_std >= KMEANS_MIN_STDDEV
    ):
        kmeans = KMeans(n_clusters=4, random_state=42)
        labels = kmeans.fit_predict(np.array(all_scores).reshape(-1, 1))
        centroids = kmeans.cluster_centers_.flatten()
        set_kmeans_thresholds_from_centroids(centroids)
        print(f"ğŸ“Š KMeans åˆ†ç¾¤æ¨™ç±¤ï¼š{labels}")
        label_map = {}
        for i, idx in enumerate(np.argsort(centroids)[::-1]):
            label_map[idx] = ['é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'å¿½ç•¥'][i]
        for i, r in enumerate(results):
            r['riskLevel'] = label_map[labels[i]]
        print(f"ğŸ“Œ KMeans åˆ†ç¾¤ä¸­å¿ƒï¼š{sorted(centroids, reverse=True)}")
    else:
        print("âš ï¸ ä¸å•Ÿç”¨ KMeansï¼Œæ”¹ç”¨å›ºå®šé–€æª»åˆ†ç´š")
        for r in results:
            r['riskLevel'] = get_risk_level(r['impactScore'])
    # â¬†â¬†â¬† åˆ†ç¾¤é‚è¼¯çµæŸ â¬†â¬†â¬†


    #cluster åˆ†ç¾¤
    
    if len(results) >= 5:
        texts = [r['configurationItem'] + " " + str(r['solution']) + " " + str(df.loc[i, 'Close notes']) for i, r in enumerate(results)]
        embeddings = bert_model.encode(texts, convert_to_tensor=True)

        n_neighbors = min(15, len(results) - 1)
        umap_model = umap.UMAP(n_neighbors=n_neighbors, n_components=5)

        reduced = umap_model.fit_transform(embeddings)

        # å‡è¨­ len(results) æ˜¯è³‡æ–™ç­†æ•¸
        min_cluster_size = max(2, min(10, len(results) // 5))
        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)

        labels = clusterer.fit_predict(reduced)

        cluster_data = defaultdict(list)
        for i, label in enumerate(labels):
            results[i]['cluster'] = f"cluster_{label}"
            cluster_data[label].append(results[i])

        os.makedirs("cluster_excels", exist_ok=True)
        for label, entries in cluster_data.items():
            cluster_texts = [e['solution'] for e in entries if e['solution']]
            cluster_name = extract_cluster_name(cluster_texts)
            filename = f"cluster_{label}_{cluster_name}.xlsx"
            cluster_df = pd.DataFrame(entries)
            cluster_df.to_excel(f"cluster_excels/{filename}", index=False)
            print(f"ğŸ“ å·²è¼¸å‡ºï¼šcluster_excels/{filename}")

        for label, entries in cluster_data.items():
            high_count = sum(1 for e in entries if e['riskLevel'] == 'é«˜é¢¨éšª')
            total = len(entries)
            ratio = high_count / total if total > 0 else 0
            if ratio >= 0.5:
                print(f"ğŸš¨ é è­¦ï¼šCluster {label} æœ‰ {ratio:.0%} é«˜é¢¨éšªäº‹ä»¶ï¼ˆ{high_count}/{total}ï¼‰")
    else:
        print("âš ï¸ è³‡æ–™ä¸è¶³ï¼Œç•¥éèªæ„ç¾¤èšåˆ†æï¼ˆå°‘æ–¼ 5 ç­†ï¼‰")



    print("\nâœ… æ‰€æœ‰è³‡æ–™åˆ†æå®Œæˆï¼")


    return {
        'data': results,
        'analysisTime': analysis_time
    }



# æ ¹æ“šåˆ†æ•¸åˆ¤æ–·é¢¨éšªç­‰ç´šï¼ˆæ”¯æ´ KMeans åˆ†ç¾¤ï¼‰
kmeans_thresholds = None  # å…¨åŸŸè®Šæ•¸ï¼Œå­˜å„² KMeans åˆ†ç¾¤é–€æª»


def get_risk_level(score):
    global kmeans_thresholds
    level = ''

    if kmeans_thresholds and len(kmeans_thresholds) == 4:
        thresholds = sorted(kmeans_thresholds)
        if score >= thresholds[3]:
            level = 'é«˜é¢¨éšª'
        elif score >= thresholds[2]:
            level = 'ä¸­é¢¨éšª'
        elif score >= thresholds[1]:
            level = 'ä½é¢¨éšª'
        else:
            level = 'å¿½ç•¥'
        print(f"ğŸ“Š KMeansï¼šimpactScore: {score} â†’ åˆ†ç´šï¼š{level}ï¼ˆä½¿ç”¨å‹•æ…‹é–€æª»ï¼‰")
    else:
        if score >= 18:
            level = 'é«˜é¢¨éšª'
        elif score >= 12:
            level = 'ä¸­é¢¨éšª'
        elif score >= 6:
            level = 'ä½é¢¨éšª'
        else:
            level = 'å¿½ç•¥'
        print(f"ğŸ“Š å›ºå®šé–€æª»ï¼šimpactScore: {score} â†’ åˆ†ç´šï¼š{level}")

    return level

# åœ¨åˆ†æå®Œæˆå¾Œè‡ªå‹•è¨­å®š kmeans_thresholds
# ï¼ˆè«‹æ”¾åœ¨ KMeans åˆ†ç¾¤å®Œæˆå¾Œï¼‰
def set_kmeans_thresholds_from_centroids(centroids):
    global kmeans_thresholds
    kmeans_thresholds = sorted(centroids)
    print(f"âœ… å·²è¨­å®š KMeans åˆ†ç¾¤é–€æª»ï¼ˆsortedï¼‰ï¼š{kmeans_thresholds}")


# ------------------------------------------------------------------------------

# å®šç¾©é¦–é è·¯ç”±
@app.route('/')
def index():
    return render_template('FrontEnd.html')  # æ¸²æŸ“é¦–é æ¨¡æ¿

# å®šç¾©çµæœé é¢è·¯ç”±
@app.route('/result')
def result_page():
    data = session.get('analysis_data', [])  # å¾ session å–å¾—åˆ†æçµæœ
    return render_template('result.html', data=data)  # æ¸²æŸ“çµæœé é¢

# å®šç¾©æ­·å²ç´€éŒ„é é¢è·¯ç”±
@app.route('/history')
def history_page():
    return render_template('history.html')  # æ¸²æŸ“æ­·å²ç´€éŒ„é é¢

# ------------------------------------------------------------------------------




@app.route('/ping')
def ping():
    return "pong", 200


# å®šç¾©æª”æ¡ˆä¸Šå‚³è·¯ç”±
@app.route('/upload', methods=['POST'])
def upload_file():
    print("ğŸ“¥ æ”¶åˆ°ä¸Šå‚³è«‹æ±‚")  # ç´€éŒ„è«‹æ±‚



    if 'file' not in request.files:  # æª¢æŸ¥æ˜¯å¦æœ‰æª”æ¡ˆæ¬„ä½
        print("âŒ æ²’æœ‰ file æ¬„ä½")
        return jsonify({'error': 'æ²’æœ‰æ‰¾åˆ°æª”æ¡ˆæ¬„ä½'}), 400

    file = request.files['file']  # å–å¾—æª”æ¡ˆ
    if file.filename == '':  # æª¢æŸ¥æª”æ¡ˆåç¨±æ˜¯å¦ç‚ºç©º
        print("âš ï¸ æª”æ¡ˆåç¨±ç‚ºç©º")
        return jsonify({'error': 'æœªé¸æ“‡æª”æ¡ˆ'}), 400

    if not allowed_file(file.filename):  # æª¢æŸ¥æª”æ¡ˆæ ¼å¼æ˜¯å¦å…è¨±
        print("âš ï¸ æª”æ¡ˆé¡å‹ä¸ç¬¦")
        return jsonify({'error': 'è«‹ä¸Šå‚³ .xlsx æª”æ¡ˆ'}), 400
        
    # æ¥æ”¶è‡ªè¨‚æ¬Šé‡
    weights_raw = request.form.get('weights')
    if not weights_raw:
        print("â„¹ï¸ æœªæä¾›è‡ªè¨‚æ¬Šé‡ï¼Œä½¿ç”¨é è¨­å€¼åˆ†æ")

    weights = None
    if weights_raw:
        try:
            weights = json.loads(weights_raw)
            print("ğŸ“¥ æ”¶åˆ°æ¬Šé‡è¨­å®šï¼š", weights)
        except Exception as e:
            print(f"âš ï¸ æ¬Šé‡è§£æå¤±æ•—ï¼š{e}")
            return jsonify({'error': 'æ¬Šé‡è§£æå¤±æ•—'}), 400
        
    # ç”¢ç”Ÿæ™‚é–“æˆ³è¨˜èˆ‡æª”å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    uid = f"result_{timestamp}" # ä¾‹å¦‚ result_20250423_152301 åˆ†æçµæœæª”åç¨±
    original_filename = f"original_{timestamp}.xlsx" # ä¾‹å¦‚ original_20250423_152301.xlsx åŸå§‹é»¨åç¨±
    original_path = os.path.join('uploads', original_filename)

    try:
        file.save(original_path)  # å„²å­˜åŸå§‹æª”æ¡ˆ
        print(f" åŸå§‹æª”å·²å„²å­˜ï¼š{original_path}")
    except Exception as e:
        return jsonify({'error': f'å„²å­˜åŸå§‹æª”å¤±æ•—ï¼š{str(e)}'}), 500

    try:
        analysis_result = analyze_excel(original_path, weights=weights)
        results = analysis_result['data']  # å–å¾—åˆ†æçµæœ


        save_analysis_files(analysis_result, uid)  # å„²å­˜åˆ†æçµæœæª”æ¡ˆ

        print(f"âœ… åˆ†æå®Œæˆï¼Œå…± {len(results)} ç­†")
        session['analysis_data'] = results  # å„²å­˜åˆ†æçµæœåˆ° session
        return jsonify({'data': results, 'uid': uid, 'weights': weights}), 200


    


    except Exception as e:
        print(f"âŒ åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        traceback.print_exc()  # å°å‡ºå®Œæ•´éŒ¯èª¤å †ç–Š
        return jsonify({'error': str(e)}), 500
    





def save_analysis_files(result, uid):
    os.makedirs('json_data', exist_ok=True)
    os.makedirs('excel_result', exist_ok=True)
    # å„²å­˜ JSON
    json_path = os.path.join(basedir, 'json_data', f"{uid}.json")
    print(f"ğŸ“ é è¨ˆå„²å­˜ JSONï¼š{json_path}")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print("âœ… JSON æª”æ¡ˆå·²å¯«å…¥æˆåŠŸ")

    # å„²å­˜åˆ†æå ±è¡¨ Excelï¼ˆåªå„²å­˜ result['data']ï¼‰
    df = pd.DataFrame(result['data'])
    excel_path = os.path.join(basedir, 'excel_result', f"{uid}.xlsx")
    df.to_excel(excel_path, index=False)

    # ç¢ºèª JSON æª”æ¡ˆæ˜¯å¦å¯«å…¥æˆåŠŸ
    if os.path.exists(json_path):
        print("âœ… JSON æª”æ¡ˆå·²æˆåŠŸå„²å­˜")
    else:
        print("âŒ JSON æª”æ¡ˆå„²å­˜å¤±æ•—ï¼")

    print(f"âœ… åˆ†æå ±è¡¨å·²å„²å­˜ï¼š{excel_path}")
    print("ğŸ“ JSON çµ•å°è·¯å¾‘ï¼š", os.path.abspath(json_path))
    print("ğŸ“ Excel çµ•å°è·¯å¾‘ï¼š", os.path.abspath(excel_path))
    print("ğŸ“ åŸå§‹æª”çµ•å°è·¯å¾‘ï¼š", os.path.abspath(os.path.join(basedir, 'uploads', f"original_{uid}.xlsx")))


@app.route('/get-results')
def get_results():
    folder = 'json_data'
    results = []
    first_weights = {}

    if not os.path.exists(folder):
        return jsonify({'error': f'è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼š{folder}'}), 404

    # ğŸ”„ è®€å–æ‰€æœ‰æª”æ¡ˆï¼Œæ‰¾å‡ºæœ€æ–°çš„é‚£ä»½åˆ†ææª”
    sorted_files = sorted(
        [f for f in os.listdir(folder) if f.endswith('.json')],
        reverse=True  # æœ€å¾Œé¢æœ€æ–°
    )

    for filename in sorted_files:
        filepath = os.path.join(folder, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = json.load(f)
                if isinstance(content, dict) and 'data' in content:
                    results.extend(content['data'])
                    if not first_weights and 'weights' in content:
                        first_weights = content['weights']
                elif isinstance(content, list):
                    results.extend(content)
        except Exception as e:
            print(f"âŒ éŒ¯èª¤è®€å– {filename}ï¼š{e}")

    return jsonify({
        'data': results,
        'weights': first_weights  # âœ… ç¢ºä¿å‚³å‡ºé€™å€‹æ¬„ä½
    })






# âœ… JSON é è¦½è·¯ç”±ï¼šæä¾› `/get-json?file=xxxx.json`
@app.route('/get-json', methods=['GET'])
def get_json_file():
    filename = request.args.get('file')  # e.g., result_20250423_152301.json
    if not filename:
        return jsonify({'error': 'ç¼ºå°‘ file åƒæ•¸'}), 400

    json_path = os.path.join('json_data', filename)
    if os.path.exists(json_path):
        return send_file(json_path, as_attachment=False)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„ JSON æª”æ¡ˆ'}), 404


# âœ… åˆ†æ Excel ä¸‹è¼‰è·¯ç”±ï¼šæä¾› `/download-excel?uid=xxxx`
@app.route('/download-excel', methods=['GET'])
def download_excel_file():
    uid = request.args.get('uid')  # e.g., result_20250423_152301
    if not uid:
        return jsonify({'error': 'ç¼ºå°‘ uid åƒæ•¸'}), 400

    excel_path = os.path.join('excel_result', f"{uid}.xlsx")
    if os.path.exists(excel_path):
        return send_file(excel_path, as_attachment=True)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„ Excel æª”æ¡ˆ'}), 404

@app.route('/download-original', methods=['GET'])
def download_original_excel():
    uid = request.args.get('uid')  # uid = result_20250423_152301
    if not uid:
        return jsonify({'error': 'ç¼ºå°‘ uid åƒæ•¸'}), 400

    # å–å‡ºå°æ‡‰çš„æ™‚é–“æˆ³
    timestamp = uid.replace('result_', '')
    original_filename = f'original_{timestamp}.xlsx'
    original_path = os.path.join('uploads', original_filename)

    if os.path.exists(original_path):
        return send_file(original_path, as_attachment=True)
    else:
        return jsonify({'error': 'æ‰¾ä¸åˆ°å°æ‡‰çš„åŸå§‹æª”æ¡ˆ'}), 404


# ------------------------------------------------------------------------------

# å®šç¾©æª”æ¡ˆåˆ—è¡¨è·¯ç”±
@app.route('/files', methods=['GET'])
def get_file_list():
    files = os.listdir(UPLOAD_FOLDER)  # åˆ—å‡ºä¸Šå‚³è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ
    return jsonify({'files': files}), 200

# ------------------------------------------------------------------------------

# å®šç¾©åŸ·è¡Œå‹•ä½œçš„è·¯ç”±
@app.route('/perform-action', methods=['POST'])
def perform_action():
    data = request.json  # å–å¾— JSON è³‡æ–™
    action = data.get('action')  # å–å¾—å‹•ä½œåç¨±

    if action == 'start':  # å¦‚æœå‹•ä½œæ˜¯ 'start'
        result = "Server received 'start' action and performed the task."
        print(result)
        return jsonify({'status': 'success', 'message': result}), 200
    else:  # å¦‚æœæ˜¯æœªçŸ¥å‹•ä½œ
        result = f"Server received unknown action: {action}"
        print(result)
        return jsonify({'status': 'error', 'message': 'Unknown action!'}), 400

# ------------------------------------------------------------------------------

# å•Ÿå‹• Flask æ‡‰ç”¨
if __name__ == '__main__':
    app.run(debug=True, use_reloader=True)