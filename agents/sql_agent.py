# sql_agent.py
import subprocess
import re
import sqlite3
import pandas as pd



class SQLAgent:
    def __init__(self, model="deepseek-coder-v2:latest", db_path="resultDB.db"):
        self.model = model
        self.db_path = db_path
        print(f"ğŸ”§ [SQLAgent] åˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡å‹ï¼š{self.model}ï¼Œè³‡æ–™åº«è·¯å¾‘ï¼š{self.db_path}")
    
    def set_model(self, model_name):
        self.model = model_name
        print(f"âœ… [SQLAgent] æ¨¡å‹å·²æ›´æ–°ç‚ºï¼š{self.model}")
        
        
        
        
        # æ‹†è§£å•é¡Œä¸¦åˆ†æˆå…©éƒ¨åˆ†ï¼Œä¸¦ä½¿ç”¨ LLM è™•ç†
    def _split_user_question(self, message):
        # LLM ç”¨ä¾†æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œæ‹†è§£ç‚ºå…©å€‹éƒ¨åˆ†
        prompt = (
            "You are an expert assistant. Based on the user's question, split the task into two parts:\n\n"
            "1. **SQL Query Prompt**:\n"
            "- Generate a prompt for another LLM to create a SQL query.\n"
            "- Use aggregation functions (e.g., COUNT, GROUP BY) to summarize data based on broad categories.\n"
            "- Do not include any filtering conditions (e.g., WHERE clauses), unless explicitly requested by the user.\n\n"
            "2. **Analysis Prompt**:\n"
            "- Create a prompt for another LLM to analyze the SQL query results.\n"
            "- Describe the user's intent (e.g., trends, insights, summarization).\n"
            "- Provide instructions on interpreting the results and deriving insights.\n"
            "- Suggest trends, anomalies, or patterns if applicable.\n\n"
            "Return both prompts separately, labeled as 'SQL Query Prompt' and 'Analysis Prompt'."
        )


        # åœ¨æç¤ºèªå¥ä¸­åŠ å…¥ä½¿ç”¨è€…å•é¡Œ
        print("ğŸ“ [SQLAgent] æ§‹é€ æ‹†è§£å•é¡Œçš„æç¤ºèªå¥...")
        prompt = f"{prompt}\n\nUser Question: {message}\n\n"

        # å‘¼å« LLM ä¾†æ‹†è§£å•é¡Œä¸¦ç”Ÿæˆå…©å€‹éƒ¨åˆ†
        try:
            print("ğŸš€ ç™¼é€æ‹†è§£å•é¡Œçš„æç¤ºåˆ° LLM...")
            result = subprocess.run(
                ["ollama", "run", self.model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )
            if result.returncode == 0:
                response = result.stdout.decode("utf-8").strip()
                print("âœ… LLM å›æ‡‰ï¼š", response)
                # å‡è¨­ LLM æœƒè¿”å›å…©å€‹éƒ¨åˆ†ï¼Œåˆ†åˆ¥æ˜¯ SQL æŸ¥è©¢æç¤ºèˆ‡åˆ†ææç¤º
                sql_query_prompt = ""
                analysis_prompt = ""
                
                # æŠ½å– SQL æŸ¥è©¢æç¤ºèˆ‡åˆ†ææç¤º
                if "SQL Query Prompt" in response and "Analysis Prompt" in response:
                    sql_query_prompt = response.split("SQL Query Prompt")[1].split("Analysis Prompt")[0].strip()
                    analysis_prompt = response.split("Analysis Prompt")[1].strip()

                return sql_query_prompt, analysis_prompt
            else:
                print("âŒ LLM éŒ¯èª¤ï¼š", result.stderr.decode("utf-8"))
                return None, None
        except Exception as e:
            print(f"âŒ å‘¼å« LLM ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            return None, None

    # SQL æŸ¥è©¢ç”Ÿæˆèˆ‡åŸ·è¡Œ 
    def _build_prompt(self, user_question):
        schema_info = """
        You are an expert data analyst.

        You are working with the following SQLite table named 'metadata':

        Columns:
        - id (integer): internal ID
        - text (text): full case description, includes issue and solution
        - subcategory (text): issue type, such as 'Login', 'Teams', etc.
        - configurationItem (text): module or system component
        - roleComponent (text): affected user role or feature
        - location (text): site or region where issue occurred
        - analysisTime (text): ISO timestamp when the issue was recorded
        - solution (text): solution text or resolution for the case

        Please write an SQL query (SELECT ...) that provides aggregated information based on the columns. 
        Do not include any filtering conditions or WHERE clauses. The query should focus on summarizing data across broad categories.
        Return only the SQL query, no explanation or formatting.
        """

        
        print("ğŸ“ [SQLAgent] æ§‹é€  SQL æç¤ºèªå¥...")
        prompt = f"{schema_info}\n\nUser question: {user_question}\nSQL:"
        return prompt
    

    # ç”¢ç”Ÿ SQL æŸ¥è©¢èªå¥
    def _generate_sql(self, prompt):
        print("ğŸ§  [SQLAgent] å˜—è©¦ä½¿ç”¨ LLM ç”¢ç”Ÿ SQL æŸ¥è©¢èªå¥...")
        print(f"ğŸ“ Prompt è¼¸å…¥ï¼š{prompt}")
        print("ä½¿ç”¨æ¨¡å‹ï¼š", self.model)
        if not prompt.strip():
            print("âš ï¸ æç¤ºèªå¥ç‚ºç©ºï¼Œç„¡æ³•ç”¢ç”Ÿ SQL")
            return None
        try:
            print("ğŸš€ å‘¼å«æ¨¡å‹ç”¢ç”Ÿ SQL ä¸­...")
            result = subprocess.run(
                ["ollama", "run", self.model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )

            if result.returncode != 0:
                err = result.stderr.decode("utf-8")
                print("âŒ æ¨¡å‹éŒ¯èª¤ï¼š", err)
                return None

            output = result.stdout.decode("utf-8").strip()
            print("ğŸ“¥ æ¨¡å‹ç”¢å‡ºï¼ˆå‰ 200 å­—ï¼‰ï¼š", output[:200])
            return output

        except Exception as e:
            print(f"âŒ å‘¼å« LLM å¤±æ•—ï¼š{str(e)}")
            return None
        
    # æŠ½å– SQL æŒ‡ä»¤
    def _extract_sql(self, text):
        print("ğŸ” [SQLAgent] å˜—è©¦å¾æ¨¡å‹å›æ‡‰ä¸­èƒå– SQL æŒ‡ä»¤...")

        # å˜—è©¦æŠ“ ```sql å€å¡Š
        code_block_match = re.search(r"```sql\s*(.*?)```", text, re.DOTALL | re.IGNORECASE)
        if code_block_match:
            sql_code = code_block_match.group(1).strip()
            print("âœ… åµæ¸¬åˆ° ```sql å€å¡Šï¼ŒæˆåŠŸæŠ½å– SQLã€‚")
            return sql_code

        # å¦å‰‡æŠ“ç¬¬ä¸€æ®µ SELECT èªå¥
        select_match = re.search(r"(SELECT\s.+?;)", text, re.IGNORECASE | re.DOTALL)
        if select_match:
            sql_code = select_match.group(1).strip()
            print("âœ… æˆåŠŸæŠ½å– SELECT é–‹é ­çš„ SQLã€‚")
            return sql_code

        # fallbackï¼šæŠ“åŒ…å« FROM çš„æ®µè½
        fallback_match = re.search(r"(SELECT.+FROM.+?)(\n|$)", text, re.IGNORECASE | re.DOTALL)
        if fallback_match:
            sql_code = fallback_match.group(1).strip()
            print("âš ï¸ å¾ fallback æŠ½å– SQL æˆåŠŸï¼ˆä½†å¯èƒ½ä¸å®Œæ•´ï¼‰ã€‚")
            return sql_code

        print("âš ï¸ ç„¡æ³•æŠ½å– SQLï¼ŒåŸå§‹è¼¸å‡ºå¦‚ä¸‹ï¼š")
        print(text[:300])
        return None
    


    
    # åŸ·è¡Œ SQL æŸ¥è©¢
    def _run_sql(self, query):
        try:
            print("ğŸ” [SQLAgent] æ­£åœ¨é€£ç·šåˆ° SQLite è³‡æ–™åº«...")
            conn = sqlite3.connect(self.db_path)
            print("ğŸ” [SQLAgent] æ­£åœ¨æŸ¥è©¢...")
            df = pd.read_sql_query(query, conn)
            conn.close()
            print(f"âœ… æŸ¥è©¢æˆåŠŸï¼Œå…± {len(df)} ç­†çµæœã€‚")
            return df
        except Exception as e:
            print("âŒ æŸ¥è©¢å¤±æ•—ï¼š", e)
            return None
        
    
    # ---------- äººé¡æ‘˜è¦ ----------
    def _summarize_sql(self, df, max_rows=5):
        if df.empty:
            return "ğŸ“­ No data found."

        print("ğŸ“ [SQLAgent] æ­£åœ¨ç”Ÿæˆ SQL çµæœæ‘˜è¦...")
        print(f"ğŸ“Š è³‡æ–™ç­†æ•¸ï¼š{len(df)}")

        preview_df = df.head(max_rows).copy()
        for col in preview_df.columns:
            if preview_df[col].dtype == "object":
                preview_df[col] = (
                    preview_df[col]
                    .astype(str)
                    .str.replace(r'\\n', '\n')
                    .str.slice(0, 200)
                )

        summary = f"ğŸ“Š Query successful. Total {len(df)} records found.<br>"
        summary += f"ğŸ“‹ Preview of first {min(max_rows, len(df))} records:<br>"
        preview = preview_df.to_string(index=False)

        print("DEBUG preview====>")
        print(repr(preview))
        print("<====DEBUG preview")

        return summary + "```\n" + preview + "\n```"
    

    # ----------- LLM å‹•æ…‹ chunk size è¨ˆç®— -----------
    # ä¼°ç®—æ¯è¡Œè³‡æ–™çš„ token æ•¸é‡
    # é€™è£¡å‡è¨­æ¯å€‹ token ç´„ç­‰æ–¼ 4 å€‹å­—å…ƒï¼ˆé€™æ˜¯ OpenAI çš„ä¼°ç®—ï¼‰
    # é€™å€‹å‡½æ•¸æœƒè¨ˆç®— DataFrame çš„å¹³å‡è¡Œé•·åº¦ï¼Œä¸¦ä¼°ç®—æ¯è¡Œçš„ token æ•¸é‡
    # é€™å€‹ä¼°ç®—æ˜¯ç²—ç•¥çš„ï¼Œå¯¦éš›æƒ…æ³å¯èƒ½æœƒæœ‰æ‰€ä¸åŒï¼Œä½†å¯ä»¥ä½œç‚ºä¸€å€‹èµ·é»

    def _estimate_tokens_per_row(self, df):
        csv_text = df.to_csv(index=False)
        avg_len = len(csv_text) / len(df) if len(df) > 0 else 1
        return int(avg_len / 4)
    
    # æ ¹æ“šæ¨¡å‹åç¨±å’Œä¿ç•™çš„æç¤º token æ•¸é‡è¨ˆç®—å‹•æ…‹ chunk size
    # é€™å€‹å‡½æ•¸æœƒæ ¹æ“šæ¨¡å‹çš„ token é™åˆ¶å’Œä¿ç•™çš„æç¤º token æ•¸é‡è¨ˆç®—å‡ºåˆé©çš„ chunk size
    # é€™æ¨£å¯ä»¥ç¢ºä¿åœ¨è™•ç†å¤§å‹ DataFrame æ™‚ä¸æœƒè¶…éæ¨¡å‹çš„ token é™åˆ¶
    # é€™å€‹å‡½æ•¸æœƒè€ƒæ…®æ¨¡å‹çš„ token é™åˆ¶ã€ä¿ç•™çš„æç¤º token æ•¸é‡ä»¥åŠæ¯è¡Œè³‡æ–™çš„å¹³å‡ token æ•¸é‡
    # æœ€å¾Œè¿”å›ä¸€å€‹åˆé©çš„ chunk sizeï¼Œç¢ºä¿ä¸æœƒè¶…éæ¨¡å‹çš„é™åˆ¶
    # å¦‚æœè¨ˆç®—å‡ºçš„ chunk size å¤§æ–¼ DataFrame çš„è¡Œæ•¸ï¼Œå‰‡è¿”å› DataFrame çš„è¡Œæ•¸
    # é€™æ¨£å¯ä»¥ç¢ºä¿åœ¨è™•ç†å°å‹ DataFrame æ™‚ä¸æœƒå‡ºç¾éŒ¯èª¤
    # æ ¹æ“šæ¨¡å‹å’Œè³‡æ–™é‡å‹•æ…‹è¨ˆç®— chunk_size
    def _calculate_dynamic_chunk_size(self, df, prompt_reserve_tokens=500):
        model_token_limits = {
            "phi4-mini": 4096,
            "phi3:mini": 4096,
            "mistral": 8192,
            "orca2": 8192,
            "orca2-13b": 8192,
            "deepseek-coder:latest": 16384,
            "deepseek-coder-v2:latest": 16384,
        }
        max_tokens = model_token_limits.get(self.model, 4096)
        tokens_per_row = self._estimate_tokens_per_row(df)
        available_tokens = max_tokens - prompt_reserve_tokens
        # ğŸ›¡ï¸ ç‚ºäº†è¼¸å‡ºå“è³ªï¼Œå†æ¸› 10 ç­†
        chunk_size = max(5, int(available_tokens / tokens_per_row) - 10)
        return min(chunk_size, len(df))
    
    # å°‡å¤šå€‹æ‘˜è¦åˆ†çµ„ä¸¦åˆä½µï¼Œç¢ºä¿ä¸è¶…é token é™åˆ¶
    # é€™å€‹å‡½æ•¸ç”¨ä¾†å°‡å¤šå€‹æ‘˜è¦åˆ†çµ„ä¸¦åˆä½µæˆæ›´å¤§çš„æ‘˜è¦
    # å®ƒæœƒå°‡æ‘˜è¦åˆ†æˆå¤šå€‹çµ„ï¼Œæ¯çµ„çš„ token æ•¸é‡ä¸è¶…éå¯ç”¨çš„ token é™åˆ¶
    # ç„¶å¾Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ä¾†åˆä½µæ¯çµ„æ‘˜è¦
    # å¦‚æœåˆä½µå¾Œçš„æ‘˜è¦ä»ç„¶è¶…é token é™åˆ¶ï¼Œå‰‡æœƒéè¿´åœ°å†æ¬¡åˆä½µ
    # æœ€å¾Œè¿”å›åˆä½µå¾Œçš„æ‘˜è¦
    # å¦‚æœåªæœ‰ä¸€å€‹åˆä½µå¾Œçš„æ‘˜è¦ï¼Œå‰‡ç›´æ¥è¿”å›è©²æ‘˜è¦
    # å¦‚æœåˆä½µå¤±æ•—ï¼Œå‰‡æœƒå˜—è©¦ä½¿ç”¨å‚™ç”¨æ¨¡å‹é€²è¡Œåˆä½µ
    # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—ï¼Œå‰‡è¿”å›ä¸€å€‹éŒ¯èª¤è¨Šæ¯
    # ğŸ“Œ å›ºå®šä½¿ç”¨ 8192 token é™åˆ¶åˆ‡ promptï¼Œå³ä½¿ fallback æ¨¡å‹ä¸Šé™æ›´å°ï¼ˆå¦‚ phi3ï¼‰
    def _split_and_merge_summaries(self, summaries, token_limit=8192, prompt_reserve=500, is_top_level=True):
        def estimate_token_count(text):
            return len(text) // 4

        fallback_models = ["orca2:13b", "nous-hermes2:10.7b", "phi3:mini"]
        available_tokens = token_limit - prompt_reserve
        grouped = []
        group = []
        token_count = 0

        for s in summaries:
            s_tokens = estimate_token_count(s)
            if token_count + s_tokens > available_tokens:
                grouped.append(group)
                group = [s]
                token_count = s_tokens
            else:
                group.append(s)
                token_count += s_tokens
        if group:
            grouped.append(group)

        def run_with_model(m, prompt):
            try:
                result = subprocess.run(
                    ["ollama", "run", m],
                    input=prompt.encode("utf-8"),
                    capture_output=True,
                    timeout=300
                )
                if result.returncode == 0:
                    return result.stdout.decode("utf-8").strip()
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {m} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None

        merged_chunks = []
        print(f"ğŸ” å…±åˆ†æˆ {len(grouped)} çµ„æ‘˜è¦ï¼Œæ¯çµ„å¹³å‡ {token_count / len(grouped):.2f} tokens")
        print("ğŸ§  é–‹å§‹åˆä½µæ‘˜è¦...")
        for i, group in enumerate(grouped, 1):
            merge_prompt = f"You are a data analyst. Please summarize the key points from the following group {i} of summaries:\n\n"
            for idx, s in enumerate(group, 1):
                merge_prompt += f"ï¼ˆæ‘˜è¦ {idx}ï¼‰{s}\n\n"
            merge_prompt += "Please consolidate the main observations:"

            print(f"ğŸ§  å˜—è©¦ä½¿ç”¨ä¸»æ¨¡å‹ {self.model} é€²è¡Œåˆä½µæ‘˜è¦...")
            print(f"ğŸ“¥ Prompt Previewï¼š{merge_prompt[:300]}{'...' if len(merge_prompt) > 300 else ''}")
            reply = run_with_model(self.model, merge_prompt)
            print(f"ğŸ“¥ å›è¦† Previewï¼š{reply[:300]}{'...' if reply and len(reply) > 300 else ''}")

            for fallback in fallback_models:
                if reply:
                    break
                print(f"ğŸ” ä½¿ç”¨ fallback æ¨¡å‹ï¼š{fallback}")
                reply = run_with_model(fallback, merge_prompt)

            merged_chunks.append(reply if reply else "âŒ æœ¬æ®µæ‘˜è¦å¤±æ•—")

        if len(merged_chunks) == 1:
            result = merged_chunks[0]
            if is_top_level:
                return f"ğŸ“Š GPT æ•´åˆæ‘˜è¦å¦‚ä¸‹ï¼š\n{result}"
            else:
                return result
        else:
            return self._split_and_merge_summaries(merged_chunks, token_limit, prompt_reserve, is_top_level=False)





    # é€™å€‹å‡½æ•¸ç”¨ä¾†ä½¿ç”¨ LLM å° SQL æŸ¥è©¢çµæœé€²è¡Œæ‘˜è¦
    # å®ƒæœƒå°‡æŸ¥è©¢çµæœåˆ†æˆå¤šå€‹ chunkï¼Œç„¶å¾Œå°æ¯å€‹ chunk ä½¿ç”¨ LLM é€²è¡Œæ‘˜è¦
    # æœ€å¾Œå°‡æ‰€æœ‰ chunk çš„æ‘˜è¦åˆä½µæˆä¸€å€‹æœ€çµ‚çš„æ‘˜è¦
    # å¦‚æœæŸ¥è©¢çµæœç‚ºç©ºï¼Œå‰‡è¿”å›ä¸€å€‹æç¤ºè¨Šæ¯
    # å¦‚æœ LLM å‘¼å«å¤±æ•—ï¼Œå‰‡æœƒå˜—è©¦ä½¿ç”¨å‚™ç”¨æ¨¡å‹é€²è¡Œæ‘˜è¦
    # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—ï¼Œå‰‡è¿”å›ä¸€å€‹éŒ¯èª¤è¨Šæ¯
    # é€™å€‹å‡½æ•¸æœƒæ ¹æ“šæ¨¡å‹çš„ token é™åˆ¶å’Œä¿ç•™çš„æç¤º token æ•¸é‡è¨ˆç®—å‡ºåˆé©çš„ chunk size
    # é€™æ¨£å¯ä»¥ç¢ºä¿åœ¨è™•ç†å¤§å‹ DataFrame æ™‚ä¸æœƒè¶…éæ¨¡å‹çš„ token é™åˆ¶
    # å®ƒé‚„æœƒè€ƒæ…®æ¯è¡Œè³‡æ–™çš„å¹³å‡ token æ•¸é‡ï¼Œä¸¦æ ¹æ“šé€™äº›è³‡è¨Šè¨ˆç®—å‡ºåˆé©çš„ chunk size
    # æœ€å¾Œè¿”å›ä¸€å€‹åˆé©çš„ chunk sizeï¼Œç¢ºä¿ä¸æœƒè¶…éæ¨¡å‹çš„é™åˆ¶
    # ä½¿ç”¨ LLM æ‘˜è¦ SQL çµæœ
    def _summarize_sql_with_llm(self, df):
        print("ğŸ§  [SQLAgent] å˜—è©¦ä½¿ç”¨ LLM é€²è¡Œ SQL çµæœæ‘˜è¦...")
        print(f"ğŸ§  ä½¿ç”¨æ¨¡å‹ï¼š{self.model}")
        print(f"ğŸ“ è³‡æ–™ç­†æ•¸ï¼š{len(df)}")

        if df.empty:
            return "ğŸ“­ æŸ¥ç„¡è³‡æ–™çµæœã€‚"

        chunk_size = self._calculate_dynamic_chunk_size(df)
        print(f"ğŸ“ é ä¼° chunk_size = {chunk_size} ç­†ï¼ˆæ¨¡å‹ï¼š{self.model}ï¼‰")

        chunk_summaries = []

        for i in range(0, len(df), chunk_size):
            chunk = df.iloc[i:i+chunk_size]
            sample_csv = chunk.to_csv(index=False)
            prompt = (
                f"You are a data analyst. The following is data chunk {i//chunk_size+1}. "
                f"Please summarize its characteristics and trends:\n\n{sample_csv}\n\nSummary:"
            )

            try:
                result = subprocess.run(
                    ["ollama", "run", self.model],
                    input=prompt.encode("utf-8"),
                    capture_output=True,
                    timeout=600
                )

                if result.returncode == 0:
                    reply = result.stdout.decode("utf-8").strip()
                    chunk_summaries.append(reply)
                    print(f"âœ… ç¬¬ {i//chunk_size+1} æ®µå®Œæˆæ‘˜è¦")
                else:
                    print(f"âš ï¸ ç¬¬ {i//chunk_size+1} æ®µæ‘˜è¦å¤±æ•—ï¼Œè·³é")

            except Exception as e:
                print(f"âŒ ç¬¬ {i//chunk_size+1} æ®µå‘¼å« LLM å¤±æ•—ï¼š{e}")

        if not chunk_summaries:
            return self._summarize_sql(df)

        print("ğŸ§  é–‹å§‹æ•´åˆæ‰€æœ‰æ®µè½æ‘˜è¦...")
        
        # âœ… ä½¿ç”¨æ•´åˆç‰ˆæ‘˜è¦æ–¹æ³•å–ä»£åŸæœ¬ run_with_fallback
        print("ğŸ§  é–‹å§‹æ•´åˆæ‰€æœ‰æ®µè½æ‘˜è¦ï¼ˆä½¿ç”¨ _split_and_merge_summariesï¼‰...")
        final_summary = self._split_and_merge_summaries(chunk_summaries)
        print("ğŸ“ æ•´åˆæ‘˜è¦å®Œæˆï¼Œé•·åº¦ï¼š", len(final_summary))
        if final_summary:
            return final_summary
        else:
            print("âš ï¸ åˆä½µæ‘˜è¦å¤±æ•—ï¼Œå›å‚³å„æ®µæ‘˜è¦é›†åˆ")
            return "\n\n".join(chunk_summaries)


    

    def handle(self, user_question, memory=None):
        print("ğŸ§  [SQLAgent] å•Ÿå‹• SQL æŸ¥è©¢æµç¨‹...")
        if not user_question:
            return "âš ï¸ ä½¿ç”¨è€…å•é¡Œç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œ SQL æŸ¥è©¢ã€‚"
        user_question_sql_prompt, user_question_analysis_prompt = self._split_user_question(user_question)
        if not user_question_sql_prompt or not user_question_analysis_prompt:
            return "âš ï¸ ç„¡æ³•å¾ LLM æ‹†è§£ä½¿ç”¨è€…å•é¡Œï¼Œè«‹æª¢æŸ¥æ¨¡å‹è¨­å®šæˆ–è¼¸å…¥æ ¼å¼ã€‚"
        # 1. æ„é€  SQL æç¤º
        sql_prompt = self._build_prompt(user_question_sql_prompt)
        print("ğŸ“ [SQLAgent] æ§‹é€  SQL Promptï¼š")
        print(sql_prompt[:300])

        # 2. ç”¢ç”Ÿ SQL æŸ¥è©¢èªå¥
        raw_sql = self._generate_sql(sql_prompt)
        if not raw_sql:
            return "âš ï¸ ç„¡æ³•å¾ LLM ç”Ÿæˆæœ‰æ•ˆ SQL æŸ¥è©¢èªå¥ã€‚"

        # 3. æŠ½å– SQL æŒ‡ä»¤
        sql_code = self._extract_sql(raw_sql)
        print("ğŸ“ [SQLAgent] æŠ½å–çš„ SQL æŒ‡ä»¤ï¼š")
        if not sql_code:
            return "âš ï¸ ç„¡æ³•å¾å›æ‡‰ä¸­æ“·å– SQL æŒ‡ä»¤ã€‚"

        # 4. åŸ·è¡ŒæŸ¥è©¢
        df = self._run_sql(sql_code)
        if df is None or df.empty:
            return "ğŸ“­ æŸ¥ç„¡è³‡æ–™çµæœï¼Œè«‹èª¿æ•´æ¢ä»¶å¾Œå†è©¦ã€‚"

        # 5. æ‘˜è¦çµæœï¼ˆäººé¡ç‰ˆ + GPT ç‰ˆï¼‰
        summary = self._summarize_sql(df)
        summaryByLLM = self._summarize_sql_with_llm(df)

        combined = (
            "ğŸ“‹ [ç³»çµ±æ‘˜è¦]\n" + summary.strip() +
            "\n\nğŸ§  [GPT æ‘˜è¦]\n" + summaryByLLM.strip()
        )

        # 6. å¦‚æœæœ‰è¨˜æ†¶é«” agentï¼Œå°±å­˜é€² context
        if memory:
            memory.save("last_sql_summary", combined[:500])

        return combined