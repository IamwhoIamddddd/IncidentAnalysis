import subprocess
import os
import pickle
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd
import matplotlib.pyplot as plt
import re
import io
import base64
import sqlite3
import requests

# å¤–éƒ¨ç¨‹å¼ç¢¼
from utils.kb_loader import load_kb  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½å‡½å¼

DB_PATH = "resultDB.db"  # ä½ åœ¨ build_kb.py è£¡è¨­å®šçš„ DB åç¨±
kb_model, kb_index, kb_texts = load_kb() # è¼‰å…¥çŸ¥è­˜åº«æ¨¡å‹ã€ç´¢å¼•å’Œæ–‡æœ¬





class SemanticAgent:
    def __init__(self, model="orca2:13b", kb_model=None, kb_index=None, kb_texts=None):
        self.model = model
        self.kb_model = kb_model
        self.kb_index = kb_index
        self.kb_texts = kb_texts



    # å£“ç¸®å¤šæ®µæ‘˜è¦
    # å°‡å¤šæ®µæ‘˜è¦åˆä½µæˆä¸€æ®µ
    def _recursive_merge(self, summaries, token_limit, prompt_reserve):
        available_tokens = token_limit - prompt_reserve

        def estimate_token(text):
            return int(len(text) / 4)

        merged_groups = []
        group = []
        token_count = 0
        for s in summaries:
            t = estimate_token(s)
            if token_count + t > available_tokens and group:
                merged_groups.append(group)
                group = [s]
                token_count = t
            else:
                group.append(s)
                token_count += t
        if group:
            merged_groups.append(group)

        results = []
        for i, group in enumerate(merged_groups, 1):
            merge_prompt = "Based on the following summaries, please synthesize the main insights:\n\n"
            for j, s in enumerate(group, 1):
                merge_prompt += f"ï¼ˆç¬¬ {j} æ®µæ‘˜è¦ï¼‰{s}\n\n"
            merge_prompt += "Please provide an overall concluding observation:"
            reply = self._run_with_fallback(merge_prompt)
            results.append(reply if reply else "âŒ åˆä½µå¤±æ•—")

        return results[0] if len(results) == 1 else self._recursive_merge(results, token_limit, prompt_reserve)
    
    # ä½¿ç”¨ ollama åŸ·è¡Œæ¨¡å‹
    def _run_with_fallback(self, prompt):
        try:
            result = subprocess.run(
                ["ollama", "run", self.model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )
            if result.returncode == 0:
                return result.stdout.decode("utf-8").strip()
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {self.model} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        fallback_model = "nous-hermes2:10.7b"
        try:
            result = subprocess.run(
                ["ollama", "run", fallback_model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )
            if result.returncode == 0:
                return result.stdout.decode("utf-8").strip()
        except Exception as e:
            print(f"âŒ fallback æ¨¡å‹ {fallback_model} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        return None

    # çŸ¥è­˜åº«æ‘˜è¦è™•ç†
    def _summarize_retrieved_kb(self, retrieved):
        if not retrieved:
            print("âš ï¸ ç„¡è³‡æ–™å¯æ‘˜è¦ï¼ˆretrieved ç‚ºç©ºï¼‰")
            return ""

        print("ğŸ§  æ­£åœ¨é€²è¡Œåˆ†æ®µæ‘˜è¦è™•ç†ï¼ˆretrieved KBï¼‰...")
        print(f"ğŸ“¦ è¼¸å…¥ç­†æ•¸ï¼š{len(retrieved)}")

        model_token_limits = {
            "orca2:13b": 8192,
            "nous-hermes2:10.7b": 8192,
            "mistral": 8192,
            "phi4-mini": 4096,
            "phi3:mini": 4096,
            "command-r7b:latest": 4096,
            "openchat:7b": 4096,
            "deepseek-coder-v2:latest": 16384,
            "deepseek-coder:latest": 16384,
        }
        token_limit = model_token_limits.get(self.model, 4096)
        print(f"ğŸ” æ¨¡å‹ {self.model} çš„ token é™åˆ¶ï¼š{token_limit}")
        prompt_reserve = 500
        available_tokens = token_limit - prompt_reserve
        print(f"ğŸ§  å¯ç”¨ token æ•¸é‡ï¼š{available_tokens}ï¼ˆæ‰£é™¤æç¤ºè©ä¿ç•™ {prompt_reserve}ï¼‰" )

        def estimate_token(text):
            return int(len(text) / 4)

        groups = []
        group = []
        token_sum = 0
        print("ğŸ” æ­£åœ¨åˆ†çµ„çŸ¥è­˜åº«è³‡æ–™...")
        for text in retrieved:
            tokens = estimate_token(text)
            print(f"ğŸ“¦ è™•ç†æ–‡æœ¬ï¼š{text[:50]}... (ä¼°è¨ˆ token: {tokens})")
            if token_sum + tokens > available_tokens and group:
                groups.append(group)
                group = [text]
                token_sum = tokens
            else:
                group.append(text)
                token_sum += tokens
        if group:
            groups.append(group)
        print(f"ğŸ“¦ å…±åˆ†æˆ {len(groups)} çµ„ï¼Œæ¯çµ„å¹³å‡ {token_sum / len(groups):.2f} tokens" )
        chunk_summaries = []
        for i, group in enumerate(groups, 1):
            prompt = "Please summarize the key points and handling methods based on the following knowledge entries (respond in English):\n\n"
            for j, txt in enumerate(group, 1):
                prompt += f"{j}. {txt.strip()}\n"
            prompt += "\nPlease provide a single summary paragraph:"

            reply = self._run_with_fallback(prompt)
            chunk_summaries.append(reply if reply else "âŒ æœ¬æ®µæ‘˜è¦å¤±æ•—")

        if len(chunk_summaries) == 1:
            return chunk_summaries[0]
        else:
            return self._recursive_merge(chunk_summaries, token_limit, prompt_reserve)



    # ä¸»è¦è™•ç†å‡½å¼
    def _determine_top_k(self, user_input, fallback=3, min_top_k=1, max_top_k=10):
        print("ğŸ§  [SemanticAgent] ä½¿ç”¨ LLM é æ¸¬åˆé©çš„ top_k æ•¸é‡...")
        prompt = (
            "You are a knowledge retrieval assistant. Based on the user's question, decide how many similar cases (top_k) should be retrieved from the knowledge base.\n\n"
            "Guidelines:\n"
            "- If the question is **very specific** (mentions error codes, clear symptoms, or keywords), return a **small** top_k (1â€“3).\n"
            "- If the question is **vague or general** (like 'why is it slow?' or 'something went wrong'), return a **larger** top_k (5â€“10).\n"
            "- If the user asks for a **summary, report, or trend**, use a **larger** top_k (8â€“10).\n"
            "- Only reply with a **single integer** between 1 and 10. Do not add explanation.\n\n"
            f"User question: {user_input}\n\nAnswer:"
        )

        def try_model(model_name):
            try:
                print(f"ğŸ§  å˜—è©¦æ¨¡å‹ï¼š{model_name}")
                result = subprocess.run(
                    ["ollama", "run", model_name],
                    input=prompt.encode("utf-8"),
                    capture_output=True,
                    timeout=240
                )
                if result.returncode == 0:
                    reply = result.stdout.decode("utf-8").strip()
                    print(f"ğŸ“¥ æ¨¡å‹å›è¦†ï¼š{reply}")
                    match = re.search(r"\b([1-9]|10)\b", reply)
                    if match:
                        top_k = int(match.group(1))
                        return max(min_top_k, min(top_k, max_top_k))
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_name} å¤±æ•—ï¼š{e}")
            return None

        for model in ["command-r7b:latest", "openchat:7b", "phi4-mini"]:
            top_k = try_model(model)
            if top_k:
                return top_k
        print(f"âš ï¸ å…¨éƒ¨æ¨¡å‹å¤±æ•—ï¼Œä½¿ç”¨ fallback {fallback}")
        return fallback

    def _search_knowledge_base(self, query, top_k=None):
        print("ğŸ” [SemanticAgent] åŸ·è¡Œèªæ„æŸ¥è©¢...")
        if self.kb_model is None or self.kb_index is None or self.kb_texts is None:
            print("âŒ çŸ¥è­˜åº«å°šæœªè¼‰å…¥")
            return []
        if top_k is None:
            top_k = self._determine_top_k(query)
            print(f"ğŸ¤– å‹•æ…‹æ±ºå®š top_k = {top_k}")

        query_vec = self.kb_model.encode([query])
        D, I = self.kb_index.search(np.array(query_vec), top_k)
        print(f"[RAG] ğŸ” æŸ¥è©¢å…§å®¹ï¼š{query}")
        print(f"[RAG] ğŸ§  å–å‡ºçŸ¥è­˜åº«è³‡æ–™ï¼š{[self.kb_texts[i][:50] for i in I[0]]}")
        return [self.kb_texts[i] for i in I[0]]
    
    def handle(self, query, top_k=None):
        print(f"ğŸ“¥ [SemanticAgent] æ¥æ”¶åˆ°æŸ¥è©¢ï¼š{query}")
        retrieved = self._search_knowledge_base(query, top_k)
        summary = self._summarize_retrieved_kb(retrieved)
        return summary

