import subprocess
import os
import pickle
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd
import matplotlib.pyplot as plt
import re
import io
import base64
import sqlite3
import requests
import hashlib
import openai


POWERAUTOMATE_URL = "https://prod-08.southeastasia.logic.azure.com:443/workflows/a9de89a708674755923e900665994521/triggers/manual/paths/invoke?api-version=2016-06-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=Eo8rgY9JHLAqYDQCYTjWYiufiHq3LYQ_kZXWmGjqLhw"  # ğŸ” è«‹æ›æˆä½ çš„å¯¦éš›ç¶²å€


# å¤–éƒ¨ç¨‹å¼ç¢¼
from utils.kb_loader import load_kb  # âœ… è«‹ç¢ºä¿ä½ å·²ç¶“å»ºç«‹é€™å€‹æª”æ¡ˆä¸¦æ”¾å¥½å‡½å¼

DB_PATH = "resultDB.db"  # ä½ åœ¨ build_kb.py è£¡è¨­å®šçš„ DB åç¨±
kb_model, kb_index, kb_texts = load_kb() # è¼‰å…¥çŸ¥è­˜åº«æ¨¡å‹ã€ç´¢å¼•å’Œæ–‡æœ¬

def id_to_int64(uid):
    return int(hashlib.sha256(uid.encode("utf-8")).hexdigest(), 16) % (1 << 63)


class SemanticAgent:
    def __init__(self, model="orca2:13b", kb_model=None, kb_index=None, kb_texts=None, metadata=None, faiss_id_to_text=None):
        self.model = model
        self.kb_model = kb_model
        self.kb_index = kb_index
        self.kb_texts = kb_texts
        self.metadata = metadata or []  # é¿å… metadata=None æ™‚å‡ºéŒ¯
        self.faiss_id_to_text = faiss_id_to_text or {}  # é¿å… faiss_id_to_text=None æ™‚å‡ºéŒ¯


        self.faiss_to_real_id = {
            id_to_int64(item["id"]): item["id"] for item in self.metadata
        }
        
    # å£“ç¸®å¤šæ®µæ‘˜è¦
    # å°‡å¤šæ®µæ‘˜è¦åˆä½µæˆä¸€æ®µ
    def _recursive_merge(self, summaries, token_limit, prompt_reserve):
        available_tokens = token_limit - prompt_reserve

        def estimate_token(text):
            return int(len(text) / 4)

        merged_groups = []
        group = []
        token_count = 0
        for s in summaries:
            t = estimate_token(s)
            if token_count + t > available_tokens and group:
                merged_groups.append(group)
                group = [s]
                token_count = t
            else:
                group.append(s)
                token_count += t
        if group:
            merged_groups.append(group)

        results = []

        for i, group in enumerate(merged_groups, 1):
            print(f"ğŸ“¡ å˜—è©¦ä½¿ç”¨ AI Builder åˆ†æç¬¬ {i} çµ„åˆä½µæ‘˜è¦...")
            group_text = "\n\n".join([f"({j+1}) {s.strip()}" for j, s in enumerate(group)])

            payload = {
                "template": "Based on the following summaries, please synthesize the main insights:\n\nSummaries:",
                "userInput": group_text + "\n\nPlease provide an overall concluding observation:"
            }

            try:
                res = requests.post(POWERAUTOMATE_URL, json=payload, timeout=360)
                if res.status_code == 200:
                    reply = res.json().get("response", "").strip()
                    print(f"âœ… AI Builder å›æ‡‰åˆä½µæ‘˜è¦ï¼š{reply}")
                else:
                    print(f"âš ï¸ AI Builder å›å‚³ç‹€æ…‹ç¢¼ç•°å¸¸ï¼š{res.status_code}")
                    reply = None
            except Exception as e:
                print(f"âŒ AI Builder å‘¼å«å¤±æ•—æˆ–é€¾æ™‚ï¼š{e}")
                reply = None

            if not reply:
                merge_prompt = "Based on the following summaries, please synthesize the main insights:\n\n"
                for j, s in enumerate(group, 1):
                    merge_prompt += f"ï¼ˆç¬¬ {j} æ®µæ‘˜è¦ï¼‰{s}\n\n"
                merge_prompt += "Please provide an overall concluding observation:"
                reply = self._run_with_fallback(merge_prompt)
                print(f"ğŸ” fallback æ¨¡å‹åˆä½µçµæœï¼š{reply}")

            results.append(reply if reply else "âŒ åˆä½µå¤±æ•—")

        return results[0] if len(results) == 1 else self._recursive_merge(results, token_limit, prompt_reserve)
    
    # ä½¿ç”¨ ollama åŸ·è¡Œæ¨¡å‹
    def _run_with_fallback(self, prompt):
        try:
            result = subprocess.run(
                ["ollama", "run", self.model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )
            if result.returncode == 0:
                return result.stdout.decode("utf-8").strip()
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {self.model} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        fallback_model = "nous-hermes2:10.7b"
        try:
            result = subprocess.run(
                ["ollama", "run", fallback_model],
                input=prompt.encode("utf-8"),
                capture_output=True,
                timeout=600
            )
            if result.returncode == 0:
                return result.stdout.decode("utf-8").strip()
        except Exception as e:
            print(f"âŒ fallback æ¨¡å‹ {fallback_model} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        return None
    
    def _summarize_retrieved_kb(self, retrieved):
        if not retrieved:
            print("âš ï¸ ç„¡è³‡æ–™å¯æ‘˜è¦ï¼ˆretrieved ç‚ºç©ºï¼‰")
            return ""

        print("ğŸ§  æ­£åœ¨é€²è¡Œåˆ†æ®µæ‘˜è¦è™•ç†ï¼ˆretrieved KBï¼‰...")
        print(f"ğŸ“¦ è¼¸å…¥ç­†æ•¸ï¼š{len(retrieved)}")

        model_token_limits = {
            "orca2:13b": 8192,
            "nous-hermes2:10.7b": 8192,
            "mistral": 8192,
            "phi4-mini": 4096,
            "phi3:mini": 4096,
            "command-r7b:latest": 4096,
            "openchat:7b": 4096,
            "deepseek-coder-v2:latest": 16384,
            "deepseek-coder:latest": 16384,
        }
        token_limit = model_token_limits.get(self.model, 4096)
        prompt_reserve = 500
        available_tokens = token_limit - prompt_reserve
        print(f"ğŸ” æ¨¡å‹ {self.model} çš„ token é™åˆ¶ï¼š{token_limit}")
        print(f"ğŸ§  å¯ç”¨ token æ•¸é‡ï¼š{available_tokens}ï¼ˆæ‰£é™¤æç¤ºè©ä¿ç•™ {prompt_reserve}ï¼‰")

        def estimate_token(text):
            return int(len(text) / 4)

        groups = []
        group = []
        token_sum = 0
        print("ğŸ” æ­£åœ¨åˆ†çµ„çŸ¥è­˜åº«è³‡æ–™...")
        for text in retrieved:
            tokens = estimate_token(text)
            print(f"ğŸ“¦ è™•ç†æ–‡æœ¬ï¼š{text[:50]}... (ä¼°è¨ˆ token: {tokens})")
            if token_sum + tokens > available_tokens and group:
                groups.append(group)
                group = [text]
                token_sum = tokens
            else:
                group.append(text)
                token_sum += tokens
        if group:
            groups.append(group)

        print(f"ğŸ“¦ å…±åˆ†æˆ {len(groups)} çµ„ï¼Œæ¯çµ„å¹³å‡ {token_sum / len(groups):.2f} tokens")
        chunk_summaries = []
        
        for i, group in enumerate(groups, 1):
            print(f"ğŸ“¡ å˜—è©¦ä½¿ç”¨ AI Builder åˆ†æç¬¬ {i} çµ„æ‘˜è¦...")
            entries_text = "\n".join([f"{j+1}. {txt.strip()}" for j, txt in enumerate(group)])
            payload = {
                "template": "Please summarize the key points and handling methods based on the following knowledge entries (respond in English):",
                "userInput": entries_text + "\n\nPlease provide a single summary paragraph:"
            }

            try:
                res = requests.post(POWERAUTOMATE_URL, json=payload, timeout=360)
                if res.status_code == 200:
                    reply = res.json().get("response", "").strip()
                    print(f"âœ… AI Builder å›æ‡‰æ‘˜è¦ï¼š{reply}")
                else:
                    print(f"âš ï¸ AI Builder å›å‚³ç‹€æ…‹ç¢¼ç•°å¸¸ï¼š{res.status_code}")
                    reply = None
            except Exception as e:
                print(f"âŒ AI Builder å‘¼å«å¤±æ•—æˆ–é€¾æ™‚ï¼š{e}")
                reply = None

            if not reply:
                prompt = (
                    "Please summarize the key points and handling methods based on the following knowledge entries (respond in English):\n\n" +
                    entries_text + "\n\nPlease provide a single summary paragraph:"
                )
                reply = self._run_with_fallback(prompt)
                print(f"ğŸ” fallback æ¨¡å‹æ‘˜è¦çµæœï¼š{reply}")

            chunk_summaries.append(reply if reply else "âŒ æœ¬æ®µæ‘˜è¦å¤±æ•—")

        if len(chunk_summaries) == 1:
            return chunk_summaries[0]
        else:
            return self._recursive_merge(chunk_summaries, token_limit, prompt_reserve)



    def _determine_top_k(self, user_input, fallback=3, min_top_k=1, max_top_k=10):
        print("ğŸ§  [SemanticAgent] ä½¿ç”¨ AI Builder é æ¸¬åˆé©çš„ top_k æ•¸é‡ï¼ˆHTTP æ¨¡å¼ï¼‰...")
        
        # âœ… Power Automate ç”¨ï¼šPrompt èˆ‡ user_input åˆ†é–‹
        powerautomate_prompt_template = (
            "You are a knowledge retrieval assistant. Based on the user's question, decide how many similar cases (top_k) should be retrieved from the knowledge base.\n\n"
            "Guidelines:\n"
            "- If the question is **very specific** (mentions error codes, clear symptoms, or keywords), return a **small** top_k (1â€“3).\n"
            "- If the question is **vague or general** (like 'why is it slow?' or 'something went wrong'), return a **larger** top_k (5â€“10).\n"
            "- If the user asks for a **summary, report, or trend**, use a **larger** top_k (8â€“10).\n"
            "- Only reply with a **single integer** between 1 and 10. Do not add explanation.\n\n"
            "User question: "
        )
        
        # âœ… åŸæœ¬çš„ promptï¼ˆä¿ç•™åˆä½µç‰ˆæœ¬ï¼‰â€” ç”¨æ–¼æœ¬åœ°æ¨¡å‹
        prompt = (
            powerautomate_prompt_template + f"{user_input}\n\nAnswer:"
        )

        # âœ… å…ˆå˜—è©¦é€é Power Automate çš„ AI Builder åˆ†æ
        try:
            payload = {
                "template": powerautomate_prompt_template,
                "userInput": user_input,
            }
            print(f"ğŸ“¡ å‚³é€ prompt è‡³ Power Automate...")
            res = requests.post(POWERAUTOMATE_URL, json=payload, timeout=360)
            if res.status_code == 200:
                reply = res.json().get("response", "").strip()
                print(f"ğŸ“¥ AI Builder å›æ‡‰ï¼š{reply}")
                match = re.search(r"\b([1-9]|10)\b", reply)
                if match:
                    top_k = int(match.group(1))
                    return max(min_top_k, min(top_k, max_top_k))
                else:
                    print("âš ï¸ å›æ‡‰ä¸­æœªåµæ¸¬åˆ°åˆæ³•æ•¸å­— top_k")
            else:
                print(f"âŒ AI Builder å›æ‡‰éŒ¯èª¤ï¼Œç‹€æ…‹ç¢¼ï¼š{res.status_code}")
        except Exception as e:
            print(f"âŒ AI Builder å‘¼å«å¤±æ•—æˆ–é€¾æ™‚ï¼Œä½¿ç”¨ fallback æ¨¡å‹ï¼š{e}")

        # âœ… æ”¹ç”¨æœ¬åœ°æ¨¡å‹ fallback
        def try_model(model_name):
            try:
                print(f"ğŸ§  å˜—è©¦æœ¬åœ°æ¨¡å‹ï¼š{model_name}")
                result = subprocess.run(
                    ["ollama", "run", model_name],
                    input=prompt.encode("utf-8"),
                    capture_output=True,
                    timeout=240
                )
                if result.returncode == 0:
                    reply = result.stdout.decode("utf-8").strip()
                    print(f"ğŸ“¥ æ¨¡å‹å›è¦†ï¼š{reply}")
                    match = re.search(r"\b([1-9]|10)\b", reply)
                    if match:
                        top_k = int(match.group(1))
                        return max(min_top_k, min(top_k, max_top_k))
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_name} å¤±æ•—ï¼š{e}")
            return None

        for model in ["command-r7b:latest", "openchat:7b", "phi4-mini"]:
            top_k = try_model(model)
            if top_k:
                return top_k

        print(f"âš ï¸ æ‰€æœ‰æ–¹å¼çš†å¤±æ•—ï¼Œä½¿ç”¨é è¨­ fallback å€¼ {fallback}")
        return fallback

    def _search_knowledge_base(self, query, top_k=None):
        print("ğŸ” [SemanticAgent] åŸ·è¡Œèªæ„æŸ¥è©¢...")
        if self.kb_model is None or self.kb_index is None or self.kb_texts is None:
            print("âŒ çŸ¥è­˜åº«å°šæœªè¼‰å…¥")
            return []

        if top_k is None:
            top_k = self._determine_top_k(query)
            print(f"ğŸ¤– å‹•æ…‹æ±ºå®š top_k = {top_k}")

        query_vec = self.kb_model.encode([query])
        D, I = self.kb_index.search(np.array(query_vec), top_k) # åŸ·è¡Œæª¢ç´¢
        

        print("ğŸ“‚ é–‹å•Ÿ SQLite è³‡æ–™åº«é€£ç·š")
        try:
            conn = sqlite3.connect(DB_PATH)
            cur = conn.cursor()
        except Exception as e:
            print(f"âŒ é€£ç·š SQLite å¤±æ•—ï¼š{e}")
            return []
        print("ğŸ” æ­£åœ¨æŸ¥è©¢ç›¸é—œçŸ¥è­˜åº«è³‡æ–™...")

        results = []
        for i, distance in zip(I[0], D[0]):
            try:
                real_id = self.faiss_to_real_id.get(i, "âŒ ç„¡æ³•å°æ‡‰")
                text = self.faiss_id_to_text.get(i, "âŒ æ‰¾ä¸åˆ°å¥å­")  # âœ… æ”¹é€™è£¡ï¼
                print(f"ğŸ“Œ FAISS ID: {i} â†’ åŸå§‹ ID: {real_id}")
                print(f"ğŸ“œ åŸå§‹å¥å­å…§å®¹ï¼š{text} ï½œğŸ“ è·é›¢ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰: {distance:.4f}")


                # æŸ¥è³‡æ–™åº«
                cur.execute("SELECT * FROM metadata WHERE id = ?", (real_id,))
                row = cur.fetchone()

                if row:
                    columns = [desc[0] for desc in cur.description]
                    row_dict = dict(zip(columns, row))
                    print(f"ğŸ§¾ æˆåŠŸæŸ¥å›è³‡æ–™åº«è³‡æ–™ï¼š{row_dict}")
                else:
                    row_dict = None
                    print("âš ï¸ æŸ¥ä¸åˆ° SQLite è³‡æ–™ï¼ˆå¯èƒ½æ˜¯ ID ä¸å­˜åœ¨ï¼‰")

                results.append({
                    "text": text,
                    "real_id": real_id,
                    "row": row_dict
                })
            except Exception as e:
                print(f"âŒ è™•ç† FAISS ID {i} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                
        try:
            conn.close()
            print("âœ… é—œé–‰ SQLite é€£ç·š")
        except Exception as e:
            print(f"âš ï¸ é—œé–‰é€£ç·šæ™‚å‡ºéŒ¯ï¼š{e}")
        print(f"ğŸ“¦ å…±æ‰¾åˆ° {len(results)} ç­†ç›¸é—œçŸ¥è­˜åº«è³‡æ–™")
        return results
        

    
    def handle(self, query, top_k=None):
        print(f"ğŸ“¥ [SemanticAgent] æ¥æ”¶åˆ°æŸ¥è©¢ï¼š{query}")
        retrieved_pairs = self._search_knowledge_base(query, top_k)

        # åŠ å…¥ Entry åˆ†éš”èˆ‡æ¬„ä½æ ¼å¼åŒ–
        retrieved_texts = []
        for idx, entry in enumerate(retrieved_pairs, 1):
            row = entry.get("row")
            if row:
                description = (
                    f"--- Incident Entry {idx} ---\n"
                    f"ID: {row.get('id', '')}\n"
                    f"Text: {row.get('text', '')}\n"
                    f"Subcategory: {row.get('subcategory', '')}\n"
                    f"ConfigurationItem: {row.get('configurationItem', '')}\n"
                    f"RoleComponent: {row.get('roleComponent', '')}\n"
                    f"Location: {row.get('location', '')}\n"
                    f"Opened: {row.get('opened', '')}\n"
                    f"AnalysisTime: {row.get('analysisTime', '')}"
                )
            else:
                description = f"--- Entry {idx} ---\n{entry.get('text', 'âŒ ç„¡åŸå§‹æ–‡å­—')}"

            retrieved_texts.append(description)

        # åšæ‘˜è¦
        summary = self._summarize_retrieved_kb(retrieved_texts)

        print("ğŸ“ [SemanticAgent] çŸ¥è­˜åº«æ‘˜è¦å®Œæˆï¼š")
        print(summary)

        # é¡å¤–åˆ—å° ID
        print("ğŸ“‹ æŸ¥è©¢å°æ‡‰ ID åœ¨ sqlite db ä¸­ï¼š")
        for idx, entry in enumerate(retrieved_pairs, 1):
            print(f"ğŸ†” Entry {idx} â†’ ID: {entry['real_id']}")

        return summary

