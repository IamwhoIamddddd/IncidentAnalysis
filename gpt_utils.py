from SmartScoring import is_actionable_resolution
import aiohttp
import asyncio


# ğŸ§  ä¸»åŠŸèƒ½ï¼šå¾æ®µè½ä¸­æŠ½å‡ºè§£æ±ºå»ºè­°å¥ï¼ˆå«ç©ºå€¼èˆ‡é—œéµå­—æª¢æŸ¥ï¼‰
async def extract_resolution_suggestion(text, model="mistral"):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    ALWAYS_ANALYZE = True
    if not ALWAYS_ANALYZE and not is_actionable_resolution(text):
        print("â­ï¸ ç„¡èªæ„ç›¸è¿‘è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼š", text[:100])
        return "ï¼ˆæœªåµæ¸¬åˆ°å…·é«”è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼‰"

    lines = text.strip().splitlines()
    text_trimmed = "\n".join(lines[:3])

    prompt = f"""Instruction: Summarize 1 actionable solution from the following.
Reply "No recommendation" if none. Limit answer to 30 words.
---
{text_trimmed}
"""

    try:
        return await call_ollama_model_async(prompt, model)
    except Exception as e:
        print("âŒ åˆæ¬¡å‘¼å«å¤±æ•—ï¼Œå˜—è©¦é‡è©¦ä¸€æ¬¡...")
        await asyncio.sleep(2)
        try:
            return await call_ollama_model_async(prompt, model)
        except Exception as e2:
            print("â›” GPT é›™æ¬¡å‘¼å«éƒ½å¤±æ•—ï¼š", e2)
            return "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"


async def extract_problem_with_custom_prompt(text, model="phi3:mini"):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    lines = text.strip().splitlines()
    text_trimmed = "\n".join(lines[:3])

    prompt = f"""You're an assistant. Read the following incident note and summarize what issue or problem it describes, in one clear sentence.
Do not suggest a solution. Only summarize the problem.
Limit to 30 words.
---
{text_trimmed}
"""

    try:
        return await call_ollama_model_async(prompt, model)
    except Exception as e:
        print("âŒ åˆæ¬¡å‘¼å«å¤±æ•—ï¼Œå˜—è©¦é‡è©¦ä¸€æ¬¡...")
        await asyncio.sleep(2)
        try:
            return await call_ollama_model_async(prompt, model)
        except Exception as e2:
            print("â›” GPT é›™æ¬¡å‘¼å«éƒ½å¤±æ•—ï¼š", e2)
            return "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"


# ğŸ”§ éåŒæ­¥å‘¼å«æœ¬åœ° Ollama API
async def call_ollama_model_async(prompt, model="mistral", timeout=120):
    url = "http://localhost:11434/api/generate"
    headers = {"Content-Type": "application/json"}

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "num_predict": 50,
            "temperature": 0.5
        }
    }

    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
        async with session.post(url, json=payload, headers=headers) as response:
            response.raise_for_status()
            result = await response.json()
            return result.get("response", "").strip()
