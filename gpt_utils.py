from SmartScoring1 import is_actionable_resolution
import aiohttp
import asyncio
import hashlib
import json
import os
from datetime import datetime
from sentence_transformers import SentenceTransformer, util
import numpy as np

MAX_CONCURRENCY = 10
DEFAULT_MODEL_SOLUTION = "mistral"
DEFAULT_MODEL_SUMMARY = "phi3:mini"
semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

# âœ… å¿«å–å„²å­˜ä½ç½®
CACHE_DIR = "cache"
CACHE_FILE = os.path.join(CACHE_DIR, "semantic_cache.json")
MAX_CACHE_SIZE = 3000

# çµ±è¨ˆç”¨è®Šæ•¸
cache_hit_count = 0
cache_total_queries = 0

# âœ… ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(CACHE_DIR, exist_ok=True)

# âœ… è¼‰å…¥èªæ„æ¨¡å‹
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# âœ… è¼‰å…¥å¿«å–è³‡æ–™
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        semantic_cache = json.load(f)
else:
    semantic_cache = []

# âœ… ç”¢ç”Ÿ hash key ç”¨æ–¼å®Œå…¨æ¯”å°
def make_hash(text):
    return hashlib.sha256(text.encode('utf-8')).hexdigest()

# âœ… èªæ„å¿«å–æŸ¥è©¢ï¼ˆå«æ–‡å­— hash + cosineï¼‰
# âœ… èªæ„å¿«å–æŸ¥è©¢ï¼ˆå«æ–‡å­— hash + cosineï¼‰
def find_semantic_cache(text, threshold=0.9, source_id=""):
    global cache_hit_count, cache_total_queries
    cache_total_queries += 1
    key = make_hash(text)

    print(f"ğŸ” [Cache] æŸ¥æ‰¾å¿«å–ä¸­... {source_id} hash={key[:8]} text='{text[:30]}'")

    for item in semantic_cache:
        if item.get("hash") == key:
            if item["response"] == "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰":
                print(f"ğŸš« [Cache] å‘½ä¸­ä½†ç‚ºå¤±æ•—å¿«å–ï¼ˆ{source_id}ï¼‰ï¼Œéœ€é€ GPT å†åˆ†æ")
                return None
            cache_hit_count += 1
            print(f"ğŸ¯ [Cache] å®Œæ•´å‘½ä¸­ï¼ï¼ˆ{source_id}ï¼‰")
            return item["response"]

    if len(semantic_cache) == 0:
        print(f"ğŸ“­ [Cache] ç„¡ä»»ä½•å¿«å–å¯æ¯”å°ï¼ˆcache ç©ºï¼‰ï¼ˆ{source_id}ï¼‰")
        return None

    try:
        query_vec = embedding_model.encode(text).astype(np.float32)
        all_vecs = np.array([item['embedding'] for item in semantic_cache], dtype=np.float32)
        sims = util.cos_sim(query_vec, all_vecs).flatten()
    except Exception as e:
        print(f"âŒ [Cache] èªæ„æ¯”å°æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼ˆ{source_id}ï¼‰ï¼š{e}")
        return None

    best_idx = int(np.argmax(sims))
    if sims[best_idx] > threshold:
        response = semantic_cache[best_idx]["response"]
        if response == "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰":
            print(f"ğŸš« [Cache] èªæ„ç›¸ä¼¼å‘½ä¸­ä½†ç‚ºå¤±æ•—å¿«å–ï¼ˆ{sims[best_idx]:.3f}ï¼‰ï¼ˆ{source_id}ï¼‰")
            return None
        cache_hit_count += 1
        print(f"ğŸ¯ [Cache] èªæ„ç›¸ä¼¼å‘½ä¸­ï¼ç›¸ä¼¼åº¦={sims[best_idx]:.3f}ï¼ˆ{source_id}ï¼‰")
        return response

    print(f"âŒ [Cache] ç„¡å‘½ä¸­ï¼Œå°‡é€ GPT æ“·å–æ–°è³‡æ–™ï¼ˆ{source_id}ï¼‰")
    return None




# âœ… å„²å­˜æ–°çš„å¿«å–ç´€éŒ„
def add_to_semantic_cache(text, response):
    key = make_hash(text)
    emb = embedding_model.encode(text).tolist()
    semantic_cache.append({
        "hash": key,
        "input": text,
        "embedding": emb,
        "response": response,
        "createdAt": datetime.now().isoformat()
    })
    if len(semantic_cache) > MAX_CACHE_SIZE:
        semantic_cache.pop(0)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(semantic_cache, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ [Cache] å·²å„²å­˜å¿«å–ï¼šhash={key[:8]} text='{text[:30]}'")

# ğŸ§  ä¸»åŠŸèƒ½ï¼šå¾æ®µè½ä¸­æŠ½å‡ºè§£æ±ºå»ºè­°å¥ï¼ˆå«ç©ºå€¼èˆ‡å¿«å–ï¼‰

def get_gpt_prompt_and_model(task="solution"):
    MAP_PATH = os.path.join("gpt_data", "gpt_prompt_map.json")
    try:
        with open(MAP_PATH, "r", encoding="utf-8") as f:
            config = json.load(f)
        if config.get(task):
            prompt = config[task].get("prompt") or ""
            model = config[task].get("model") or ""
            # æª¢æŸ¥æ˜¯å¦ prompt/model ç¼ºå¤±
            if not prompt or not model:
                print(f"âš ï¸ [PromptMap] {task} çš„ prompt æˆ– model æ¬„ä½ç‚ºç©ºï¼Œå·²å•Ÿç”¨é è¨­å€¼ï¼")
            return prompt, model
        else:
            print(f"âš ï¸ [PromptMap] {task} è¨­å®šä¸å­˜åœ¨ï¼Œå·²å•Ÿç”¨é è¨­å€¼ï¼")
    except Exception as e:
        print(f"âŒ [PromptMap] è®€å– {task} è¨­å®šå¤±æ•—ï¼š{e}")
    # å›å‚³é è¨­å€¼
    if task == "solution":
        print("âš ï¸ [PromptMap] ä½¿ç”¨ solution é è¨­ prompt/model")
        return "è«‹å¾ä»¥ä¸‹æ®µè½æå–ä¸€å€‹å…·é«”çš„è¡Œå‹•å»ºè­°", DEFAULT_MODEL_SOLUTION
    elif task == "ai_summary":
        print("âš ï¸ [PromptMap] ä½¿ç”¨ ai_summary é è¨­ prompt/model")
        return "è«‹ç”¨ä¸€å¥è©±æè¿°äº‹ä»¶æ˜¯ä»€éº¼", DEFAULT_MODEL_SUMMARY
    else:
        print(f"âš ï¸ [PromptMap] æœªçŸ¥ç”¨é€” {task}ï¼Œå›å‚³ç©ºå€¼")
        return "", ""



async def extract_resolution_suggestion(text, model=DEFAULT_MODEL_SOLUTION, source_id=""):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    ALWAYS_ANALYZE = True
    if not ALWAYS_ANALYZE and not is_actionable_resolution(text):
        print(f"â­ï¸ ç„¡èªæ„ç›¸è¿‘è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼ˆ{source_id}ï¼‰ï¼š", text[:100])
        return "ï¼ˆæœªåµæ¸¬åˆ°å…·é«”è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼‰"

    # è®€å–ç›®å‰çš„ prompt èˆ‡ model è¨­å®š
    custom_prompt, custom_model = get_gpt_prompt_and_model("solution")
    model = model or custom_model

    lines = text.strip().splitlines()
    text_trimmed = "\n".join(lines[:3])
    print(f"ğŸ” [GPT] æº–å‚™æ“·å–è§£æ±ºå»ºè­°ï¼š{text_trimmed[:30]}...ï¼ˆ{source_id}ï¼‰")

    cached = find_semantic_cache(text_trimmed, source_id=source_id)
    if cached:
        print(f"ğŸ¯ å¿«å–å‘½ä¸­ï¼šç•¥é GPT åˆ†æï¼ˆ{source_id}ï¼‰")
        return cached

    prompt = f"{custom_prompt}\n---\n{text_trimmed}"
    max_retry = 5
    retry_count = 0

    while retry_count < max_retry:
        try:
            result = await call_ollama_model_async(prompt, model)
            if result and "æ“·å–å¤±æ•—" not in result and "æœªåµæ¸¬" not in result:
                print(f"âœ… GPT (æ“·å–è§£æ±ºå»ºè­°)ç¬¬ {retry_count + 1} æ¬¡å‘¼å«æˆåŠŸï¼ˆ{source_id}ï¼‰")
                add_to_semantic_cache(text_trimmed, result)
                return result
            else:
                print(f"âš ï¸ GPT å›å‚³å…§å®¹ä¸å®Œæ•´ï¼Œç¬¬ {retry_count + 1} æ¬¡çµæœç‚ºï¼š{result[:30]}...")
        except Exception as e:
            print(f"âš ï¸ GPT å‘¼å«å¤±æ•—ï¼ˆç¬¬ {retry_count + 1} æ¬¡ï¼‰ï¼ˆ{source_id}ï¼‰ï¼š{e}")
        retry_count += 1
        await asyncio.sleep(2)

    print(f"â›” GPT åˆ†æå¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ {max_retry} æ¬¡")
    return "ï¼ˆAI (æ“·å–è§£æ±ºå»ºè­°)æ“·å–å¤±æ•—ï¼‰"


async def extract_problem_with_custom_prompt(text, model=None, source_id=""):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    # è®€å–ç›®å‰çš„ prompt èˆ‡ model è¨­å®š
    custom_prompt, custom_model = get_gpt_prompt_and_model("ai_summary")
    model = model or custom_model

    lines = text.strip().splitlines()
    text_trimmed = "\n".join(lines[:3])
    print(f"ğŸ” [GPT] æº–å‚™æ“·å–å•é¡Œæ‘˜è¦ï¼š{text_trimmed[:30]}...ï¼ˆ{source_id}ï¼‰")

    cached = find_semantic_cache(text_trimmed, source_id=source_id)
    if cached:
        print(f"ğŸ¯ å¿«å–å‘½ä¸­ï¼šç•¥é GPT (æ“·å–å•é¡Œæ‘˜è¦) åˆ†æï¼ˆ{source_id}ï¼‰")
        return cached

    prompt = f"{custom_prompt}\n---\n{text_trimmed}"
    max_retry = 5
    retry_count = 0

    while retry_count < max_retry:
        try:
            result = await call_ollama_model_async(prompt, model)
            if result and "æ“·å–å¤±æ•—" not in result and "æœªåµæ¸¬" not in result:
                print(f"âœ… GPT (æ“·å–å•é¡Œæ‘˜è¦) ç¬¬ {retry_count + 1} æ¬¡å‘¼å«æˆåŠŸï¼ˆ{source_id}ï¼‰")
                add_to_semantic_cache(text_trimmed, result)
                return result
            else:
                print(f"âš ï¸ GPT å›å‚³å…§å®¹ä¸å®Œæ•´ï¼Œç¬¬ {retry_count + 1} æ¬¡çµæœç‚ºï¼š{result[:30]}...")
        except Exception as e:
            print(f"âš ï¸ GPT (æ“·å–å•é¡Œæ‘˜è¦) ç¬¬ {retry_count + 1} æ¬¡å‘¼å«å¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼š{e}")
        retry_count += 1
        await asyncio.sleep(2)

    print(f"â›” GPT (æ“·å–å•é¡Œæ‘˜è¦) åˆ†æå¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ {max_retry} æ¬¡")
    return "ï¼ˆAI (æ“·å–å•é¡Œæ‘˜è¦)æ“·å–å¤±æ•—ï¼‰"
# ğŸ§  ä¸»åŠŸèƒ½ï¼šæ“·å–å•é¡Œæ‘˜è¦ï¼ˆåŒæ¨£æ”¯æ´ source_idï¼‰

# ğŸ“Š å¿«å–å‘½ä¸­ç‡å ±å‘Š
def print_cache_report():
    if cache_total_queries == 0:
        print("ğŸ“Š æœ¬æ¬¡æœªåŸ·è¡Œä»»ä½•èªæ„å¿«å–æŸ¥è©¢ã€‚")
        return
    ratio = cache_hit_count / cache_total_queries * 100
    print(f"ğŸ“Š å¿«å–å‘½ä¸­ {cache_hit_count} / {cache_total_queries} ç­†ï¼Œå‘½ä¸­ç‡ {ratio:.1f}%")

# ğŸ”§ éåŒæ­¥å‘¼å«æœ¬åœ° Ollama API
async def call_ollama_model_async(prompt, model="phi3:mini", timeout=120):
    async with semaphore:
        url = "http://localhost:11434/api/generate"
        headers = {"Content-Type": "application/json"}

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": 50,
                "temperature": 0.5
            }
        }

        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
            async with session.post(url, json=payload, headers=headers) as response:
                response.raise_for_status()
                result = await response.json()
                return result.get("response", "").strip()
