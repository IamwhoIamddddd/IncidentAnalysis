from SmartScoring import is_actionable_resolution
import requests
import time

# ğŸ§  ä¸»åŠŸèƒ½ï¼šå¾æ®µè½ä¸­æŠ½å‡ºè§£æ±ºå»ºè­°å¥ï¼ˆå«ç©ºå€¼èˆ‡é—œéµå­—æª¢æŸ¥ï¼‰
def extract_resolution_suggestion(text, model="mistral"):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    # âœ… ä½¿ç”¨ SmartScoring æä¾›çš„è™•ç†å‹•ä½œåˆ¤æ–·ï¼ˆæ¨¡çµ„åˆ†å·¥ï¼‰
    ALWAYS_ANALYZE = True  # ğŸ‘ˆ è¨­æˆ True å°±è·³éç¯©é¸

    if not ALWAYS_ANALYZE and not is_actionable_resolution(text):
        print("â­ï¸ ç„¡èªæ„ç›¸è¿‘è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼š", text[:100])
        return "ï¼ˆæœªåµæ¸¬åˆ°å…·é«”è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼‰"



    # âœ… é™åˆ¶è¼¸å…¥é•·åº¦ç‚ºå‰ 3 è¡Œï¼Œé¿å… timeout
    lines = text.strip().splitlines()
    text_trimmed = "\n".join(lines[:3])

    # âœ… è‹±æ–‡ Promptï¼Œå›ºå®šé¢¨æ ¼è®“æ¨¡å‹åŠ å¿«ç”Ÿæˆ
    prompt = f"""Instruction: Summarize 1 actionable solution from the following.
    Reply "No recommendation" if none. Limit answer to 30 words.
    ---
    {text_trimmed}
    """

    try:
        return call_ollama_model(prompt, model)
    except Exception as e:
        print("âŒ åˆæ¬¡å‘¼å«å¤±æ•—ï¼Œå˜—è©¦é‡è©¦ä¸€æ¬¡...")
        time.sleep(2)
        try:
            return call_ollama_model(prompt, model)
        except Exception as e2:
            print("â›” GPT é›™æ¬¡å‘¼å«éƒ½å¤±æ•—ï¼š", e2)
            return "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"
        

def extract_problem_with_custom_prompt(text, model="mistral"):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    lines = text.strip().splitlines()
    text_trimmed = "\n".join(lines[:3])

    # ğŸ†• ä½¿ç”¨æ–°çš„ Promptï¼ˆ30 å­—å…§çš„ actionable solutionï¼‰
    prompt = f"""You're an assistant. Read the following incident note and summarize what issue or problem it describes, in one clear sentence.
Do not suggest a solution. Only summarize the problem.
Limit to 30 words.
---
{text_trimmed}
"""



    try:
        return call_ollama_model(prompt, model)
    except Exception as e:
        print("âŒ åˆæ¬¡å‘¼å«å¤±æ•—ï¼Œå˜—è©¦é‡è©¦ä¸€æ¬¡...")
        time.sleep(2)
        try:
            return call_ollama_model(prompt, model)
        except Exception as e2:
            print("â›” GPT é›™æ¬¡å‘¼å«éƒ½å¤±æ•—ï¼š", e2)
            return "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"



# ğŸ”§ åŸºç¤å‡½æ•¸ï¼šå‘¼å«æœ¬åœ° Ollama API
def call_ollama_model(prompt, model="mistral", timeout=60):
    url = "http://localhost:11434/api/generate"
    headers = {"Content-Type": "application/json"}

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "num_predict": 50,
            "temperature": 0.5
        }
    }

    response = requests.post(url, json=payload, headers=headers, timeout=timeout)
    response.raise_for_status()
    result = response.json()

    return result.get("response", "").strip()
