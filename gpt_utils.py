from SmartScoring import is_actionable_resolution
import aiohttp
import asyncio
import hashlib
import json
import os
from datetime import datetime
from sentence_transformers import SentenceTransformer, util
import numpy as np

MAX_CONCURRENCY = 10
DEFAULT_MODEL_SOLUTION = "mistral"
DEFAULT_MODEL_SUMMARY = "phi3:mini"
semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

# âœ… å¿«å–å„²å­˜ä½ç½®
CACHE_DIR = "cache"
CACHE_FILE = os.path.join(CACHE_DIR, "semantic_cache.json")
MAX_CACHE_SIZE = 3000

# çµ±è¨ˆç”¨è®Šæ•¸
cache_hit_count = 0
cache_total_queries = 0

# âœ… ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(CACHE_DIR, exist_ok=True)

# âœ… è¼‰å…¥èªæ„æ¨¡å‹
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# âœ… è¼‰å…¥å¿«å–è³‡æ–™
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        semantic_cache = json.load(f)
else:
    semantic_cache = []

# âœ… ç”¢ç”Ÿ hash key ç”¨æ–¼å®Œå…¨æ¯”å°
def make_hash(text):
    return hashlib.sha256(text.encode('utf-8')).hexdigest()

# âœ… èªæ„å¿«å–æŸ¥è©¢ï¼ˆå«æ–‡å­— hash + cosineï¼‰
# âœ… èªæ„å¿«å–æŸ¥è©¢ï¼ˆå«æ–‡å­— hash + cosineï¼‰
def find_semantic_cache(text, threshold=0.9, source_id=""):
    global cache_hit_count, cache_total_queries
    cache_total_queries += 1
    key = make_hash(text)

    print(f"ğŸ” [Cache] æŸ¥æ‰¾å¿«å–ä¸­... {source_id} hash={key[:8]} text='{text[:30]}'")

    for item in semantic_cache:
        if item.get("hash") == key:
            if item["response"] == "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰":
                print(f"ğŸš« [Cache] å‘½ä¸­ä½†ç‚ºå¤±æ•—å¿«å–ï¼ˆ{source_id}ï¼‰ï¼Œéœ€é€ GPT å†åˆ†æ")
                return None
            cache_hit_count += 1
            print(f"ğŸ¯ [Cache] å®Œæ•´å‘½ä¸­ï¼ï¼ˆ{source_id}ï¼‰")
            return item["response"]

    if len(semantic_cache) == 0:
        print(f"ğŸ“­ [Cache] ç„¡ä»»ä½•å¿«å–å¯æ¯”å°ï¼ˆcache ç©ºï¼‰ï¼ˆ{source_id}ï¼‰")
        return None

    try:
        query_vec = embedding_model.encode(text).astype(np.float32)
        all_vecs = np.array([item['embedding'] for item in semantic_cache], dtype=np.float32)
        sims = util.cos_sim(query_vec, all_vecs).flatten()
    except Exception as e:
        print(f"âŒ [Cache] èªæ„æ¯”å°æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼ˆ{source_id}ï¼‰ï¼š{e}")
        return None

    best_idx = int(np.argmax(sims))
    if sims[best_idx] > threshold:
        response = semantic_cache[best_idx]["response"]
        if response == "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰":
            print(f"ğŸš« [Cache] èªæ„ç›¸ä¼¼å‘½ä¸­ä½†ç‚ºå¤±æ•—å¿«å–ï¼ˆ{sims[best_idx]:.3f}ï¼‰ï¼ˆ{source_id}ï¼‰")
            return None
        cache_hit_count += 1
        print(f"ğŸ¯ [Cache] èªæ„ç›¸ä¼¼å‘½ä¸­ï¼ç›¸ä¼¼åº¦={sims[best_idx]:.3f}ï¼ˆ{source_id}ï¼‰")
        return response

    print(f"âŒ [Cache] ç„¡å‘½ä¸­ï¼Œå°‡é€ GPT æ“·å–æ–°è³‡æ–™ï¼ˆ{source_id}ï¼‰")
    return None




# âœ… å„²å­˜æ–°çš„å¿«å–ç´€éŒ„
def add_to_semantic_cache(text, response):
    key = make_hash(text)
    emb = embedding_model.encode(text).tolist()
    semantic_cache.append({
        "hash": key,
        "input": text,
        "embedding": emb,
        "response": response,
        "createdAt": datetime.now().isoformat()
    })
    if len(semantic_cache) > MAX_CACHE_SIZE:
        semantic_cache.pop(0)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(semantic_cache, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ [Cache] å·²å„²å­˜å¿«å–ï¼šhash={key[:8]} text='{text[:30]}'")

# ğŸ§  ä¸»åŠŸèƒ½ï¼šå¾æ®µè½ä¸­æŠ½å‡ºè§£æ±ºå»ºè­°å¥ï¼ˆå«ç©ºå€¼èˆ‡å¿«å–ï¼‰
# ğŸ§  ä¸»åŠŸèƒ½ï¼šå¾æ®µè½ä¸­æŠ½å‡ºè§£æ±ºå»ºè­°å¥ï¼ˆå«ç©ºå€¼èˆ‡å¿«å–ï¼‰
async def extract_resolution_suggestion(text, model="mistral", source_id=""):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    ALWAYS_ANALYZE = True
    if not ALWAYS_ANALYZE and not is_actionable_resolution(text):
        print(f"â­ï¸ ç„¡èªæ„ç›¸è¿‘è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼ˆ{source_id}ï¼‰ï¼š", text[:100])
        return "ï¼ˆæœªåµæ¸¬åˆ°å…·é«”è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼‰"

    lines = text.strip().splitlines()
    text_trimmed = "\n".join(lines[:3])
    print(f"ğŸ” [GPT] æº–å‚™æ“·å–è§£æ±ºå»ºè­°ï¼š{text_trimmed[:30]}...ï¼ˆ{source_id}ï¼‰")

    cached = find_semantic_cache(text_trimmed, source_id=source_id)
    if cached:
        print(f"ğŸ¯ å¿«å–å‘½ä¸­ï¼šç•¥é GPT åˆ†æï¼ˆ{source_id}ï¼‰")
        return cached

    prompt = f"""Instruction: Summarize 1 actionable solution from the following.\nReply \"No recommendation\" if none. Limit answer to 30 words.\n---\n{text_trimmed}"""

    try:
        result = await call_ollama_model_async(prompt, model)
        print(f"âœ… GPT ç¬¬ä¸€æ¬¡å‘¼å«æˆåŠŸï¼ˆ{source_id}ï¼‰")
        add_to_semantic_cache(text_trimmed, result)
        return result
    except Exception as e1:
        print(f"âš ï¸ GPT ç¬¬ä¸€æ¬¡å‘¼å«å¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼š{e1}")
        print(f"ğŸ” æ­£åœ¨é‡æ–°å˜—è©¦ç¬¬ 2 æ¬¡ GPT å‘¼å«...ï¼ˆ{source_id}ï¼‰")

        await asyncio.sleep(2)
        try:
            result = await call_ollama_model_async(prompt, model)
            print(f"âœ… GPT ç¬¬äºŒæ¬¡å‘¼å«æˆåŠŸï¼ˆ{source_id}ï¼‰")
            add_to_semantic_cache(text_trimmed, result)
            return result
        except Exception as e2:
            print(f"â›” GPT ç¬¬äºŒæ¬¡å‘¼å«ä¹Ÿå¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼š{e2}")
            return "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"




# ğŸ§  ä¸»åŠŸèƒ½ï¼šæ“·å–å•é¡Œæ‘˜è¦ï¼ˆåŒæ¨£æ”¯æ´ source_idï¼‰
async def extract_problem_with_custom_prompt(text, model="phi3:mini", source_id=""):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    lines = text.strip().splitlines()
    text_trimmed = "\n".join(lines[:3])
    print(f"ğŸ” [GPT] æº–å‚™æ“·å–å•é¡Œæ‘˜è¦ï¼š{text_trimmed[:30]}...ï¼ˆ{source_id}ï¼‰")

    cached = find_semantic_cache(text_trimmed, source_id=source_id)
    if cached:
        print(f"ğŸ¯ å¿«å–å‘½ä¸­ï¼šç•¥é GPT åˆ†æï¼ˆ{source_id}ï¼‰")
        return cached

    prompt = f"""You're an assistant. Read the following incident note and summarize what issue or problem it describes, in one clear sentence.\nDo not suggest a solution. Only summarize the problem.\nLimit to 30 words.\n---\n{text_trimmed}"""

    try:
        result = await call_ollama_model_async(prompt, model)
        print(f"âœ… GPT ç¬¬ä¸€æ¬¡å‘¼å«æˆåŠŸï¼ˆ{source_id}ï¼‰")
        add_to_semantic_cache(text_trimmed, result)
        return result
    except Exception as e1:
        print(f"âš ï¸ GPT ç¬¬ä¸€æ¬¡å‘¼å«å¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼š{e1}")
        print(f"ğŸ” æ­£åœ¨é‡æ–°å˜—è©¦ç¬¬ 2 æ¬¡ GPT å‘¼å«...ï¼ˆ{source_id}ï¼‰")

        await asyncio.sleep(2)
        try:
            result = await call_ollama_model_async(prompt, model)
            print(f"âœ… GPT ç¬¬äºŒæ¬¡å‘¼å«æˆåŠŸï¼ˆ{source_id}ï¼‰")
            add_to_semantic_cache(text_trimmed, result)
            return result
        except Exception as e2:
            print(f"â›” GPT ç¬¬äºŒæ¬¡å‘¼å«ä¹Ÿå¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼š{e2}")
            return "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"


# ğŸ“Š å¿«å–å‘½ä¸­ç‡å ±å‘Š
def print_cache_report():
    if cache_total_queries == 0:
        print("ğŸ“Š æœ¬æ¬¡æœªåŸ·è¡Œä»»ä½•èªæ„å¿«å–æŸ¥è©¢ã€‚")
        return
    ratio = cache_hit_count / cache_total_queries * 100
    print(f"ğŸ“Š å¿«å–å‘½ä¸­ {cache_hit_count} / {cache_total_queries} ç­†ï¼Œå‘½ä¸­ç‡ {ratio:.1f}%")

# ğŸ”§ éåŒæ­¥å‘¼å«æœ¬åœ° Ollama API
async def call_ollama_model_async(prompt, model="phi3:mini", timeout=120):
    async with semaphore:
        url = "http://localhost:11434/api/generate"
        headers = {"Content-Type": "application/json"}

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": 50,
                "temperature": 0.5
            }
        }

        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
            async with session.post(url, json=payload, headers=headers) as response:
                response.raise_for_status()
                result = await response.json()
                return result.get("response", "").strip()
