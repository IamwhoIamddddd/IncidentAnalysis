from SmartScoring import is_actionable_resolution
import aiohttp
import asyncio
import hashlib
import json
import os
from datetime import datetime
from sentence_transformers import SentenceTransformer, util
import numpy as np
import re
import time


POWERAUTOMATEANALYSIS_URL = "https://prod-68.southeastasia.logic.azure.com:443/workflows/de1c05e9860c4296873b019585edcb7f/triggers/manual/paths/invoke?api-version=2016-06-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=TlEzbTE_tkcvnKUuiziBXthKuvW7ONK6VP4bxi_-_bY"  # ğŸ” è«‹æ›æˆä½ çš„å¯¦éš›ç¶²å€


MAX_CONCURRENCY = 10
CHAR_LIMIT_OLLAMA_RESOLUTION = 16000  # æˆ–ä½ æƒ³è¦çš„å­—æ•¸é™åˆ¶
CHAR_LIMIT_OLLAMA_SUMMARY = 16000  # æˆ–ä½ æƒ³è¦çš„å­—æ•¸é™åˆ¶
CHAR_LIMIT_AIBUILDER_RESOLUTION = 128000  # æˆ–ä½ æƒ³è¦çš„å­—æ•¸é™åˆ¶
CHAR_LIMIT_AIBUILDER_SUMMARY = 128000  # æˆ–ä½ æƒ³è¦çš„å­—æ•¸é™åˆ¶

DEFAULT_MODEL_SOLUTION = "command-r7b:latest"
DEFAULT_MODEL_SUMMARY = "command-r7b:latest"

cache_hit_count_resolution = 0
cache_total_queries_resolution = 0
cache_hit_count_summary = 0
cache_total_queries_summary = 0



PA_MAX_CONCURRENCY = 50  # ä½ æƒ³è¦åŒæ™‚é€å¹¾ç­† request
pa_semaphore = asyncio.Semaphore(PA_MAX_CONCURRENCY)
semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

# âœ… å¿«å–å„²å­˜ä½ç½®
CACHE_DIR = "cache"
CACHE_FILE_RESOLUTION = os.path.join(CACHE_DIR, "semantic_cache_resolution.json")
CACHE_FILE_SUMMARY = os.path.join(CACHE_DIR, "semantic_cache_summary.json") 
# âœ… å¿«å–å¤§å°é™åˆ¶
MAX_CACHE_SIZE = 3000

# çµ±è¨ˆç”¨è®Šæ•¸
cache_hit_count = 0
cache_total_queries = 0

# âœ… ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(CACHE_DIR, exist_ok=True)

# âœ… è¼‰å…¥èªæ„æ¨¡å‹
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# âœ… è¼‰å…¥å¿«å–è³‡æ–™
if os.path.exists(CACHE_FILE_RESOLUTION):
    with open(CACHE_FILE_RESOLUTION, "r", encoding="utf-8") as f:
        semantic_cache_resolution = json.load(f)
else:
    semantic_cache_resolution = []

if os.path.exists(CACHE_FILE_SUMMARY):
    with open(CACHE_FILE_SUMMARY, "r", encoding="utf-8") as f:
        semantic_cache_summary = json.load(f)
else:
    semantic_cache_summary = []


def clean_text(text):
    return re.sub(r"[\x00-\x1f\x7f-\x9f]", "", text)  # å»é™¤æ§åˆ¶ç¬¦è™Ÿï¼ˆemoji å¯ä¿ç•™ï¼‰

# âœ… ç”¢ç”Ÿ hash key ç”¨æ–¼å®Œå…¨æ¯”å°
def make_hash(text):
    return hashlib.sha256(text.encode('utf-8')).hexdigest()



# âœ… èªæ„å¿«å–æŸ¥è©¢ï¼ˆå«æ–‡å­— hash + cosineï¼‰
def find_semantic_cache(text, kind="resolution", threshold=0.9, source_id=""):
    global cache_hit_count_resolution, cache_total_queries_resolution
    global cache_hit_count_summary, cache_total_queries_summary

    key = make_hash(text)

    if kind == "resolution":
        cache = semantic_cache_resolution
        cache_total_queries_resolution += 1
    elif kind == "summary":
        cache = semantic_cache_summary
        cache_total_queries_summary += 1
    else:
        raise ValueError("Unknown cache kind: must be 'resolution' or 'summary'")

    print(f"ğŸ” [Cache:{kind}] æŸ¥æ‰¾å¿«å–ä¸­... {source_id} hash={key[:8]} text='{text[:30]}'")

    for item in cache:
        if item.get("hash") == key:
            if item["response"] == "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰":
                print(f"ğŸš« [Cache:{kind}] å‘½ä¸­ä½†ç‚ºå¤±æ•—å¿«å–ï¼ˆ{source_id}ï¼‰ï¼Œéœ€é€ GPT å†åˆ†æ")
                return None
            if kind == "resolution":
                cache_hit_count_resolution += 1
            elif kind == "summary":
                cache_hit_count_summary += 1
            print(f"ğŸ¯ [Cache:{kind}] å®Œæ•´å‘½ä¸­ï¼ï¼ˆ{source_id}ï¼‰")
            return item["response"]

    if len(cache) == 0:
        print(f"ğŸ“­ [Cache:{kind}] ç„¡ä»»ä½•å¿«å–å¯æ¯”å°ï¼ˆcache ç©ºï¼‰ï¼ˆ{source_id}ï¼‰")
        return None

    try:
        query_vec = embedding_model.encode(text).astype(np.float32)
        all_vecs = np.array([item['embedding'] for item in cache], dtype=np.float32)
        sims = util.cos_sim(query_vec, all_vecs).flatten()
    except Exception as e:
        print(f"âŒ [Cache:{kind}] èªæ„æ¯”å°æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼ˆ{source_id}ï¼‰ï¼š{e}")
        return None

    best_idx = int(np.argmax(sims))
    if sims[best_idx] > threshold:
        response = cache[best_idx]["response"]
        if response == "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰":
            print(f"ğŸš« [Cache:{kind}] èªæ„ç›¸ä¼¼å‘½ä¸­ä½†ç‚ºå¤±æ•—å¿«å–ï¼ˆ{sims[best_idx]:.3f}ï¼‰ï¼ˆ{source_id}ï¼‰")
            return None
        if kind == "resolution":
            cache_hit_count_resolution += 1
        elif kind == "summary":
            cache_hit_count_summary += 1
        print(f"ğŸ¯ [Cache:{kind}] èªæ„ç›¸ä¼¼å‘½ä¸­ï¼ç›¸ä¼¼åº¦={sims[best_idx]:.3f}ï¼ˆ{source_id}ï¼‰")
        return response

    print(f"âŒ [Cache:{kind}] ç„¡å‘½ä¸­ï¼Œå°‡é€ GPT æ“·å–æ–°è³‡æ–™ï¼ˆ{source_id}ï¼‰")
    return None



# âœ… å„²å­˜æ–°çš„å¿«å–ç´€éŒ„
def add_to_semantic_cache(text, response, kind="resolution"):
    key = make_hash(text)
    emb = embedding_model.encode(text).tolist()

    if kind == "resolution":
        cache = semantic_cache_resolution
        cache_file = CACHE_FILE_RESOLUTION
    elif kind == "summary":
        cache = semantic_cache_summary
        cache_file = CACHE_FILE_SUMMARY
    else:
        raise ValueError("Unknown cache kind: must be 'resolution' or 'summary'")

    cache.append({
        "hash": key,
        "input": text,
        "embedding": emb,
        "response": response,
        "createdAt": datetime.now().isoformat()
    })
    if len(cache) > MAX_CACHE_SIZE:
        cache.pop(0)
    with open(cache_file, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ [Cache:{kind}] å·²å„²å­˜å¿«å–ï¼šhash={key[:8]} text='{text[:1000]}'")


# ğŸ§  ä¸»åŠŸèƒ½ï¼šå¾æ®µè½ä¸­æŠ½å‡ºè§£æ±ºå»ºè­°å¥ï¼ˆå«ç©ºå€¼èˆ‡å¿«å–ï¼‰

def get_gpt_prompt_and_model(task="solution"):
    MAP_PATH = os.path.join("gpt_data", "gpt_prompt_map.json")
    try:
        with open(MAP_PATH, "r", encoding="utf-8") as f:
            config = json.load(f)
        if config.get(task):
            prompt = config[task].get("prompt") or ""
            model = config[task].get("model") or ""
            # æª¢æŸ¥æ˜¯å¦ prompt/model ç¼ºå¤±
            if not prompt or not model:
                print(f"âš ï¸ [PromptMap] {task} çš„ prompt æˆ– model æ¬„ä½ç‚ºç©ºï¼Œå·²å•Ÿç”¨é è¨­å€¼ï¼")
            return prompt, model
        else:
            print(f"âš ï¸ [PromptMap] {task} è¨­å®šä¸å­˜åœ¨ï¼Œå·²å•Ÿç”¨é è¨­å€¼ï¼")
    except Exception as e:
        print(f"âŒ [PromptMap] è®€å– {task} è¨­å®šå¤±æ•—ï¼š{e}")
    # å›å‚³é è¨­å€¼
    if task == "solution":
        print("âš ï¸ [PromptMap] ä½¿ç”¨ solution é è¨­ prompt/model")
        return "è«‹å¾ä»¥ä¸‹æ®µè½æå–ä¸€å€‹å…·é«”çš„è¡Œå‹•å»ºè­°", DEFAULT_MODEL_SOLUTION
    elif task == "ai_summary":
        print("âš ï¸ [PromptMap] ä½¿ç”¨ ai_summary é è¨­ prompt/model")
        return "è«‹ç”¨ä¸€å¥è©±æè¿°äº‹ä»¶æ˜¯ä»€éº¼", DEFAULT_MODEL_SUMMARY
    else:
        print(f"âš ï¸ [PromptMap] æœªçŸ¥ç”¨é€” {task}ï¼Œå›å‚³ç©ºå€¼")
        return "", ""


async def extract_resolution_suggestion(text, model=DEFAULT_MODEL_SOLUTION, source_id=""):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    ALWAYS_ANALYZE = True
    if not ALWAYS_ANALYZE and not is_actionable_resolution(text):
        print(f"â­ï¸ ç„¡èªæ„ç›¸è¿‘è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼ˆ{source_id}ï¼‰ï¼š", text[:100])
        return "ï¼ˆæœªåµæ¸¬åˆ°å…·é«”è§£æ³•èªæ°£ï¼Œç•¥éåˆ†æï¼‰"

    # è®€å–ç›®å‰çš„ prompt èˆ‡ model è¨­å®š
    custom_prompt, custom_model = get_gpt_prompt_and_model("solution")
    model = model or custom_model

    text_trimmed = text.strip()[:CHAR_LIMIT_OLLAMA_RESOLUTION]
    print(f"ğŸ” [GPT] æº–å‚™æ“·å–è§£æ±ºå»ºè­°ï¼š{text_trimmed[:500]}...ï¼ˆ{source_id}ï¼‰")

    # âœ… é€™è£¡åŠ  kind
    cached = find_semantic_cache(text_trimmed, kind="resolution", source_id=source_id)
    if cached:
        print(f"ğŸ¯ å¿«å–å‘½ä¸­ï¼šç•¥é GPT åˆ†æï¼ˆ{source_id}ï¼‰")
        return cached

    prompt = f"{custom_prompt}\n---\n{text_trimmed}"
    print(f"ğŸ“ [GPT] æœ€å¾Œè¼¸å…¥é€²çš„promptï¼š{prompt[:600]}...")
    max_retry = 5
    retry_count = 0

    while retry_count < max_retry:
        try:
            result = await call_ollama_model_async(prompt, model)
            if result and "æ“·å–å¤±æ•—" not in result and "æœªåµæ¸¬" not in result:
                print(f"âœ… GPT (æ“·å–è§£æ±ºå»ºè­°)ç¬¬ {retry_count + 1} æ¬¡å‘¼å«æˆåŠŸï¼ˆ{source_id}ï¼‰")
                # âœ… é€™è£¡åŠ  kind
                add_to_semantic_cache(text_trimmed, result, kind="resolution")
                return result
            else:
                print(f"âš ï¸ GPT å›å‚³å…§å®¹ä¸å®Œæ•´ï¼Œç¬¬ {retry_count + 1} æ¬¡çµæœç‚ºï¼š{result[:30]}...")
        except Exception as e:
            print(f"âš ï¸ GPT å‘¼å«å¤±æ•—ï¼ˆç¬¬ {retry_count + 1} æ¬¡ï¼‰ï¼ˆ{source_id}ï¼‰ï¼š{e}")
        retry_count += 1
        await asyncio.sleep(2)

    print(f"â›” GPT åˆ†æå¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ {max_retry} æ¬¡")
    return "ï¼ˆAI (æ“·å–è§£æ±ºå»ºè­°)æ“·å–å¤±æ•—ï¼‰"
# ğŸ§  ä¸»åŠŸèƒ½ï¼šæ“·å–å•é¡Œæ‘˜è¦ï¼ˆåŒæ¨£æ”¯æ´ source_idï¼‰


async def extract_problem_with_custom_prompt(text, model=None, source_id=""):
    if not isinstance(text, str) or not text.strip():
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    # è®€å–ç›®å‰çš„ prompt èˆ‡ model è¨­å®š
    custom_prompt, custom_model = get_gpt_prompt_and_model("ai_summary")
    model = model or custom_model

    text_trimmed = text.strip()[:CHAR_LIMIT_OLLAMA_SUMMARY]
    
    print(f"ğŸ” [GPT] æº–å‚™æ“·å–å•é¡Œæ‘˜è¦ï¼š{text_trimmed[:500]}...ï¼ˆ{source_id}ï¼‰")

    # âœ… æŸ¥è©¢ summary cache
    cached = find_semantic_cache(text_trimmed, kind="summary", source_id=source_id)
    if cached:
        print(f"ğŸ¯ å¿«å–å‘½ä¸­ï¼šç•¥é GPT (æ“·å–å•é¡Œæ‘˜è¦) åˆ†æï¼ˆ{source_id}ï¼‰")
        return cached

    prompt = f"{custom_prompt}\n---\n{text_trimmed}"
    print(f"ğŸ“ [GPT] æœ€å¾Œè¼¸å…¥é€²çš„promptï¼š{prompt[:600]}...")
    max_retry = 5
    retry_count = 0

    while retry_count < max_retry:
        try:
            result = await call_ollama_model_async(prompt, model)
            if result and "æ“·å–å¤±æ•—" not in result and "æœªåµæ¸¬" not in result:
                print(f"âœ… GPT (æ“·å–å•é¡Œæ‘˜è¦) ç¬¬ {retry_count + 1} æ¬¡å‘¼å«æˆåŠŸï¼ˆ{source_id}ï¼‰")
                # âœ… å„²å­˜é€² summary cache
                add_to_semantic_cache(text_trimmed, result, kind="summary")
                return result
            else:
                print(f"âš ï¸ GPT å›å‚³å…§å®¹ä¸å®Œæ•´ï¼Œç¬¬ {retry_count + 1} æ¬¡çµæœç‚ºï¼š{result[:30]}...")
        except Exception as e:
            print(f"âš ï¸ GPT (æ“·å–å•é¡Œæ‘˜è¦) ç¬¬ {retry_count + 1} æ¬¡å‘¼å«å¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼š{e}")
        retry_count += 1
        await asyncio.sleep(2)

    print(f"â›” GPT (æ“·å–å•é¡Œæ‘˜è¦) åˆ†æå¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ {max_retry} æ¬¡")
    return "ï¼ˆAI (æ“·å–å•é¡Œæ‘˜è¦)æ“·å–å¤±æ•—ï¼‰"



# ğŸ“Š å¿«å–å‘½ä¸­ç‡å ±å‘Š
def print_cache_report():
    print("=== Resolution Cache å‘½ä¸­å ±å‘Š ===")
    if cache_total_queries_resolution == 0:
        print("ğŸ“Š Resolution æœ¬æ¬¡æœªåŸ·è¡Œä»»ä½•èªæ„å¿«å–æŸ¥è©¢ã€‚")
    else:
        ratio = cache_hit_count_resolution / cache_total_queries_resolution * 100
        print(f"ğŸ“Š Resolution å¿«å–å‘½ä¸­ {cache_hit_count_resolution} / {cache_total_queries_resolution} ç­†ï¼Œå‘½ä¸­ç‡ {ratio:.1f}%")
    
    print("=== Summary Cache å‘½ä¸­å ±å‘Š ===")
    if cache_total_queries_summary == 0:
        print("ğŸ“Š Summary æœ¬æ¬¡æœªåŸ·è¡Œä»»ä½•èªæ„å¿«å–æŸ¥è©¢ã€‚")
    else:
        ratio = cache_hit_count_summary / cache_total_queries_summary * 100
        print(f"ğŸ“Š Summary å¿«å–å‘½ä¸­ {cache_hit_count_summary} / {cache_total_queries_summary} ç­†ï¼Œå‘½ä¸­ç‡ {ratio:.1f}%")



# ğŸ”§ éåŒæ­¥å‘¼å«æœ¬åœ° Ollama API
async def call_ollama_model_async(prompt, model="command-r7b:latest", timeout=600):
    async with semaphore:
        url = "http://localhost:11434/api/generate"
        headers = {"Content-Type": "application/json"}
        print(f"ğŸ“ [GPT] ä½¿ç”¨æ¨¡å‹ {model}")

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": 50,
                "temperature": 0.5
            }
        }

        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
            async with session.post(url, json=payload, headers=headers) as response:
                response.raise_for_status()
                result = await response.json()
                return result.get("response", "").strip()
            
        
# ğŸ“ ä¸»åŠŸèƒ½ï¼šåˆ†æè§£æ±ºå»ºè­°èˆ‡å•é¡Œæ‘˜è¦ï¼Œå…ˆä½¿ç”¨ Power Automateï¼Œå†ä½¿ç”¨æœ¬åœ°æ¨¡å‹ä½œç‚º fallback    
async def analyze_with_ai_builder_then_fallback(resolution_text, summary_input, source_id=""):
    if not resolution_text.strip() and not summary_input.strip():
        print(f"[{time.strftime('%X')}] â›”ï¸ ç©ºç™½è¼¸å…¥ï¼Œç•¥éåˆ†æï¼ˆ{source_id}ï¼‰")
        return "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰", "ï¼ˆç„¡åŸå§‹æè¿°ï¼‰"

    res_trimmed = clean_text(resolution_text.strip())[:CHAR_LIMIT_AIBUILDER_RESOLUTION]
    sum_trimmed = clean_text(summary_input.strip())[:CHAR_LIMIT_AIBUILDER_SUMMARY]
    print(f"[{time.strftime('%X')}] ğŸ“ æº–å‚™åˆ†æï¼šè§£æ±ºå»ºè­°='{res_trimmed[:50]}' æ‘˜è¦='{sum_trimmed[:50]}'ï¼ˆ{source_id}ï¼‰")
    
    # 1ï¸âƒ£ æŸ¥æ‰¾æ™‚åˆ†æµ
    cached_solution = find_semantic_cache(res_trimmed, kind="resolution", source_id=source_id + "-sol")
    cached_summary = find_semantic_cache(sum_trimmed, kind="summary", source_id=source_id + "-sum")
    if cached_solution and cached_summary:
        print(f"[{time.strftime('%X')}] ğŸ¯ å¿«å–å‘½ä¸­ï¼ˆ{source_id}ï¼‰ï¼Œç•¥é Power Automate èˆ‡æœ¬åœ°åˆ†æ")
        print(f"[{time.strftime('%X')}] ğŸ“ å¿«å–è§£æ±ºå»ºè­°ï¼š'{cached_solution}'")
        print(f"[{time.strftime('%X')}] ğŸ“ å¿«å–å•é¡Œæ‘˜è¦ï¼š'{cached_summary}'")
        return cached_solution, cached_summary
    
    custom_solution_prompt, custom_solution_model = get_gpt_prompt_and_model("solution")
    custom_summary_prompt, custom_summary_model = get_gpt_prompt_and_model("ai_summary")

    MAX_RETRY = 5
    for attempt in range(1, MAX_RETRY + 1):
        try:
            print(f"[{time.strftime('%X')}] âš¡ï¸ Power Automate å˜—è©¦ç¬¬ {attempt} æ¬¡ï¼ˆ{source_id}ï¼‰...")
            async with pa_semaphore:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        POWERAUTOMATEANALYSIS_URL,
                        json={
                            "resolution": res_trimmed, 
                            "summary": sum_trimmed,
                            "solutionPrompt": custom_solution_prompt,
                            "summaryPrompt": custom_summary_prompt  
                        },
                        timeout=360
                    ) as response:
                        print(f"[{time.strftime('%X')}] â© Power Automate response status: {response.status} ({source_id})")
                        response_text = await response.text()
                        print(f"[{time.strftime('%X')}] âª Power Automate raw response: {response_text} ({source_id})")
                        if response.status == 200:
                            try:
                                result = json.loads(response_text)
                            except Exception as e:
                                print(f"[{time.strftime('%X')}] âŒ å›å‚³é JSON æ ¼å¼ï¼({source_id}) err={e}")
                                continue
                            solution = result.get("solution", "").strip()
                            summary = result.get("aiSummary", "").strip()
                            print(f"[{time.strftime('%X')}] ğŸ“ è§£æå‡º solution: '{solution[:50]}' summary: '{summary[:50]}' ({source_id})")

                            if solution and summary:
                                print(f"[{time.strftime('%X')}] âœ… Power Automate æˆåŠŸï¼ˆ{source_id}ï¼‰")
                                # 2ï¸âƒ£ å„²å­˜æ™‚åˆ†æµ
                                add_to_semantic_cache(res_trimmed, solution, kind="resolution")
                                add_to_semantic_cache(sum_trimmed, summary, kind="summary")
                                return solution, summary
                            else:
                                print(f"[{time.strftime('%X')}] âš ï¸ Power Automate å›å‚³å…§å®¹ä¸å®Œæ•´ï¼ˆ{source_id}ï¼‰: {result}")
                        else:
                            print(f"[{time.strftime('%X')}] âŒ Power Automate HTTP status: {response.status} ({source_id}) | Text: {response_text}")
        except Exception as e:
            print(f"[{time.strftime('%X')}] âš ï¸ Power Automate ç™¼ç”ŸéŒ¯èª¤ï¼ˆ{source_id}ï¼‰ç¬¬ {attempt} æ¬¡ï¼š{e}")
        await asyncio.sleep(2)

    print(f"[{time.strftime('%X')}] ğŸ” Power Automate å…¨éƒ¨å¤±æ•—ï¼Œfallback åˆ°æœ¬åœ°æ¨¡å‹ï¼ˆ{source_id}ï¼‰")
    try:
        ai_suggestion, ai_summary = await asyncio.gather(
            extract_resolution_suggestion(res_trimmed, source_id=source_id + "-sol"),
            extract_problem_with_custom_prompt(sum_trimmed, source_id=source_id + "-sum")
        )
        print(f"[{time.strftime('%X')}] ğŸŸ© fallback æœ¬åœ°æ¨¡å‹å®Œæˆ solution: '{ai_suggestion[:50]}' summary: '{ai_summary[:50]}' ({source_id})")
        return ai_suggestion, ai_summary
    except Exception as e:
        print(f"[{time.strftime('%X')}] âŒ fallback æœ¬åœ°æ¨¡å‹ä¹Ÿå¤±æ•—ï¼ˆ{source_id}ï¼‰ï¼š{e}")
        return "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰", "ï¼ˆAI æ“·å–å¤±æ•—ï¼‰"
