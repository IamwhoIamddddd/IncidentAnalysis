from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
from keybert import KeyBERT
import spacy
import nltk
import pandas as pd
# åŒ¯å…¥ os æ¨¡çµ„è™•ç†æª”æ¡ˆèˆ‡è·¯å¾‘
import os
import sys  # âœ… æ–°å¢ž sys åŒ¯å…¥ä»¥æ”¯æ´ PyInstaller æ‰“åŒ…
import requests
import torch  # âœ… æ–°å¢ž torch åŒ¯å…¥ä»¥æ”¯æ´ç›¸ä¼¼åº¦æ¯”å°
import time
import json

t_start = time.time()
print("ðŸ”¥ å•Ÿå‹•æ™‚é–“è¨ºæ–·ä¸­...")

# ========== âœ… è¼‰å…¥èªžæ„æ¨¡åž‹ ==========
t_model_load = time.time()

def get_model_path(folder_or_name):
    base = getattr(sys, '_MEIPASS', os.path.abspath('.'))
    path = os.path.join(base, 'models', folder_or_name)
    if not os.path.exists(path):
        path = os.path.join(os.path.abspath('.'), 'models', folder_or_name)
    return path

bert_model = SentenceTransformer(get_model_path('paraphrase-MiniLM-L6-v2'))
print(f"ðŸ“¦ BERT æ¨¡åž‹è¼‰å…¥å®Œæˆï¼Œç”¨æ™‚ï¼š{time.time() - t_model_load:.2f} ç§’")

# ========== âœ… åˆå§‹åŒ– KeyBERT ==========
t_keybert = time.time()
keybert_model = KeyBERT(bert_model)
print(f"ðŸ§  KeyBERT åˆå§‹åŒ–å®Œæˆï¼Œç”¨æ™‚ï¼š{time.time() - t_keybert:.2f} ç§’")

# ========== âœ… è¼‰å…¥ spaCy æ¨¡åž‹ ==========
t_spacy = time.time()
nlp = spacy.load("en_core_web_sm")
print(f"ðŸ§¬ spaCy æ¨¡åž‹è¼‰å…¥å®Œæˆï¼Œç”¨æ™‚ï¼š{time.time() - t_spacy:.2f} ç§’")

t_nltk = time.time()
nltk.download('punkt')
nltk.download('stopwords')


print(f"ðŸ“š NLTK åˆå§‹åŒ–å®Œæˆï¼Œç”¨æ™‚ï¼š{time.time() - t_nltk:.2f} ç§’")

# ========== âœ… ç¸½çµ ==========
print(f"ðŸš€ æ¨¡åž‹åˆå§‹åŒ–ç¸½è€—æ™‚ï¼šç´„ {time.time() - t_start:.2f} ç§’")



# æŒ‡å®š data è³‡æ–™å¤¾è·¯å¾‘
DATA_DIR = "data/sentences"



def load_examples_from_json(filepath):
    if not os.path.exists(filepath):
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨ï¼š{filepath}")
        return []
    with open(filepath, encoding="utf-8") as f:
        try:
            data = json.load(f)
            if isinstance(data, list):
                return data
            elif isinstance(data, dict):
                # å¦‚æžœæ˜¯ dictï¼Œå¯èƒ½è¦æŒ‡å®š key
                print(f"âš ï¸ æª”æ¡ˆå…§å®¹ç‚º dictï¼š{filepath}ï¼Œè«‹æª¢æŸ¥çµæ§‹")
                return []
            else:
                print(f"âš ï¸ æª”æ¡ˆå…§å®¹ä¸æ˜¯ list æˆ– dictï¼š{filepath}")
                return []
        except Exception as e:
            print(f"âŒ è®€å– json å¤±æ•—ï¼š{e}")
            return []
        

def load_embeddings(tag):
    examples = load_examples_from_json(os.path.join(DATA_DIR, f"{tag}.json"))
    print(f"ðŸŸ¦ [{tag}] æœ¬æ¬¡è¼‰å…¥èªžå¥ {len(examples)} ç­†")
    if examples:
        print(f"    å‰ 3 å¥ï¼š{examples[:3]}")
        print(f"    å€’æ•¸ 3 å¥ï¼š{examples[-3:]}")
    else:
        print("    âš ï¸ æ²’æœ‰èªžå¥ï¼ˆç©ºæ¸…å–®ï¼‰")
    if len(examples) == 0:
        print(f"âš ï¸ {tag} examples ç‚ºç©º")
        return [], None
    embeddings = bert_model.encode(examples, convert_to_tensor=True)
    print(f"âœ… {tag} embedding shapeï¼š{embeddings.shape}")
    return examples, embeddings

# ========== âœ… è¼‰å…¥èªžå¥æ¨£æœ¬ ==========


# é«˜é¢¨éšªèªžå¥æ¨£æœ¬
high_risk_examples = load_examples_from_json(os.path.join(DATA_DIR, "high_risk.json"))
print(f"âœ… è¼‰å…¥é«˜é¢¨éšªèªžå¥ï¼š{len(high_risk_examples)} ç­†ï¼Œå‰ 3 ç­†ï¼š{high_risk_examples[:3]}ï¼Œå€’æ•¸ 3 ç­†ï¼š{high_risk_examples[-3:]}")
print("high_risk_examples:", len(high_risk_examples))
high_risk_embeddings = bert_model.encode(high_risk_examples, convert_to_tensor=True)
print(f"âœ… é«˜é¢¨éšª embedding shapeï¼š{high_risk_embeddings.shape}")


# å‡ç´šè™•ç†èªžå¥æ¨£æœ¬
escalation_examples = load_examples_from_json(os.path.join(DATA_DIR, "escalate.json"))
print("escalation_examples:", len(escalation_examples))

print(f"âœ… è¼‰å…¥å‡ç´šè™•ç†èªžå¥ï¼š{len(escalation_examples)} ç­†ï¼Œå‰ 3 ç­†ï¼š{escalation_examples[:3]}ï¼Œå€’æ•¸ 3 ç­†ï¼š{escalation_examples[-3:]}")

escalation_embeddings = bert_model.encode(escalation_examples, convert_to_tensor=True)
print(f"âœ… å‡ç´šè™•ç† embedding shapeï¼š{escalation_embeddings.shape}")


# å¤šäººå—å½±éŸ¿èªžå¥æ¨£æœ¬
multi_user_examples = load_examples_from_json(os.path.join(DATA_DIR, "multi_user.json"))
print("multi_user_examples:", len(multi_user_examples))

print(f"âœ… è¼‰å…¥å¤šäººå—å½±éŸ¿èªžå¥ï¼š{len(multi_user_examples)} ç­†ï¼Œå‰ 3 ç­†ï¼š{multi_user_examples[:3]}ï¼Œå€’æ•¸ 3 ç­†ï¼š{multi_user_examples[-3:]}")


multi_user_embeddings = bert_model.encode(multi_user_examples, convert_to_tensor=True)
print(f"âœ… å¤šäººå—å½±éŸ¿ embedding shapeï¼š{multi_user_embeddings.shape}")


# ---------- èªžæ„åˆ¤æ–·å‡½å¼ ----------
def is_high_risk(text, examples, embeddings):
    if not examples or embeddings is None or len(examples) == 0:
        print("  [é«˜é¢¨éšªæ¯”å°] ç„¡èªžå¥åº«ï¼Œä¸åŸ·è¡Œæ¯”å°")
        return 0
    test_emb = bert_model.encode([text], convert_to_tensor=True)
    sims = util.cos_sim(test_emb, embeddings).flatten()
    max_idx = int(sims.argmax())
    max_score = sims[max_idx].item()
    print(f"  [é«˜é¢¨éšªæ¯”å°] æª¢æŸ¥ï¼š'{text[:30]}'")
    print(f"    - æœ€é«˜åˆ†èªžå¥: '{examples[max_idx]}' ç›¸ä¼¼åº¦: {max_score:.3f}")
    if max_score > 0.7:
        return 1
    elif max_score > 0.5:
        return 0.5
    else:
        return 0

def is_escalated(text, examples, embeddings):
    if not examples or embeddings is None or len(examples) == 0:
        print("  [å‡ç´šæ¯”å°] ç„¡èªžå¥åº«ï¼Œä¸åŸ·è¡Œæ¯”å°")
        return 0
    test_emb = bert_model.encode([text], convert_to_tensor=True)
    sims = util.cos_sim(test_emb, embeddings).flatten()
    max_idx = int(sims.argmax())
    max_score = sims[max_idx].item()
    print(f"  [å‡ç´šæ¯”å°] æª¢æŸ¥ï¼š'{text[:30]}'")
    print(f"    - æœ€é«˜åˆ†èªžå¥: '{examples[max_idx]}' ç›¸ä¼¼åº¦: {max_score:.3f}")
    if max_score > 0.7:
        return 1
    elif max_score > 0.5:
        return 0.5
    else:
        return 0

def is_multi_user(text, examples, embeddings):
    if not examples or embeddings is None or len(examples) == 0:
        print("  [å¤šäººæ¯”å°] ç„¡èªžå¥åº«ï¼Œä¸åŸ·è¡Œæ¯”å°")
        return 0
    test_emb = bert_model.encode([text], convert_to_tensor=True)
    sims = util.cos_sim(test_emb, embeddings).flatten()
    max_idx = int(sims.argmax())
    max_score = sims[max_idx].item()
    print(f"  [å¤šäººæ¯”å°] æª¢æŸ¥ï¼š'{text[:30]}'")
    print(f"    - æœ€é«˜åˆ†èªžå¥: '{examples[max_idx]}' ç›¸ä¼¼åº¦: {max_score:.3f}")
    if max_score > 0.7:
        return 1
    elif max_score > 0.5:
        return 0.5
    else:
        return 0



# ---------- è‡ªå‹•é—œéµå­—æŠ½å– ----------

def extract_keywords(text, top_n=3):
    if not isinstance(text, str):
        if pd.isna(text):
            text = ""
        else:
            text = str(text).strip()

    return [kw[0] for kw in keybert_model.extract_keywords(text, top_n=top_n)]


# ---------- æ“´å……ï¼šè§£æ³•æŽ¨è–¦ ----------

def recommend_solution(text):
    if not isinstance(text, str):
        if pd.isna(text):
            text = ""
        else:
            text = str(text).strip()

    lowered = text.lower()

    if "login" in lowered:
        return "Please check your username/password, SSO settings, and permissions."
    elif "network" in lowered or "connection" in lowered:
        return "Please verify your network connection, VPN settings, and DNS configuration."
    elif "crash" in lowered or "freeze" in lowered:
        return "Try restarting the system and checking the application version."
    else:
        return "Refer to similar cases or contact the support team for assistance."
    


def is_actionable_resolution(text):
    if not isinstance(text, str) or not text.strip():
        return False

    # âœ… æ¨™æº–çš„ã€Œæœ‰æä¾›è§£æ³•ã€èªžæ°£æ¨£æ¿ï¼ˆå¯æ“´å……ï¼‰
    reference_texts = [
        "The issue was fixed by restarting the system.",
        "Steps were provided to the user.",
        "We guided the user through the process.",
        "Enabled access via admin portal.",
        "Action was completed successfully.",
        "The user's account was reactivated.",
        "Password was reset to restore access.",
        "Configuration settings were updated.",
        "Provided instructions to resolve the issue.",
        "Assisted the user remotely via Teams.",
        "Cleared cache and restarted the application.",
        "The permission issue was resolved by updating roles.",
        "Resolved by reinstalling the software.",
        "User was instructed to follow internal SOP.",
        "Helped user reset MFA settings.",
        "Added the user as a guest in the tenant.",
        "Reimaged the device to resolve the problem.",
        "VPN settings were corrected.",
        "Shared the fix through internal documentation.",
        "Confirmed the issue was resolved with user.",
        "Escalated issue was resolved by SME.",
        "Firewall rules were updated to allow access.",
        "License was reassigned to the correct user.",
        "System was patched to address the issue.",
        "Session was terminated and re-established to fix connectivity."
    ]


    try:
        # Encode ç›®æ¨™æ–‡å­—èˆ‡æ¨£æ¿
        target_embedding = bert_model.encode(text, convert_to_tensor=True)
        reference_embeddings = bert_model.encode(reference_texts, convert_to_tensor=True)

        # å–æœ€å¤§èªžæ„ç›¸ä¼¼åº¦
        cosine_scores = util.cos_sim(target_embedding, reference_embeddings)
        max_score = cosine_scores.max().item()

        print(f"ðŸ§  Resolution é¡žä¼¼åº¦æœ€é«˜åˆ†ï¼š{max_score:.2f}")  # âœ… å¯å°å‡º debug åˆ†æ•¸

        return max_score >= 0.5  # é–€æª»å¯èª¿æ•´
    except Exception as e:
        print("âŒ é¡žä¼¼åº¦åˆ†æžéŒ¯èª¤ï¼š", e)
        return False

def extract_cluster_name(texts, max_features=5, top_k=2):
    """
    å¾žä¸€çµ„æ–‡å­—ä¸­æŠ½å–ä»£è¡¨ä¸»é¡Œçš„é—œéµè©žï¼Œç”¨æ–¼å‘½å clusterã€‚
    """
    if not texts:
        return "cluster"
    
    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')
    X = vectorizer.fit_transform(texts)
    keywords = vectorizer.get_feature_names_out()
    return "_".join(keywords[:top_k]) if len(keywords) >= top_k else "_".join(keywords) if keywords.size > 0 else "cluster"

